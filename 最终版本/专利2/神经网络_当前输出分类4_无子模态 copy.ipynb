{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 库文件\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import copy\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.optimize import NonlinearConstraint\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from skopt import gp_minimize\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.font_manager import FontProperties\n",
    "# 设置中文字体\n",
    "font = FontProperties(fname=r\"c:\\windows\\fonts\\simsun.ttc\", size=12)  # 替换为你的中文字体文件路径\n",
    "import sys\n",
    "sys.path.append(r\"C:\\Users\\Admin\\Documents\\GitHub\\gaolu\\MPC\\高炉\")\n",
    "import optuna\n",
    "import numpy as np\n",
    "import optuna\n",
    "import numpy as np\n",
    "import base \n",
    "# 库文件\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import os\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from skopt import gp_minimize\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.font_manager import FontProperties\n",
    "# 设置中文字体\n",
    "font = FontProperties(fname=r\"c:\\windows\\fonts\\simsun.ttc\", size=12)  # 替换为你的中文字体文件路径\n",
    "import sys\n",
    "sys.path.append(r\"C:\\Users\\Admin\\Documents\\GitHub\\gaolu\\MPC\\高炉\")\n",
    "from collections import deque\n",
    "\n",
    "import base \n",
    "# 基础库\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import sys\n",
    "import os\n",
    "import datetime\n",
    "import pickle\n",
    "# 机器学习库\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "# 数据归一化、逆归一化\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# 优化相关库\n",
    "from skopt import gp_minimize\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# 深度学习库\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# 忽略警告\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# 中文字体设置\n",
    "from matplotlib.font_manager import FontProperties\n",
    "font = FontProperties(fname=r\"c:\\windows\\fonts\\simsun.ttc\", size=12)  # 替换为你的中文字体文件路径\n",
    "\n",
    "# 其他路径设置\n",
    "sys.path.append(r\"C:\\Users\\Admin\\Documents\\GitHub\\gaolu\\MPC\\高炉\")\n",
    "\n",
    "# 自定义模块\n",
    "import base \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取Excel文件\n",
    "# C:\\Users\\Admin\\Documents\\GitHub\\gaolu\\MPC\\高炉\\0数据处理\\新输入输出模式\\1h_mean.xlsx\n",
    "excel_path = f'C:\\\\Users\\\\Admin\\\\Documents\\\\GitHub\\\\gaolu\\\\MPC\\\\高炉\\\\0数据处理\\\\新输入输出模式\\\\1h_mean.xlsx'\n",
    "df_sheet_yuansu = pd.read_excel(excel_path, sheet_name='原始输出') \n",
    "# df_sheet_yuansu = pd.read_excel(excel_path, sheet_name='剔除直线输出') \n",
    "# df_sheet_yuansu = pd.read_excel(excel_path, sheet_name='单SI_0.2_0.8') \n",
    "# print(df_sheet_yuansu.info())\n",
    "# print(df_sheet_yuansu.columns)\n",
    "\n",
    "excel_path = f'C:\\\\Users\\\\Admin\\\\Documents\\\\GitHub\\\\gaolu\\\\MPC\\\\高炉\\\\0数据处理\\\\新输入输出模式\\\\1h_mean.xlsx'\n",
    "df_sheet_params = pd.read_excel(excel_path, sheet_name='1h_mean_all') \n",
    "\n",
    "# print(df_sheet_params.info())\n",
    "# print(df_sheet_params.columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 检查 DataFrame 中是否包含 NaN 值\n",
    "def check_if_NaN(data):\n",
    "    print(data.shape)\n",
    "    contains_nan = data.isna().any().any()\n",
    "    if contains_nan:\n",
    "        print(\"数据包含 NaN 值\")\n",
    "    else:\n",
    "        print(\"数据不包含 NaN 值\")\n",
    "\n",
    "        \n",
    "check_if_NaN(df_sheet_yuansu)\n",
    "check_if_NaN(df_sheet_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sheet_params.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_term =        ['富氧流量', '冷风流量', '热风压力', '热风温度']\n",
    "last_input_term =        ['富氧流量2', '设定喷煤量2', '热风压力2', '热风温度2']\n",
    "output_term = ['铁水温度[MIT]', '铁水硅含量[SI]']\n",
    "last_output_term = ['铁水温度[MIT]2', '铁水硅含量[SI]2']\n",
    "time_term= '时间戳h'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 处理异常值\n",
    "\n",
    "# 创建数据框副本以避免修改原始数据\n",
    "\n",
    "df_sheet_X = df_sheet_params.copy()\n",
    "df_sheet_X_process = df_sheet_X.copy()\n",
    "df_sheet_Y = df_sheet_yuansu.copy()\n",
    "df_sheet_Y_process = df_sheet_Y.copy()\n",
    "\n",
    "\n",
    "def IQR_process(df_IQR, columns):\n",
    "    df_IQR = df_IQR\n",
    "    columns = columns\n",
    "\n",
    "    print(columns)      # 获取数据框的所有列名\n",
    "    outlier_indices = set()  # 用于存储异常值的行索引\n",
    "\n",
    "    # 1. 分别处理每个变量\n",
    "    for column in columns:\n",
    "        # 计算描述性统计\n",
    "        stats = df_IQR[column].describe()\n",
    "\n",
    "        # 计算IQR（四分位距）以及上下须的范围\n",
    "        Q1 = stats['25%']\n",
    "        Q3 = stats['75%']\n",
    "        IQR = Q3 - Q1\n",
    "        lower_whisker = Q1 - 8 * IQR\n",
    "        upper_whisker = Q3 + 8 * IQR\n",
    "        # if column == '热风压力':\n",
    "        #     lower_whisker = Q1 - 2 * IQR\n",
    "        #     upper_whisker = Q3 + 1.5 * IQR\n",
    "\n",
    "        # # 绘制箱线图\n",
    "        # plt.figure(figsize=(8, 6))\n",
    "        # sns.boxplot(data=df_IQR[column])\n",
    "        # plt.title(f'Boxplot of {column}', fontproperties=font)\n",
    "        # plt.xlabel('Feature', fontproperties=font)\n",
    "        # plt.ylabel('Value', fontproperties=font)\n",
    "        # plt.show()\n",
    "\n",
    "        # 查找异常值的索引\n",
    "        outliers = df_IQR[(df_IQR[column] < lower_whisker) | \n",
    "                            (df_IQR[column] > upper_whisker)].index\n",
    "        outlier_indices.update(outliers)\n",
    "\n",
    "        # # 打印统计信息和异常值范围\n",
    "        # print(f\"列: {column}\")\n",
    "        # print(f\"第一四分位数 (Q1): {Q1}\")\n",
    "        # print(f\"第三四分位数 (Q3): {Q3}\")\n",
    "        # print(f\"下须 (lower whisker): {lower_whisker}\")\n",
    "        # print(f\"上须 (upper whisker): {upper_whisker}\")\n",
    "        # print(f\"找到的异常值索引: {list(outliers)}\")\n",
    "\n",
    "        \n",
    "        # print(f\"异常值数量: {len(outliers)}\")\n",
    "        # print(f\"总数: {len(df_IQR[column])}\")\n",
    "\n",
    "        # print(f\"异常值比例: {len(outliers)/len(df_IQR[column])}\\n\")\n",
    "\n",
    "    # 2. 删除所有异常值\n",
    "    df_cleaned = df_IQR.drop(index=outlier_indices)\n",
    "    # 重新设置索引，使索引从 0 开始，并丢弃旧索引\n",
    "    df_cleaned.reset_index(drop=True, inplace=True)\n",
    "    # 输出处理后的数据框信息\n",
    "    print(f\"原始数据行数: {df_IQR.shape[0]}\")\n",
    "    print(f\"删除异常值后的数据行数: {df_cleaned.shape[0]}\")\n",
    "\n",
    "    # 你可以继续对 df_cleaned 进行后续处理\n",
    "\n",
    "\n",
    "\n",
    "    return df_cleaned\n",
    "\n",
    "\n",
    "df_cleaned_X = IQR_process(df_sheet_X_process, input_term)\n",
    "df_cleaned_Y = IQR_process(df_sheet_Y_process, output_term)\n",
    "\n",
    "# print(np.max(df_cleaned_Y['铁水温度[MIT]']))\n",
    "# print(np.min(df_cleaned_Y['铁水温度[MIT]']))\n",
    "# print(np.max(df_cleaned_Y['铁水硅含量[SI]']))\n",
    "# print(np.min(df_cleaned_Y['铁水硅含量[SI]']))\n",
    "\n",
    "\n",
    "# 画出数据\n",
    "def plot_subplot(data_x_yuan,data_y_yuan,data_x,data_y,column):\n",
    "    plt.plot(data_x_yuan,data_y_yuan,'r-')\n",
    "    plt.plot(data_x,data_y,'m-')\n",
    "    # plt.xlabel(time_term, fontproperties=font)  # 使用中文标签\n",
    "    plt.ylabel(column, fontproperties=font)  # 使用中文标签\n",
    "    # 使用中文标签\n",
    "\n",
    "\n",
    "    \n",
    "plt.figure(figsize=(15, 4))\n",
    "for idx, column in enumerate(input_term):\n",
    "    plt.subplot(len(input_term), 1, idx+1)\n",
    "    plot_subplot(   df_sheet_X[time_term].values,   df_sheet_X[column].values, \n",
    "                    df_cleaned_X[time_term].values, df_cleaned_X[column].values,\n",
    "                    column                )\n",
    "    \n",
    "plt.figure(figsize=(15, 4))\n",
    "for idx, column in enumerate(output_term):\n",
    "    plt.subplot(len(output_term), 1, idx+1)\n",
    "    plot_subplot(   df_sheet_Y[time_term].values,   df_sheet_Y[column].values, \n",
    "                    df_cleaned_Y[time_term].values, df_cleaned_Y[column].values,\n",
    "                    column                )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 异常数据处理-处理前后对比\n",
    "# 创建数据框副本以避免修改原始数据\n",
    "df_sheet_yuansu_process = df_cleaned_Y.copy()\n",
    "df_sheet_params_process = df_cleaned_X.copy()\n",
    "# 定义一个函数，用前后两个值的差值按照距离进行加权替换异常值\n",
    "def replace_outliers_with_weighted_diff(x, y):\n",
    "    # 计算列的中位数\n",
    "    median_value = y.median()\n",
    "    # 检测异常值的索引\n",
    "    outliers_index = (y - median_value).abs() > 2.5 * y.std()\n",
    "    \n",
    "    # 遍历异常值的索引\n",
    "    for idx in outliers_index[outliers_index].index:\n",
    "        # 获取异常值前一个和后一个值的索引\n",
    "        prev_idx = idx - 1 if idx - 1 >= 0 else idx\n",
    "        next_idx = idx + 1 if idx + 1 < len(y) else idx\n",
    "        # 计算当前 x 与前后两个 x 的距离\n",
    "        dist_prev = abs(x[idx] - x[prev_idx])\n",
    "        dist_next = abs(x[next_idx] - x[idx])\n",
    "        total_dist = dist_prev + dist_next\n",
    "        # 计算权重\n",
    "        weight_prev = dist_next / total_dist\n",
    "        weight_next = dist_prev / total_dist\n",
    "        # 计算前后两个值的差值\n",
    "        diff = y[next_idx] - y[prev_idx]\n",
    "        # 根据权重进行插值\n",
    "        interpolated_value = y[prev_idx] + weight_prev * diff\n",
    "        # 用插值结果替代异常值\n",
    "        y[idx] = interpolated_value\n",
    "\n",
    "# 画出数据\n",
    "def plot_subplot(data_x,data_y_yuan,data_y,column):\n",
    "    plt.plot(data_x,data_y_yuan,'r-')\n",
    "    plt.plot(data_x,data_y,'m-')\n",
    "    # plt.xlabel(time_term, fontproperties=font)  # 使用中文标签\n",
    "    plt.ylabel(column, fontproperties=font)  # 使用中文标签\n",
    "    # 使用中文标签\n",
    "\n",
    "\n",
    "# 对指定列应用替代异常值的函数\n",
    "# 对指定列应用替代异常值的函数\n",
    "replace_outliers_with_weighted_diff(df_sheet_params_process[time_term], df_sheet_params_process[input_term[0]])\n",
    "replace_outliers_with_weighted_diff(df_sheet_params_process[time_term], df_sheet_params_process[input_term[1]])\n",
    "replace_outliers_with_weighted_diff(df_sheet_params_process[time_term], df_sheet_params_process[input_term[2]])\n",
    "replace_outliers_with_weighted_diff(df_sheet_params_process[time_term], df_sheet_params_process[input_term[3]])\n",
    "# replace_outliers_with_weighted_diff(df_sheet_params_process[time_term], df_sheet_params_process[input_term[4]])\n",
    "# replace_outliers_with_weighted_diff(df_sheet_params_process[time_term], df_sheet_params_process[input_term[5]])\n",
    "# replace_outliers_with_weighted_diff(df_sheet_params_process[time_term], df_sheet_params_process[input_term[6]])\n",
    "\n",
    "# replace_outliers_with_weighted_diff(df_sheet_yuansu_process[time_term], df_sheet_yuansu_process[output_term[0]])\n",
    "# replace_outliers_with_weighted_diff(df_sheet_yuansu_process[time_term], df_sheet_yuansu_process[output_term[1]])\n",
    "\n",
    "plt.figure(figsize=(15, 4))\n",
    "for idx, column in enumerate(input_term):\n",
    "    plt.subplot(len(input_term), 1, idx+1)\n",
    "    plot_subplot(df_sheet_params_process[time_term].values,df_cleaned_X[column].values,df_sheet_params_process[column].values,column)\n",
    "\n",
    "plt.figure(figsize=(15, 2))\n",
    "for idx, column in enumerate(output_term):\n",
    "    plt.subplot(len(output_term), 1, idx+1)\n",
    "    plot_subplot(df_sheet_yuansu_process[time_term].values,df_cleaned_Y[column].values,df_sheet_yuansu_process[column].values,column)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 画出选取的数据\n",
    "def plot_subplot(data_x,data_y,column,index_predict,index_gaolu):\n",
    "    plt.plot(data_x,data_y,'-', label='origin_data')\n",
    "    plt.plot(data_x[index_gaolu],data_y[index_gaolu],'r.', label='gaolu_data')\n",
    "    plt.plot(data_x[index_predict],data_y[index_predict],'g-', label='predict_data')\n",
    "    plt.legend()\n",
    "    # plt.xlabel(time_term, fontproperties=font)  # 使用中文标签\n",
    "    plt.ylabel(column, fontproperties=font)  # 使用中文标签\n",
    "\n",
    "\n",
    "\n",
    "length1 = 400\n",
    "start1 = 0\n",
    "length2 = 400\n",
    "start2 = 400\n",
    "\n",
    "\n",
    "index_gaolu   = range(start1, start1+length1+1, 1)\n",
    "index_predict     = range(start2, start2+length2+1, 1)\n",
    "# index = range(1, 7572, 1)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15, 6))\n",
    "for idx, column in enumerate(output_term):\n",
    "    plt.subplot(len(input_term+output_term), 1, idx+1)\n",
    "    plot_subplot(df_sheet_yuansu_process[time_term].values,df_sheet_yuansu_process[column].values,column,index_predict,index_gaolu)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sheet_yuansu_process.describe(percentiles=[.10, .90])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据归一化、逆归一化\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# 将数据存储为字典，每个键对应一列数据\n",
    "original_data_dict = {\n",
    "    input_term[0]:   df_sheet_params_process[input_term[0]].values,\n",
    "    input_term[1]:   df_sheet_params_process[input_term[1]].values,\n",
    "    input_term[2]:   df_sheet_params_process[input_term[2]].values,\n",
    "    input_term[3]:   df_sheet_params_process[input_term[3]].values,\n",
    "    # input_term[4]:   df_sheet_params_process[input_term[4]].values,\n",
    "    # input_term[5]:   df_sheet_params_process[input_term[5]].values,\n",
    "    # input_term[6]:   df_sheet_params_process[input_term[6]].values,\n",
    "    output_term[0]:  df_sheet_yuansu_process[output_term[0]].values,\n",
    "    output_term[1]:  df_sheet_yuansu_process[output_term[1]].values\n",
    "}\n",
    "\n",
    "# 初始化缩放器\n",
    "scalers = {}\n",
    "\n",
    "# 进行拟合\n",
    "for column, data in original_data_dict.items():\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    scaler.fit(data.reshape(-1, 1))  # 保证数据是列向量\n",
    "    scalers[column] = scaler\n",
    "\n",
    "# 进行归一化\n",
    "normalized_data_dict = {}\n",
    "for column, scaler in scalers.items():\n",
    "    normalized_data_dict[column] = scaler.transform(original_data_dict[column].reshape(-1, 1)).flatten()\n",
    "\n",
    "# 进行反归一化\n",
    "original_data_dict = {}\n",
    "for column, scaler in scalers.items():\n",
    "    original_data_dict[column] = scaler.inverse_transform(normalized_data_dict[column].reshape(-1, 1)).flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 标定归一化前后数据\n",
    "data_point = np.array([1500]).reshape(-1, 1)\n",
    "data1 = scalers[output_term[0]].transform(data_point).flatten()\n",
    "\n",
    "data_point = np.array(data1).reshape(-1, 1)\n",
    "data2 = scalers[output_term[0]].inverse_transform(data_point).flatten()\n",
    "\n",
    "data_point = np.array([1510]).reshape(-1, 1)\n",
    "data3 = scalers[output_term[0]].transform(data_point).flatten()\n",
    "\n",
    "data_point = np.array(data3).reshape(-1, 1)\n",
    "data4 = scalers[output_term[0]].inverse_transform(data_point).flatten()\n",
    "\n",
    "print(data1)\n",
    "print(data2)\n",
    "print(data3)\n",
    "print(data4)\n",
    "d_temp = (data3-data1)/(data4-data2)\n",
    "print('每摄氏度的输出差：',d_temp)\n",
    "\n",
    "\n",
    "\n",
    "data_point = np.array([0.50]).reshape(-1, 1)\n",
    "data1 = scalers[output_term[1]].transform(data_point).flatten()\n",
    "\n",
    "data_point = np.array(data1).reshape(-1, 1)\n",
    "data2 = scalers[output_term[1]].inverse_transform(data_point).flatten()\n",
    "\n",
    "data_point = np.array([0.60]).reshape(-1, 1)\n",
    "data3 = scalers[output_term[1]].transform(data_point).flatten()\n",
    "\n",
    "data_point = np.array(data3).reshape(-1, 1)\n",
    "data4 = scalers[output_term[1]].inverse_transform(data_point).flatten()\n",
    "\n",
    "print(data1)\n",
    "print(data2)\n",
    "print(data3)\n",
    "print(data4)\n",
    "d_yuansu = (data3-data1)/(data4-data2)\n",
    "print('每浓度的输出差：',(data3-data1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isShuffle = True\n",
    "isShuffle = False\n",
    "time_steps = 2\n",
    "test_size = 0.15\n",
    "val_size = 0.15\n",
    "train_size = 1-val_size-test_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 组合训练数据--拆分训练、测试集\n",
    "\n",
    "# 定义时间步数和特征数\n",
    "\n",
    "# 构成    \n",
    "# X = [X(t),X(t-1),Y(t-1)]\n",
    "# Y = [Y(t)]\n",
    "def make_data(u1_data,u2_data,u3_data,u4_data,y1_data,y2_data,index_fanwei):\n",
    "    X = np.column_stack((u1_data,u2_data,u3_data,u4_data))\n",
    "    y = np.column_stack((y1_data, y2_data))\n",
    "\n",
    "    X_modified = []\n",
    "    y_modified = []\n",
    "    \n",
    "    for i in range(3,len(y1_data)):\n",
    "        if i in index_fanwei:\n",
    "            # print(i)\n",
    "            # print(df_sheet_yuansu[time_term][i])\n",
    "            yuansu_time = df_sheet_yuansu[time_term][i]\n",
    "            closest_10 = df_sheet_params[df_sheet_params[time_term] <= yuansu_time].nlargest(time_steps, time_term)\n",
    "            # print(closest_10)\n",
    "            \n",
    "            index = closest_10.index\n",
    "            # print(index)\n",
    "            # print(closest_10.iloc[-1][time_term])\n",
    "            if closest_10.iloc[-1][time_term] < yuansu_time - time_steps + 1:\n",
    "                print(i,yuansu_time,'errloss')\n",
    "            else:\n",
    "\n",
    "                # print(X[index, :])\n",
    "                new_x_sample = np.concatenate([X[i, :] for i in index],axis=0)\n",
    "                # print(new_x_sample)\n",
    "                y_last = y[i-1, :]\n",
    "                # print(y_last, 'y_last time : ',df_sheet_yuansu[time_term][i-1])\n",
    "                new_x_sample = np.concatenate([new_x_sample,y_last],axis=0)\n",
    "                # print(new_x_sample)\n",
    "                y_sample = y[i, :]  \n",
    "                X_modified.append(new_x_sample)\n",
    "                y_modified.append(y_sample)\n",
    "                print(i,yuansu_time,index[0],index[-1], end='\\r')\n",
    "                # break\n",
    "\n",
    "    # 将列表转换为 NumPy 数组\n",
    "    X_modified = np.array(X_modified)\n",
    "    y_modified = np.array(y_modified)\n",
    "    X_reshaped = X_modified.reshape((X_modified.shape[0], X_modified.shape[1]))\n",
    "\n",
    "    # 打印新数据的形状\n",
    "    print(\"Modified Input Shape:\", X_reshaped.shape)\n",
    "    print(\"Modified Output Shape:\", y_modified.shape)\n",
    "\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_reshaped, y_modified, \n",
    "                                                        test_size=test_size, \n",
    "                                                        random_state=42, \n",
    "                                                        shuffle=isShuffle)\n",
    "\n",
    "    # 将剩余的70%训练数据再次拆分成训练数据和验证数据（20%验证数据，50%训练数据）\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, \n",
    "                                                        test_size=val_size/(train_size+val_size), \n",
    "                                                        random_state=42, \n",
    "                                                        shuffle=isShuffle)\n",
    "\n",
    "    print('训练数量：',X_train.shape,y_train.shape)\n",
    "    print('验证数量：',X_val.shape,y_val.shape)\n",
    "    print('测试数量：',X_test.shape,y_test.shape)\n",
    "\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def symmetrical_moving_average(data, N):\n",
    "    \"\"\"\n",
    "    使用对称的移动平均滤波，当前值由其自身及其前后的值决定。\n",
    "    \n",
    "    :param data: 输入的数据序列，一般为列表或者NumPy数组。\n",
    "    :return: 经过滤波处理的数据序列。\n",
    "    \"\"\"\n",
    "    filtered_data = []\n",
    "    N = 9\n",
    "    percent = 0.8\n",
    "    # 遍历数据，从索引1开始到倒数第二个元素结束\n",
    "    for i in range(1, len(data) - 1):\n",
    "        # 计算当前值及其前后值的平均\n",
    "        average = (data[i - 1]*(1-percent)/2 + data[i]*percent + data[i + 1]*(1-percent)/2)\n",
    "        filtered_data.append(average)\n",
    "    \n",
    "    # 对于序列的第一个和最后一个元素，直接使用原始值\n",
    "    # 或者可以使用其他边界处理策略\n",
    "    filtered_data.insert(0, data[0])\n",
    "    filtered_data.append(data[-1])\n",
    "    \n",
    "    return np.array(filtered_data)\n",
    "\n",
    "# 示例数据\n",
    "data = [2, 4, 6, 8, 10, 12, 14]\n",
    "filtered_data = symmetrical_moving_average(data,9)\n",
    "print(filtered_data)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 高炉模型列数据\n",
    "u1_data = normalized_data_dict[input_term[0]]\n",
    "u2_data = normalized_data_dict[input_term[1]]\n",
    "u3_data = normalized_data_dict[input_term[2]]\n",
    "u4_data = normalized_data_dict[input_term[3]]\n",
    "y1_data = normalized_data_dict[output_term[0]]\n",
    "y2_data = normalized_data_dict[output_term[1]]\n",
    "num_samples = y2_data.shape[0]\n",
    "\n",
    "# filter_windows = 2\n",
    "# u1_data = symmetrical_moving_average(u1_data, filter_windows)\n",
    "# u2_data = symmetrical_moving_average(u2_data, filter_windows)\n",
    "# u3_data = symmetrical_moving_average(u3_data, filter_windows)\n",
    "# u4_data = symmetrical_moving_average(u4_data, filter_windows)\n",
    "# u5_data = symmetrical_moving_average(u5_data, filter_windows)\n",
    "# u6_data = symmetrical_moving_average(u6_data, filter_windows)\n",
    "# u7_data = symmetrical_moving_average(u7_data, filter_windows)\n",
    "# y1_data = symmetrical_moving_average(y1_data, filter_windows)\n",
    "# y2_data = symmetrical_moving_average(y2_data, filter_windows)\n",
    "\n",
    "print('高炉模型数据')\n",
    "X_gaolu_train, X_gaolu_val, X_gaolu_test,\\\n",
    "y_gaolu_train, y_gaolu_val, y_gaolu_test = make_data(u1_data,u2_data,u3_data,u4_data,\n",
    "                                                            y1_data,y2_data,\n",
    "                                                            index_fanwei=index_gaolu)\n",
    "\n",
    "\n",
    "\n",
    "# 预测模型列数据\n",
    "u1_data = normalized_data_dict[input_term[0]]\n",
    "u2_data = normalized_data_dict[input_term[1]]\n",
    "u3_data = normalized_data_dict[input_term[2]]\n",
    "u4_data = normalized_data_dict[input_term[3]]\n",
    "y1_data = normalized_data_dict[output_term[0]]\n",
    "y2_data = normalized_data_dict[output_term[1]]\n",
    "num_samples = y2_data.shape[0]\n",
    "\n",
    "# filter_windows = 2\n",
    "# u1_data = symmetrical_moving_average(u1_data, filter_windows)\n",
    "# u2_data = symmetrical_moving_average(u2_data, filter_windows)\n",
    "# u3_data = symmetrical_moving_average(u3_data, filter_windows)\n",
    "# u4_data = symmetrical_moving_average(u4_data, filter_windows)\n",
    "# u5_data = symmetrical_moving_average(u5_data, filter_windows)\n",
    "# u6_data = symmetrical_moving_average(u6_data, filter_windows)\n",
    "# u7_data = symmetrical_moving_average(u7_data, filter_windows)\n",
    "# y1_data = symmetrical_moving_average(y1_data, filter_windows)\n",
    "# y2_data = symmetrical_moving_average(y2_data, filter_windows)\n",
    "print('预测模型数据')\n",
    "X_predict_train, X_predict_val, X_predict_test,\\\n",
    "y_predict_train, y_predict_val, y_predict_test = make_data(u1_data,u2_data,u3_data,u4_data,\n",
    "                                                            y1_data,y2_data,\n",
    "                                                            index_fanwei=index_predict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('高炉模型数据')\n",
    "X_1, X_2, X_3,\\\n",
    "y_1, y_2, y_3 = make_data(u1_data,u2_data,u3_data,u4_data,\n",
    "                                                            y1_data,y2_data,\n",
    "                                                            index_fanwei=range(0, 7000, 1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_concat = np.concatenate((X_1, X_2, X_3), axis=0)\n",
    "print(X_concat.shape)\n",
    "y_concat = np.concatenate((y_1, y_2, y_3), axis=0)\n",
    "print(y_concat.shape)\n",
    "new_concat = np.concatenate((X_concat[1:,0:4], y_concat[1:,0:2], y_concat[:-1,0:2]), axis=1)\n",
    "\n",
    "print(new_concat.shape)\n",
    "print(X_concat[0:2,:])\n",
    "print(y_concat[0:2,:])\n",
    "print(new_concat[0:2,:])\n",
    "\n",
    "\n",
    "\n",
    "data2_all = {\n",
    "    input_term[0]: new_concat[1:,0],\n",
    "    input_term[1]: new_concat[1:,1],\n",
    "    input_term[2]: new_concat[1:,2],\n",
    "    input_term[3]: new_concat[1:,3],\n",
    "    last_input_term[0]: new_concat[:-1,0],\n",
    "    last_input_term[1]: new_concat[:-1:,1],\n",
    "    last_input_term[2]: new_concat[:-1:,2],\n",
    "    last_input_term[3]: new_concat[:-1:,3],\n",
    "    output_term[0]: new_concat[1:,4],\n",
    "    output_term[1]: new_concat[1:,5],\n",
    "    last_output_term[0]: new_concat[1:,6],\n",
    "    last_output_term[1]: new_concat[1:,7]\n",
    "}\n",
    "\n",
    "# 将字典转换为 DataFrame\n",
    "data2_all = pd.DataFrame(data2_all)\n",
    "\n",
    "# 查看生成的 DataFrame\n",
    "print(data2_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_once_time = 50\n",
    "ischuangxin = True\n",
    "# ischuangxin = False\n",
    "cengshu = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义模型\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class MyNeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, if_chuangxin = False,gamma = 0.1):\n",
    "        self.if_chuangxin = if_chuangxin\n",
    "        super(MyNeuralNetwork, self).__init__()\n",
    "        if cengshu == 3:    \n",
    "            if self.if_chuangxin:            \n",
    "                self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "                self.relu = nn.ReLU()\n",
    "                self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "                self.fc3 = nn.Linear(hidden_size, hidden_size)\n",
    "                self.fc4 = nn.Linear(hidden_size, output_size)\n",
    "            else:\n",
    "                self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "                self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "                self.fc3 = nn.Linear(hidden_size, hidden_size)\n",
    "                self.fc4 = nn.Linear(hidden_size, output_size)\n",
    "                self.relu = nn.ReLU()\n",
    "        elif cengshu == 2:  \n",
    "            if self.if_chuangxin:            \n",
    "                self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "                self.relu = nn.ReLU()\n",
    "                self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "                self.fc3 = nn.Linear(hidden_size, output_size)\n",
    "            else:\n",
    "                self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "                self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "                self.fc3 = nn.Linear(hidden_size, output_size)\n",
    "                self.relu = nn.ReLU()\n",
    "\n",
    "\n",
    "    def forward(self, x0):\n",
    "        if cengshu == 3:    \n",
    "            if self.if_chuangxin:\n",
    "\n",
    "                x = self.fc1(x0)\n",
    "                x = self.relu(x)\n",
    "\n",
    "                x2 = self.fc2(x)\n",
    "                x2 = self.relu(x2)\n",
    "\n",
    "                x3 = self.fc3(x2)\n",
    "                x3 = self.relu(x3)\n",
    "\n",
    "                x4 = x + x2 + x3\n",
    "                output = self.fc4(x4)\n",
    "            else:\n",
    "                x = self.fc1(x0)\n",
    "                x = self.relu(x)\n",
    "\n",
    "                x2 = self.fc2(x)\n",
    "                x2 = self.relu(x2)\n",
    "\n",
    "                x3 = self.fc3(x2)\n",
    "                x3 = self.relu(x3)\n",
    "\n",
    "                output = self.fc4(x3)\n",
    "        elif cengshu == 2:  \n",
    "            if self.if_chuangxin:\n",
    "\n",
    "                x = self.fc1(x0)\n",
    "                x = self.relu(x)\n",
    "\n",
    "                x2 = self.fc2(x)\n",
    "                x2 = self.relu(x2)\n",
    "\n",
    "                x3 = x + x2\n",
    "                output = self.fc3(x3)\n",
    "            else:\n",
    "                x = self.fc1(x0)\n",
    "                x = self.relu(x)\n",
    "\n",
    "                x2 = self.fc2(x)\n",
    "                x2 = self.relu(x2)\n",
    "\n",
    "                output = self.fc3(x2)\n",
    "        return output\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    def custom_loss(self, y_true, y_pred):\n",
    "\n",
    "        squared_diff = torch.pow(y_true - y_pred, 2)\n",
    "        sum_squared_diff = torch.sum(squared_diff)\n",
    "        mse = sum_squared_diff / len(y_true)\n",
    "        return mse\n",
    "    \n",
    "\n",
    "    def my_fit(self, \n",
    "                X_train, y_train, \n",
    "                X_val, y_val, \n",
    "                train_loss_list,val_loss_list,\n",
    "                epochs=1, batch_size=32, lr=0.001):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            epoch_loss = 0\n",
    "            for i in range(0, len(X_train), batch_size):\n",
    "                x_batch = torch.tensor(X_train[i:i+batch_size], dtype=torch.float32)\n",
    "                y_batch = torch.tensor(y_train[i:i+batch_size], dtype=torch.float32)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                y_pred = self(x_batch)\n",
    "                loss = self.custom_loss(y_batch, y_pred)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "            average_epoch_train_loss = epoch_loss / (len(X_train) / batch_size)\n",
    "            # 验证集评估\n",
    "            self.eval()\n",
    "            with torch.no_grad():\n",
    "                val_loss = 0\n",
    "                for i in range(0, len(X_val), batch_size):\n",
    "                    x_batch_val = torch.tensor(X_val[i:i+batch_size], dtype=torch.float32)\n",
    "                    y_batch_val = torch.tensor(y_val[i:i+batch_size], dtype=torch.float32)\n",
    "\n",
    "                    y_pred_val = self(x_batch_val)\n",
    "                    val_loss += self.custom_loss(y_batch_val, y_pred_val).item()\n",
    "\n",
    "                average_epoch_val_loss = val_loss / (len(X_val) / batch_size)\n",
    "\n",
    "            print(f'第 {epoch + 1}/{epochs} 轮, 训练误差: {average_epoch_train_loss:.4f}, 验证误差: {average_epoch_val_loss:.4f}', end='\\r')\n",
    "            train_loss_list.append(average_epoch_train_loss)\n",
    "            val_loss_list.append(average_epoch_val_loss)\n",
    "\n",
    "        return train_loss_list,val_loss_list\n",
    "    \n",
    "    def model_update(self, \n",
    "                X_train, y_train, \n",
    "                epochs=1, batch_size=32, lr=0.001,ifprint = False):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "        for epoch in range(epochs):\n",
    "            epoch_loss = 0\n",
    "            for i in range(0, len(X_train), batch_size):\n",
    "                x_batch = torch.tensor(X_train[i:i+batch_size], dtype=torch.float32)\n",
    "                y_batch = torch.tensor(y_train[i:i+batch_size], dtype=torch.float32)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                y_pred = self(x_batch)\n",
    "                loss = self.custom_loss(y_batch, y_pred)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "            average_epoch_train_loss = epoch_loss / (len(X_train) / batch_size)\n",
    "            if(ifprint):print(f'第 {epoch + 1}/{epochs} 轮, 训练误差: {average_epoch_train_loss:.4f}')\n",
    "            \n",
    "            \n",
    "        return 0\n",
    "    \n",
    "    \n",
    "\n",
    "    def my_predict(self, X_test):\n",
    "        # 设置模型为评估模式，这会关闭 dropout 等层\n",
    "        self.eval()\n",
    "        # 将输入数据转换为张量，并设置 requires_grad=True\n",
    "        x_tensor = torch.tensor(X_test, dtype=torch.float32, requires_grad=True)\n",
    "        \n",
    "        # 获取模型的预测输出\n",
    "        y_pred = self(x_tensor)\n",
    "        # 保留预测值的梯度信息\n",
    "        y_pred.retain_grad()\n",
    "        # 返回预测结果和包含梯度信息的张量\n",
    "        return y_pred[:,0].detach().numpy(),y_pred[:,1].detach().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立高炉模型实例\n",
    "input_size = 10  # 输入特征大小\n",
    "hidden_size = 16  # 32\n",
    "output_size = 2  # 输出大小\n",
    "# 设置随机种子\n",
    "torch.manual_seed(0)\n",
    "model_gaolu = MyNeuralNetwork(input_size, \n",
    "                            hidden_size,\n",
    "                            output_size,\n",
    "                            ischuangxin,\n",
    "                            gamma = 0.1)\n",
    "epoch_sum_gaolu = 0\n",
    "gaolu_train_loss_list = []\n",
    "gaolu_val_loss_list = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 高炉模型训练\n",
    "epoch_once = epoch_once_time\n",
    "epoch_sum_gaolu = epoch_sum_gaolu + epoch_once\n",
    "gaolu_train_loss_list,gaolu_val_loss_list = model_gaolu.my_fit(X_gaolu_train, y_gaolu_train,\n",
    "                                    X_gaolu_val, y_gaolu_val, \n",
    "                                    gaolu_train_loss_list, gaolu_val_loss_list,\n",
    "                                    epochs=epoch_once, \n",
    "                                    batch_size=32,\n",
    "                                    lr = 0.002)\n",
    "\n",
    "print('\\nepoch_sum:',epoch_sum_gaolu)\n",
    "\n",
    "# 绘制训练和验证损失曲线\n",
    "plt.plot(gaolu_train_loss_list, label='Train Loss')\n",
    "plt.plot(gaolu_val_loss_list, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_term\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用于子图编号的字母序列\n",
    "subplot_labels = ['(a)', '(b)', '(c)', '(d)', '(e)', '(f)', '(g)', '(h)']\n",
    "input_term333 =       input_term\n",
    "output_term333 = ['铁水温度MIT', '铁水硅含量[Si]']\n",
    "time_term= '时间戳h'\n",
    "print(input_term333)\n",
    "print(output_term333)\n",
    "input_term222 =        ['富氧流量/(m\\u00b3/h)', '冷风流量/(m\\u00b3/h)', '热风压力/kPa', '热风温度/℃']\n",
    "output_term222 = ['铁水温度MIT/℃', '铁水硅含量[Si]/%']\n",
    "time_term= '时间戳h'\n",
    "print(input_term222)\n",
    "print(output_term222)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ditem = -0.65\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def double_control_train_test_result(scalers, output_term, y_test, y_pred_0, y_pred_1, y_test_2, y_pred_0_2, y_pred_1_2,ditem,a,b):\n",
    "    y_test_0 = scalers[output_term[0]].inverse_transform((y_test[:, 0]).reshape(-1, 1)).flatten()\n",
    "    y_test_1 = scalers[output_term[1]].inverse_transform((y_test[:, 1]).reshape(-1, 1)).flatten()\n",
    "    y_pred_0_inverse_transform = scalers[output_term[0]].inverse_transform((y_pred_0).reshape(-1, 1)).flatten()\n",
    "    y_pred_1_inverse_transform = scalers[output_term[1]].inverse_transform((y_pred_1).reshape(-1, 1)).flatten()\n",
    "\n",
    "    output0 = y_test_0 - y_pred_0_inverse_transform\n",
    "    output1 = y_test_1 - y_pred_1_inverse_transform\n",
    "\n",
    "    plt.figure(figsize=(9, 8))\n",
    "    \n",
    "    \n",
    "    ax = plt.subplot(4, 1, 1)\n",
    "    plt.plot(y_test_0,'k', label=\"真实值\")\n",
    "    plt.plot(y_pred_0_inverse_transform,'r--', label=\"预测值\")\n",
    "    ax.legend(prop=font, loc='upper right', bbox_to_anchor=(a, b), ncol=4) \n",
    "    \n",
    "    plt.ylabel(output_term222[0], fontproperties=font)  \n",
    "    ax.yaxis.set_label_coords(-0.07, 0.5)  \n",
    "    ax.text(0.5, ditem, f'{subplot_labels[0]} {output_term333[0]}建模效果', \n",
    "            transform=ax.transAxes, ha='center', fontproperties=font)  \n",
    "    ax.set_xlabel('训练样本', fontproperties=font)  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ax = plt.subplot(4, 1, 2)\n",
    "    plt.plot(y_test_1,'k')\n",
    "    plt.plot(y_pred_1_inverse_transform,'r--')\n",
    "    plt.ylabel(output_term222[1], fontproperties=font) \n",
    "\n",
    "    ax.yaxis.set_label_coords(-0.07, 0.5)  \n",
    "    ax.text(0.5, ditem, f'{subplot_labels[1]} {output_term333[1]}建模效果', \n",
    "            transform=ax.transAxes, ha='center', fontproperties=font)  \n",
    "    \n",
    "    ax.set_xlabel('训练样本', fontproperties=font)  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    y_test_0 = scalers[output_term[0]].inverse_transform((y_test_2[:, 0]).reshape(-1, 1)).flatten()\n",
    "    y_test_1 = scalers[output_term[1]].inverse_transform((y_test_2[:, 1]).reshape(-1, 1)).flatten()\n",
    "    y_pred_0_inverse_transform = scalers[output_term[0]].inverse_transform((y_pred_0_2).reshape(-1, 1)).flatten()\n",
    "    y_pred_1_inverse_transform = scalers[output_term[1]].inverse_transform((y_pred_1_2).reshape(-1, 1)).flatten()\n",
    "\n",
    "    output0 = y_test_0 - y_pred_0_inverse_transform\n",
    "    output1 = y_test_1 - y_pred_1_inverse_transform\n",
    "\n",
    "    ax = plt.subplot(4, 1, 3)\n",
    "    plt.plot(y_test_0,'k', label=\"真实值\")\n",
    "    plt.plot(y_pred_0_inverse_transform,'r--', label=\"预测值\")\n",
    "    plt.ylabel(output_term222[0], fontproperties=font)  \n",
    "    ax.yaxis.set_label_coords(-0.07, 0.5)  \n",
    "    ax.text(0.5, ditem, f'{subplot_labels[2]} {output_term333[0]}预测效果', \n",
    "            transform=ax.transAxes, ha='center', fontproperties=font)  \n",
    "    ax.set_xlabel('测试样本', fontproperties=font)  \n",
    "\n",
    "\n",
    "\n",
    "    ax = plt.subplot(4, 1, 4)\n",
    "    plt.plot(y_test_1,'k')\n",
    "    plt.plot(y_pred_1_inverse_transform,'r--')\n",
    "    plt.ylabel(output_term222[1], fontproperties=font)  \n",
    "    ax.yaxis.set_label_coords(-0.07, 0.5)  \n",
    "    ax.text(0.5, ditem, f'{subplot_labels[3]} {output_term333[1]}预测效果', \n",
    "            transform=ax.transAxes, ha='center', fontproperties=font)  \n",
    "    ax.set_xlabel('测试样本', fontproperties=font)  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    plt.subplots_adjust(hspace=0.6)  # 调整子图之间的垂直间距\n",
    "    plt.tight_layout()  # 自动调整子图布局\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 高炉模型建模效果\n",
    "y_train_pred_0,y_train_pred_1 = model_gaolu.my_predict(X_gaolu_train)\n",
    "y_test_pred_0,y_test_pred_1 = model_gaolu.my_predict(X_gaolu_test)\n",
    "\n",
    "double_control_train_test_result(scalers,  output_term,\n",
    "                                        y_gaolu_train,  y_train_pred_0, y_train_pred_1,\n",
    "                                        y_gaolu_test ,   y_test_pred_0,  y_test_pred_1,\n",
    "                                        ditem = -0.65   ,a = 0.63, b = 0.38 )\n",
    "\n",
    "# base.double_control_train_test_result(scalers,  output_term,\n",
    "#                                         y_gaolu_train[:-1],  y_train_pred_0[1:], y_train_pred_1[1:],\n",
    "#                                         y_gaolu_test[:-1] ,   y_test_pred_0[1:],  y_test_pred_1[1:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建预测模型实例\n",
    "# 设置随机种子\n",
    "torch.manual_seed(0)\n",
    "model_predict = MyNeuralNetwork(input_size, hidden_size, output_size,ischuangxin)\n",
    "epoch_sum_predict = 0\n",
    "predict_train_loss_list = []\n",
    "predict_val_loss_list = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预测模型训练\n",
    "epoch_once = epoch_once_time\n",
    "epoch_sum = epoch_sum_predict + epoch_once\n",
    "predict_train_loss_list, predict_val_loss_list = model_predict.my_fit(X_predict_train, y_predict_train,\n",
    "                                    X_predict_val, y_predict_val, \n",
    "                                    predict_train_loss_list, predict_val_loss_list,\n",
    "                                    epochs=epoch_once, \n",
    "                                    batch_size=64,\n",
    "                                    lr = 0.002)\n",
    "\n",
    "print('\\nepoch_sum:',epoch_sum_predict)\n",
    "\n",
    "\n",
    "# 绘制训练和验证损失曲线\n",
    "plt.plot(predict_train_loss_list, label='Train Loss')\n",
    "plt.plot(predict_val_loss_list, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预测模型建模效果\n",
    "y_train_pred_0,y_train_pred_1 = model_predict.my_predict(X_predict_train)\n",
    "\n",
    "\n",
    "y_test_pred_0,y_test_pred_1 = model_predict.my_predict(X_predict_test)\n",
    "\n",
    "double_control_train_test_result(scalers,  output_term,\n",
    "                                        y_predict_train,  y_train_pred_0, y_train_pred_1,\n",
    "                                        y_predict_test,   y_test_pred_0,  y_test_pred_1,\n",
    "                                        ditem = -0.65   ,a = 0.31, b = 0.38 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base.gaolu_predict_raw(scalers,output_term,model_predict,model_gaolu,X_predict_test,y_predict_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用NumPy重新构建神经网络架构\n",
    "class MyNeuralNetworkNumpy:\n",
    "    def __init__(self, model, input_size, hidden_size, output_size,ifchuangxin):\n",
    "        self.ifchuangxin = ifchuangxin\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        params = {name: param.detach().numpy() for name, param in model.state_dict().items()}\n",
    "        if cengshu == 3:    \n",
    "            self.weights_fc1 = params['fc1.weight']\n",
    "            self.bias_fc1 = params['fc1.bias']\n",
    "            self.weights_fc2 = params['fc2.weight']\n",
    "            self.bias_fc2 = params['fc2.bias']\n",
    "            self.weights_fc3 = params['fc3.weight']\n",
    "            self.bias_fc3 = params['fc3.bias']\n",
    "            self.weights_fc4 = params['fc4.weight']\n",
    "            self.bias_fc4 = params['fc4.bias']\n",
    "        elif cengshu == 2:  \n",
    "            self.weights_fc1 = params['fc1.weight']\n",
    "            self.bias_fc1 = params['fc1.bias']\n",
    "            self.weights_fc2 = params['fc2.weight']\n",
    "            self.bias_fc2 = params['fc2.bias']\n",
    "            self.weights_fc3 = params['fc3.weight']\n",
    "            self.bias_fc3 = params['fc3.bias']\n",
    "        \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    def relu(self, x):\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if cengshu == 3:    \n",
    "            if self.ifchuangxin:\n",
    "                hidden1 = np.dot(x, self.weights_fc1.T) + self.bias_fc1\n",
    "                hidden1 = self.relu(hidden1)\n",
    "\n",
    "                hidden2 = np.dot(hidden1, self.weights_fc2.T) + self.bias_fc2\n",
    "                hidden2 = self.relu(hidden2)\n",
    "\n",
    "                hidden3 = np.dot(hidden2, self.weights_fc3.T) + self.bias_fc3\n",
    "                hidden3 = self.relu(hidden3)\n",
    "\n",
    "                hidden4 = hidden1 + hidden2 + hidden3\n",
    "                output = np.dot(hidden4, self.weights_fc4.T) + self.bias_fc4\n",
    "\n",
    "            else:\n",
    "                hidden1 = np.dot(x, self.weights_fc1.T) + self.bias_fc1\n",
    "                hidden1 = self.relu(hidden1)\n",
    "\n",
    "                hidden2 = np.dot(hidden1, self.weights_fc2.T) + self.bias_fc2\n",
    "                hidden2 = self.relu(hidden2)\n",
    "\n",
    "                hidden3 = np.dot(hidden2, self.weights_fc3.T) + self.bias_fc3\n",
    "                hidden3 = self.relu(hidden3)\n",
    "\n",
    "                output = np.dot(hidden3, self.weights_fc4.T) + self.bias_fc4\n",
    "        elif cengshu == 2:  \n",
    "            if self.ifchuangxin:\n",
    "                hidden1 = np.dot(x, self.weights_fc1.T) + self.bias_fc1\n",
    "                hidden1 = self.relu(hidden1)\n",
    "\n",
    "                hidden2 = np.dot(hidden1, self.weights_fc2.T) + self.bias_fc2\n",
    "                hidden2 = self.relu(hidden2)\n",
    "\n",
    "                hidden3 = hidden1 + hidden2\n",
    "                output = np.dot(hidden3, self.weights_fc3.T) + self.bias_fc3\n",
    "                # hidden2 = self.relu(hidden2)\n",
    "\n",
    "                # hidden3 = np.concatenate([x, hidden1, hidden2], axis=1)  # 按列连接\n",
    "                # output = np.dot(hidden3, self.weights_fc3.T) + self.bias_fc3\n",
    "\n",
    "            else:\n",
    "                hidden1 = np.dot(x, self.weights_fc1.T) + self.bias_fc1\n",
    "                hidden1 = self.relu(hidden1)\n",
    "\n",
    "                hidden2 = np.dot(hidden1, self.weights_fc2.T) + self.bias_fc2\n",
    "                hidden2 = self.relu(hidden2)\n",
    "\n",
    "                output = np.dot(hidden2, self.weights_fc3.T) + self.bias_fc3\n",
    "\n",
    "\n",
    "            \n",
    "        return output\n",
    "    \n",
    "    \n",
    "    def my_predict(self, data_input):\n",
    "        input = data_input  # 随机初始化一个输入序列\n",
    "        output_prediction = model_numpy.forward(input)\n",
    "        return output_prediction[:,0], output_prediction[:,1]\n",
    "\n",
    "# 使用NumPy模型进行预测\n",
    "model_numpy = MyNeuralNetworkNumpy(model_predict, input_size, hidden_size, output_size,ischuangxin)\n",
    "\n",
    "\n",
    "y_pred_0, y_pred_1 = model_numpy.my_predict(X_predict_test)\n",
    "\n",
    "# 计算 RMSE、MRE\n",
    "y_test = y_predict_test\n",
    "model_temp = copy.deepcopy(model_predict)\n",
    "base.double_control_predict_result(scalers,output_term,y_test,y_pred_0,y_pred_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 的方式公司的r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(last_input_term)\n",
    "print(output_term)\n",
    "print(input_term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import MeanShift\n",
    "from sklearn.decomposition import KernelPCA\n",
    "# 提取 output_term 和 input_term 的数据\n",
    "def extract_data(data, terms):\n",
    "    return np.column_stack([data[term] for term in terms])\n",
    "\n",
    "# output_data = extract_data(data2_all, output_term+last_output_term)\n",
    "# input_data = extract_data(data2_all, input_term)\n",
    "output_data = extract_data(data2_all, output_term+last_input_term)\n",
    "input_data = extract_data(data2_all, input_term)\n",
    "print(output_data.shape)\n",
    "print(type(output_data))\n",
    "print(f'Output data shape: {output_data.shape}')\n",
    "print(f'Input data shape: {input_data.shape}')\n",
    "\n",
    "\n",
    "\n",
    "# 使用 KernelPCA 将数据映射到高维空间\n",
    "kpca = KernelPCA(n_components=10, kernel='rbf', gamma=0.2)\n",
    "# transformed_data = kpca.fit_transform(output_data)\n",
    "\n",
    "\n",
    "# 使用 K-Means 对 output_term 进行聚类5\n",
    "n_clusters = 12\n",
    "random_state = 25  # 固定随机数种子\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=random_state)\n",
    "# 使用 K-Means 对 output_term 进行聚类5\n",
    "# n_clusters = len(set(labels))\n",
    "# # random_state = 42  # 固定随机数种子\n",
    "# kmeans = MeanShift()\n",
    "labels = kmeans.fit_predict(output_data)\n",
    "\n",
    "# n_clusters = len(set(labels))\n",
    "\n",
    "# 计算每个子模态的 input_term 空间分布范围\n",
    "def compute_limits(input_data, labels, n_clusters):\n",
    "    limits = {}\n",
    "    for i in range(n_clusters):\n",
    "        cluster_data = input_data[labels == i]\n",
    "        min_limits = cluster_data.min(axis=0)\n",
    "        max_limits = cluster_data.max(axis=0)\n",
    "        print(min_limits)\n",
    "        print(max_limits)\n",
    "        # limits[i] = {input_term[j]: (min_limits[j], max_limits[j]) for j in range(len(input_term))}\n",
    "        limits[i] = {input_term[j]: (min_limits[j], max_limits[j]) for j in range(len(input_term))}\n",
    "    return limits\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "input_term_limits = compute_limits(input_data, labels, n_clusters)\n",
    "\n",
    "# 输出每个子模态的 input_term 限制范围\n",
    "def print_limits(limits):\n",
    "    for mode, terms in limits.items():\n",
    "        print(f\"Mode {mode}:\")\n",
    "        for term, (min_val, max_val) in terms.items():\n",
    "            print(f\"  {term}: Min={min_val}, Max={max_val}\")\n",
    "\n",
    "print_limits(input_term_limits)\n",
    "\n",
    "\n",
    "def plot_clusters_pairwise(output_data, labels, terms):\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    n_clusters = len(set(labels))  # 假设labels是从0开始编号的簇标签\n",
    "    num_terms = len(terms)\n",
    "\n",
    "    # 遍历所有的维度组合（两两配对）\n",
    "    for i in range(num_terms):\n",
    "        for j in range(i+1, num_terms):\n",
    "            plt.subplot(num_terms-1, num_terms-1, i*(num_terms-1) + j)\n",
    "            for k in range(n_clusters):\n",
    "                cluster_data = output_data[labels == k]\n",
    "                plt.scatter(cluster_data[:, i], cluster_data[:, j], s=5, label=f'Mode {k}')\n",
    "            plt.title(f'{terms[i]} vs {terms[j]}', fontproperties=font)\n",
    "            plt.xlabel(terms[i], fontproperties=font)\n",
    "            plt.ylabel(terms[j], fontproperties=font)\n",
    "            # plt.legend()\n",
    "            plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 假设output_term和last_output_term组合在一起\n",
    "# terms = output_term + last_output_term\n",
    "terms = output_term+last_input_term\n",
    "plot_clusters_pairwise(output_data, labels, terms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看 labels 的种类及数量\n",
    "unique_labels = np.unique(labels)\n",
    "print(f\"Unique labels: {unique_labels}\")\n",
    "print(f\"Number of unique labels: {len(unique_labels)}\")\n",
    "# 统计每个标签出现的次数\n",
    "label_counts = np.bincount(labels)\n",
    "for label, count in enumerate(label_counts):\n",
    "    print(f\"Label {label}: {count} points\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from sklearn.cluster import KMeans\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.decomposition import KernelPCA\n",
    "# # 提取 output_term 和 input_term 的数据\n",
    "# def extract_data(data, terms):\n",
    "#     return np.column_stack([data[term] for term in terms])\n",
    "\n",
    "# output_data = extract_data(data2_all, output_term+last_output_term)\n",
    "# input_data = extract_data(data2_all, input_term)\n",
    "# print(output_data.shape)\n",
    "# print(type(output_data))\n",
    "# print(f'Output data shape: {output_data.shape}')\n",
    "# print(f'Input data shape: {input_data.shape}')\n",
    "\n",
    "\n",
    "\n",
    "# # 使用 KernelPCA 将数据映射到高维空间\n",
    "# # kpca = KernelPCA(n_components=10, kernel='rbf', gamma=0.1)\n",
    "# # transformed_data = kpca.fit_transform(output_data)\n",
    "\n",
    "\n",
    "\n",
    "# # 使用 K-Means 对 output_term 进行聚类5\n",
    "# n_clusters = 5\n",
    "# # random_state = 42  # 固定随机数种子\n",
    "# kmeans = KMeans(n_clusters=n_clusters)\n",
    "# labels = kmeans.fit_predict(output_data)\n",
    "\n",
    "# # 计算每个子模态的 input_term 空间分布范围\n",
    "# def compute_limits(input_data, labels, n_clusters):\n",
    "#     limits = {}\n",
    "#     for i in range(n_clusters):\n",
    "#         cluster_data = input_data[labels == i]\n",
    "#         min_limits = cluster_data.min(axis=0)\n",
    "#         max_limits = cluster_data.max(axis=0)\n",
    "#         print(min_limits)\n",
    "#         print(max_limits)\n",
    "#         limits[i] = {input_term[j]: (min_limits[j], max_limits[j]) for j in range(len(input_term))}\n",
    "#     return limits\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# input_term_limits = compute_limits(input_data, labels, n_clusters)\n",
    "\n",
    "# # 输出每个子模态的 input_term 限制范围\n",
    "# def print_limits(limits):\n",
    "#     for mode, terms in limits.items():\n",
    "#         print(f\"Mode {mode}:\")\n",
    "#         for term, (min_val, max_val) in terms.items():\n",
    "#             print(f\"  {term}: Min={min_val}, Max={max_val}\")\n",
    "\n",
    "# print_limits(input_term_limits)\n",
    "\n",
    "\n",
    "# def plot_clusters_pairwise(output_data, labels, terms):\n",
    "#     plt.figure(figsize=(12, 12))\n",
    "#     n_clusters = len(set(labels))  # 假设labels是从0开始编号的簇标签\n",
    "#     num_terms = len(terms)\n",
    "\n",
    "#     # 遍历所有的维度组合（两两配对）\n",
    "#     for i in range(num_terms):\n",
    "#         for j in range(i+1, num_terms):\n",
    "#             plt.subplot(num_terms-1, num_terms-1, i*(num_terms-1) + j)\n",
    "#             for k in range(n_clusters):\n",
    "#                 cluster_data = output_data[labels == k]\n",
    "#                 plt.scatter(cluster_data[:, i], cluster_data[:, j], s=5, label=f'Mode {k}')\n",
    "#             plt.title(f'{terms[i]} vs {terms[j]} space', fontproperties=font)\n",
    "#             plt.xlabel(terms[i], fontproperties=font)\n",
    "#             plt.ylabel(terms[j], fontproperties=font)\n",
    "#             plt.legend()\n",
    "#             plt.grid(True)\n",
    "\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "# # 假设output_term和last_output_term组合在一起\n",
    "# terms = output_term + last_output_term\n",
    "# plot_clusters_pairwise(output_data, labels, terms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# 可视化不同标签下 input_term 数据的二维分布，并绘制边界框\n",
    "def plot_input_term_distribution(ax, input_data, labels, input_term, x_index=0, y_index=1, limits=None, n_clusters=12):\n",
    "    # 使用颜色映射为每个簇生成不同的颜色\n",
    "    colors = plt.cm.get_cmap('tab20', n_clusters)  # 选用 'tab20' 调色板，也可选其他如 'viridis', 'plasma'\n",
    "    \n",
    "    for i in range(n_clusters):\n",
    "        cluster_data = input_data[labels == i]\n",
    "        \n",
    "        # 从 colormap 中获取当前簇的颜色\n",
    "        color = colors(i)\n",
    "        \n",
    "        # 在传入的 ax 上绘制散点\n",
    "        ax.scatter(cluster_data[:, x_index], cluster_data[:, y_index], s=10, label=f'Mode {i}', color=color)\n",
    "        \n",
    "        # 如果提供了边界条件，绘制边界框\n",
    "        if limits is not None and i in limits:\n",
    "            x_min, x_max = limits[i][input_term[x_index]]\n",
    "            y_min, y_max = limits[i][input_term[y_index]]\n",
    "            \n",
    "            # 绘制矩形，颜色与散点颜色一致\n",
    "            rect = plt.Rectangle((x_min, y_min), x_max - x_min, y_max - y_min, \n",
    "                                 linewidth=2, edgecolor=color, facecolor='none', label=f'Boundary {i}')\n",
    "            ax.add_patch(rect)\n",
    "    \n",
    "    ax.set_title(f'{input_term[x_index]} vs {input_term[y_index]}',fontproperties = font)\n",
    "    ax.set_xlabel(input_term[x_index],fontproperties = font)\n",
    "    ax.set_ylabel(input_term[y_index],fontproperties = font)\n",
    "    ax.grid(True)\n",
    "\n",
    "# 获取 input_term 的数量\n",
    "# num_terms = len(input_term)\n",
    "num_terms = len(input_term)\n",
    "\n",
    "# 创建 num_terms x num_terms 的子图\n",
    "fig, axes = plt.subplots(num_terms, num_terms, figsize=(num_terms*5, num_terms*5))\n",
    "fig.tight_layout(pad=3.0)\n",
    "\n",
    "# 遍历所有的 (i, j) 组合，分别绘制在各自的子图上\n",
    "for i in range(num_terms):\n",
    "    for j in range(num_terms):\n",
    "        ax = axes[i, j]  # 获取对应的子图\n",
    "        if i == j :\n",
    "            continue\n",
    "        # plot_input_term_distribution(ax, input_data, labels, input_term, \n",
    "        #                              x_index=i, y_index=j, \n",
    "        #                              limits=input_term_limits, \n",
    "        #                              n_clusters=n_clusters)\n",
    "        plot_input_term_distribution(ax, input_data, labels, input_term, \n",
    "                                     x_index=i, y_index=j, \n",
    "                                     limits=input_term_limits, \n",
    "                                     n_clusters=n_clusters)\n",
    "\n",
    "# 显示所有子图\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(4, 1, figsize=(10, 10))\n",
    "\n",
    "for i in range(len(input_term)):\n",
    "    plt.subplot(4, 1, i + 1)\n",
    "    \n",
    "    # 绘制每个簇的数据\n",
    "    for j in range(n_clusters):\n",
    "        cluster_data = input_data[labels == j]\n",
    "        plt.plot(cluster_data[:, i], label=f'Cluster {j}')  # 使用标签显示簇编号\n",
    "    \n",
    "    # 添加坐标轴标签和标题\n",
    "    plt.xlabel('Index')  # x 轴是索引或时间步长\n",
    "    plt.ylabel(input_term[i],fontproperties = font)  # y 轴是当前的 input_term 项\n",
    "    plt.title(f'{input_term[i]} Distribution Across Clusters',fontproperties = font) # 标题根据 input_term 的名称设定\n",
    "    plt.legend()  # 添加图例\n",
    "    plt.grid(True)  # 显示网格\n",
    "\n",
    "plt.tight_layout()  # 自动调整子图布局以避免重叠\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(4, 1, figsize=(10, 10))\n",
    "for i in range(0,len(input_term)):\n",
    "    plt.subplot(4,1,i+1)\n",
    "    for j in range(0,n_clusters):\n",
    "        cluster_data = input_data[labels == j]\n",
    "        # print(cluster_data[:,0].shape)\n",
    "        plt.plot(cluster_data[:,i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iscontrol = True\n",
    "# iscontrol = False\n",
    "Times = 20*25*80\n",
    "\n",
    "Times = 600\n",
    "\n",
    "# 过度系数  0.1 越小过度越快\n",
    "rou = 0.1\n",
    "if_add_noise = 0\n",
    "if_gaolu_is_predict = 0\n",
    "if_update_model = True\n",
    "maxlen = 50\n",
    "\n",
    "ifGAN = False\n",
    "\n",
    "scalers_X = scalers\n",
    "# scalers_X = scalers_X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成参考轨迹\n",
    "def get_yr(aim_value,current_value,alpha,P):\n",
    "    # 生成设定信号\n",
    "    setpoint_signal = np.full(10, aim_value)\n",
    "    # 初始化参数\n",
    "    alpha = alpha\n",
    "    y_r = np.zeros(P)\n",
    "    y_r[0] = current_value\n",
    "    # 模拟一阶模型\n",
    "    for k in range(1,P):\n",
    "        y_r[k] = alpha * y_r[k-1] + (1 - alpha) * aim_value\n",
    "\n",
    "    # # 绘制结果\n",
    "    # plt.plot(setpoint_signal, label='Setpoint Signal')\n",
    "    # plt.plot(y_r,'o-', label='Output Signal (Tracked)')\n",
    "    # plt.legend()\n",
    "    # plt.xlabel('Time')\n",
    "    # plt.ylabel('Amplitude')\n",
    "    # plt.title('Tracking Setpoint Signal with One-Order Model')\n",
    "    # plt.show()\n",
    "    return y_r\n",
    "# 测试\n",
    "y_r = get_yr(1,-0.5,rou,20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "20*40*80/278/20\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_gaolu_train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y_gaolu_train[0:380,0],'.-')\n",
    "plt.plot(y_gaolu_train[0:380,1],'.-')\n",
    "0.30-0.65\n",
    "1485-1535\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_repeated = np.repeat(y_gaolu_test[10:40], repeats=20, axis=0)\n",
    "\n",
    "plt.plot(y_repeated[20:580,0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 生成期望数据\n",
    "\n",
    "def generate_y_aim_data(Times):\n",
    "\n",
    "\n",
    "\n",
    "    # # 打印原始形状\n",
    "    # print(\"Original shape:\", y_predict_test[:10].shape)\n",
    "\n",
    "    # 重复每个数字10次\n",
    "    y_repeated = np.repeat(y_gaolu_train[65:95], repeats=20, axis=0)\n",
    "    # y_repeated = np.repeat(y_gaolu_train[195:225], repeats=20, axis=0)\n",
    "    # y_repeated = np.repeat(y_gaolu_train[0:278], repeats=20, axis=0)\n",
    "\n",
    "    # y_repeated = np.repeat(y_gaolu_test[10:40], repeats=20, axis=0)\n",
    "\n",
    "    # # 打印新形状和内容\n",
    "    # print(\"New shape:\", y_repeated.shape)\n",
    "    # print(y_repeated)\n",
    "    if Times == 200:\n",
    "        set_y1 = np.repeat(np.arange(1455, 1560, 5), 20)[10+75:410-125]\n",
    "        set_y2 = np.repeat(np.arange(0.34, 0.76, 0.02), 20)[0+75:400-125]\n",
    "\n",
    "\n",
    "\n",
    "    elif Times == 400:\n",
    "        set_y1 = np.repeat(np.arange(1455, 1560, 5), 20)[10:410]\n",
    "        set_y2 = np.repeat(np.arange(0.34, 0.76, 0.02), 20)[0:400]\n",
    "        \n",
    "    elif Times == 600:\n",
    "        # y_repeated = np.repeat(y_gaolu_test[10:40], repeats=20, axis=0)\n",
    "        set_y1 = y_repeated[:,0]\n",
    "        set_y2 = y_repeated[:,1]\n",
    "    elif Times == 278*20:\n",
    "        \n",
    "        y_repeated = np.repeat(y_gaolu_train[0:278], repeats=20, axis=0)\n",
    "        set_y1 = y_repeated[:,0]\n",
    "        set_y2 = y_repeated[:,1]\n",
    "\n",
    "    elif Times == 3000:\n",
    "        y_repeated = np.repeat(y_gaolu_train[120:270], repeats=20, axis=0)\n",
    "        set_y1 = y_repeated[:,0]\n",
    "        set_y2 = y_repeated[:,1]\n",
    "    elif Times == 1000:\n",
    "        y_repeated = np.repeat(y_gaolu_train[330:380], repeats=20, axis=0)\n",
    "        set_y1 = y_repeated[:,0]\n",
    "        set_y2 = y_repeated[:,1]\n",
    "\n",
    "    # elif Times == 1000:\n",
    "        # set_y1 = np.repeat(np.arange(1455, 1560, 5), 20)[10:410]\n",
    "        # set_y2 = np.repeat(np.arange(0.34, 0.76, 0.02), 20)[0:400]\n",
    "        # set_y1 = np.repeat(np.arange(1457.5, 1562.5, 5), 20)[10:410]\n",
    "        # set_y2 = np.repeat(np.arange(0.35, 0.77, 0.02), 20)[0:400]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    elif Times == 20*25*80:\n",
    "        \n",
    "        # set_y1: 从 1485 到 1534 的整数，每个数字重复 20*35 次\n",
    "        set_y1 = np.repeat(np.arange(1480, 1560,1), 25 * 20)\n",
    "\n",
    "        # set_y2: 从 0.30 到 0.64 的小数（步长为 0.01），每个数字重复 20 次，再将整个数组重复 50 次\n",
    "        set_y2 = np.repeat(np.arange(0.25, 0.50, 0.01), 20)\n",
    "        set_y2 = np.tile(set_y2, 80)\n",
    "\n",
    "    else:\n",
    "        set_y1 = np.full(Times,1500)\n",
    "        set_y2 = np.full(Times,0.45)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # set_y1_trans = scalers[output_term[0]].transform(set_y1.reshape(-1,1)).flatten()\n",
    "    # set_y2_trans = scalers[output_term[1]].transform(set_y2.reshape(-1,1)).flatten()\n",
    "\n",
    "    set_y1_trans = set_y1\n",
    "    set_y2_trans = set_y2\n",
    "    set_y1 = scalers[output_term[0]].inverse_transform(set_y1_trans.reshape(-1,1)).flatten()\n",
    "    set_y2 = scalers[output_term[1]].inverse_transform(set_y2_trans.reshape(-1,1)).flatten()\n",
    "\n",
    "    return set_y1, set_y2, set_y1_trans, set_y2_trans\n",
    "set_y1, set_y2, set_y1_trans, set_y2_trans = generate_y_aim_data(Times)\n",
    "# print(set_y1.shape)\n",
    "# print(set_y2.shape)\n",
    "# print(set_y1_trans.shape)\n",
    "# print(set_y2_trans.shape)\n",
    "# print(set_y1)\n",
    "# print(set_y2)\n",
    "# print(set_y1_trans)\n",
    "# print(set_y2_trans)\n",
    "plt.plot(set_y1_trans)\n",
    "plt.plot(set_y2_trans)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#生成控制时域的数据格式\n",
    "def generate_k_data(u1_data, u2_data, u3_data, u4_data, y1_data,y2_data, num_samples, P):\n",
    "    nearest_index = np.abs(y1_data - (-0.5)).argmin()\n",
    "    # 生成随机索引值\n",
    "    #从原有数据的randint时刻开始往下进行控制\n",
    "    randint = np.random.randint(1, num_samples - 2 - P - 1)\n",
    "    randint = nearest_index  # 如果你希望使用固定的值而不是随机生成\n",
    "    # randint = 250  # 如果你希望使用固定的值而不是随机生成\n",
    "    print(randint)\n",
    "    # 提取数据并构成 k_data\n",
    "    # 第一次得到下面五个变量，固定好格式构成k_data\n",
    "    u1   = u1_data[randint  :randint+3  ]\n",
    "    u2   = u2_data[randint  :randint+3  ]\n",
    "    u3   = u3_data[randint  :randint+3  ]\n",
    "    u4   = u4_data[randint  :randint+3  ]\n",
    "\n",
    "    y1   = y1_data[randint  :randint+3  ]\n",
    "    y2   = y2_data[randint  :randint+3  ]\n",
    "    k_data = np.concatenate((u1, u2, u3, u4, y1, y2), axis=0)\n",
    "    print(k_data.shape)\n",
    "\n",
    "    k_data = np.zeros_like(k_data)\n",
    "    return k_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 生成高斯噪声,设置随机种子，以便结果可重现\n",
    "# np.random.seed(42)\n",
    "# gaussian_noise_SI = np.random.normal(0,d_yuansu*0.001,Times)\n",
    "# gaussian_noise_TEMP = np.random.normal(0,d_temp*0.1,Times)\n",
    "# # plt.subplot(2, 1, 1)\n",
    "# # plt.plot(gaussian_noise_SI)\n",
    "# # plt.subplot(2, 1, 2)\n",
    "# # plt.plot(gaussian_noise_TEMP)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_model(model_predict,model_gaolu,x,gaolu_data_past_x,gaolu_data_past_y):\n",
    "    y1_pred, y2_pred = model_gaolu.my_predict(x)\n",
    "    y_label = np.column_stack((y1_pred, y2_pred))\n",
    "    gaolu_data_past_x.append(x)\n",
    "    gaolu_data_past_y.append(y_label)\n",
    "\n",
    "    \n",
    "    X_modified = np.array(gaolu_data_past_x)\n",
    "    y_modified = np.array(gaolu_data_past_y)\n",
    "    # print(X_modified)\n",
    "    # print(y_modified)\n",
    "    X = X_modified.reshape(X_modified.shape[0],X_modified.shape[2])\n",
    "    Y = y_modified.reshape(y_modified.shape[0],y_modified.shape[2])\n",
    "    # print(X_modified.shape)\n",
    "    # print(y_modified.shape)\n",
    "    if y_modified.shape[0]% 1 == 0:\n",
    "        model_predict.model_update(X, Y,\n",
    "                                epochs=10, \n",
    "                                batch_size=64,\n",
    "                                lr = 0.002)\n",
    "        # gaolu_data_past_x = []\n",
    "        # gaolu_data_past_y = []\n",
    "\n",
    "    return model_predict,model_gaolu,gaolu_data_past_x,gaolu_data_past_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义单时刻的MPC问题优化\n",
    "def my_MPC(k_data,params,M,P,y1_aim,y2_aim,isprint,ifGAN):\n",
    "\n",
    "    h1 = 1.0\n",
    "    h2 = 1.0\n",
    "    lamda1 = 0.01\n",
    "    lamda2 =lamda1\n",
    "    lamda3 =lamda1\n",
    "    lamda4 =lamda1\n",
    "    y1_percent = 1.0\n",
    "    y2_percent = 1.0\n",
    "\n",
    "    # 从固定格式k_data里面读取信息\n",
    "    u1   = k_data[0:3]\n",
    "    u2   = k_data[3:6]\n",
    "    u3   = k_data[6:9]\n",
    "    u4   = k_data[9:12]\n",
    "\n",
    "    y1   = k_data[12:15]\n",
    "    y2   = k_data[15:18]\n",
    "\n",
    "    \n",
    "    # 获取猜测值[h U1 U2]\n",
    "\n",
    "    # h, U1, U2  =params[0], params[1:M+1],params[M+1:]\n",
    "    if ifGAN:\n",
    "        params = series2U_numpy_in_control(params,M, generated_numpy,\n",
    "                                    scalers_X, scalers,\n",
    "                                    input_term, isprint=False)\n",
    "    U1, U2, U3, U4  =params[0:M], params[M:2*M],params[2*M:3*M], params[3*M:4*M]\n",
    "    \n",
    "    # 整理数据见   MPC推到.escel\n",
    "    u1   = np.concatenate((u1,U1,U1[-1]*np.ones(P-M)))\n",
    "    u2   = np.concatenate((u2,U2,U2[-1]*np.ones(P-M)))\n",
    "    u3   = np.concatenate((u3,U3,U3[-1]*np.ones(P-M)))\n",
    "    u4   = np.concatenate((u4,U4,U4[-1]*np.ones(P-M)))\n",
    "    y1   = np.concatenate((y1,np.zeros(P)))\n",
    "    y2   = np.concatenate((y2,np.zeros(P)))\n",
    "    if isprint:\n",
    "        print(u1.round(4))\n",
    "        print(u2.round(4))\n",
    "        print(u3.round(4))\n",
    "        print(u4.round(4))\n",
    "        print(y1.round(4))    \n",
    "        print(y2.round(4))\n",
    "        print('开始预测')\n",
    "\n",
    "    y1_k = y1[2]\n",
    "    y2_k = y2[2]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # 总共预测 P+1 次\n",
    "    # 对k时刻进行预测-----1次\n",
    "    for j in range(1):   # j = 0\n",
    "        x = np.column_stack((   u1[j+2],u2[j+2],u3[j+2],u4[j+2],\n",
    "                                u1[j+1],u2[j+1],u3[j+1],u4[j+1],\n",
    "                                y1[j+1],y2[j+1]))\n",
    "        # x = x.reshape((x.shape[0], 1, x.shape[1]))\n",
    "        y1_m_k, y2_m_k = model_numpy.my_predict(x)\n",
    "        E1_k = y1_k - y1_m_k\n",
    "        E2_k = y2_k - y2_m_k\n",
    "        if isprint:\n",
    "            print(j,'mode = 0')\n",
    "            print(x.round(4))\n",
    "            print(y1_k.round(4),y2_k.round(4))\n",
    "            print(y1_m_k.round(4),y2_m_k.round(4))\n",
    "\n",
    "    # 对控制时刻进行预测-----M次\n",
    "    for j in range(1,M+1):  # j = 1,2\n",
    "        x = np.column_stack((   u1[j+2],u2[j+2],u3[j+2],u4[j+2],\n",
    "                                u1[j+1],u2[j+1],u3[j+1],u4[j+1],\n",
    "                                y1[j+1],y2[j+1]))\n",
    "        # x = x.reshape((x.shape[0], 1, x.shape[1]))\n",
    "        y1_k_j, y2_k_j = model_numpy.my_predict(x)\n",
    "        y1[j+2] = y1_k_j.item()\n",
    "        y2[j+2] = y2_k_j.item()\n",
    "        if isprint:\n",
    "            print(j,'mode = 1')\n",
    "            print(x.round(4))\n",
    "            print(y1_k_j.round(4),y2_k_j.round(4))\n",
    "            print('更新后:')\n",
    "            print(u1.round(4))\n",
    "            print(u2.round(4))\n",
    "            print(u3.round(4))\n",
    "            print(u4.round(4))\n",
    "            print(u5.round(4))\n",
    "            print(u6.round(4))\n",
    "            print(u7.round(4))\n",
    "            print(y1.round(4))    \n",
    "            print(y2.round(4))\n",
    "\n",
    "    # 对控制时域外的部分进行预测-----P-M次\n",
    "    # 注意：这部分的信号是保持控制不变下进行\n",
    "    for j in range(M+1,P+1):  #j = 3,4\n",
    "        x = np.column_stack((   u1[j+2],u2[j+2],u3[j+2],u4[j+2],\n",
    "                                u1[j+1],u2[j+1],u3[j+1],u4[j+1],\n",
    "                                y1[j+1],y2[j+1]))\n",
    "        # x = x.reshape((x.shape[0], 1, x.shape[1]))\n",
    "        y1_k_j, y2_k_j = model_numpy.my_predict(x)\n",
    "        y1[j+2] = y1_k_j.item()#将预测值作为下一步的输出值\n",
    "        y2[j+2] = y2_k_j.item()\n",
    "        if isprint:\n",
    "            print(j,'mode = 2')\n",
    "            print(x.round(4))\n",
    "            print(y1_k_j.round(4),y2_k_j.round(4))\n",
    "            print('更新后:')\n",
    "            print(u1.round(4))\n",
    "            print(u2.round(4))\n",
    "            print(u3.round(4))\n",
    "            print(u4.round(4))\n",
    "            print(y1.round(4))    \n",
    "            print(y2.round(4))\n",
    "\n",
    "\n",
    "\n",
    "    k_data2 = np.concatenate((u1[1:4],u2[1:4],u3[1:4],u4[1:4],y1[1:4],y2[1:4]),axis=0)\n",
    "    if isprint:\n",
    "        print('更新k_data')\n",
    "        print(k_data2.round(4))\n",
    "\n",
    "\n",
    "    #获取参考轨迹\n",
    "    # 一定要对照好做差的序列\n",
    "    y1_r_aim  = get_yr(y1_aim,y1_k,rou,P+1)\n",
    "    y1_r = y1_r_aim[1:] \n",
    "\n",
    "\n",
    "    y2_r_aim  = get_yr(y2_aim,y2_k,rou,P+1)\n",
    "    y2_r = y2_r_aim[1:] \n",
    "\n",
    "    y1_M_k = y1[3:]\n",
    "    y2_M_k = y2[3:]\n",
    "    if isprint==1:\n",
    "        print('反馈补偿:')\n",
    "        print('y1_k',y1_k.round(4))  \n",
    "        print('y1_m_k',y1_m_k.round(4))    \n",
    "        print('h*E1_k',(h1*E1_k).round(4)) \n",
    "        print('y2_k',y2_k.round(4))  \n",
    "        print('y2_m_k',y2_m_k.round(4))   \n",
    "        print('h*E2_k',(h2*E2_k).round(4))\n",
    "\n",
    "        print('temp:')\n",
    "        print('y1_aim',y1_aim.round(4))\n",
    "        print('y1_r_aim',y1_r_aim.round(4))\n",
    "        print('y1_r',y1_r.round(4))\n",
    "        print('y1_M_k',y1_M_k.round(4))\n",
    "        print('y1_M_k+h1*E1_k',(y1_M_k+h1*E1_k).round(4))\n",
    "\n",
    "        print('Si_percent:')\n",
    "        print('y2_aim',y2_aim.round(4))\n",
    "        print('y2_r_aim',y2_r_aim.round(4))\n",
    "        print('y2_r',y2_r.round(4))\n",
    "        print('y2_M_k',y2_M_k.round(4))\n",
    "        print('y2_M_k+h2*E2_k',(y2_M_k+h2*E2_k).round(4))\n",
    "\n",
    "        print('u:')\n",
    "        print(u1[2:].round(4))\n",
    "        print(u2[2:].round(4))\n",
    "        print(u3[2:].round(4))\n",
    "        print(u4[2:].round(4))\n",
    "        \n",
    "    # 计算mse\n",
    "    # lamda1太大的话会导致y1_r和y1_M_k的误差加大*****************导致超调的原因\\与目标值之间存在间隙\n",
    "\n",
    "\n",
    "    y1_err = y1_percent*np.sum((y1_r-(y1_M_k+h1*E1_k))**2) \n",
    "    y2_err = y2_percent*np.sum((y2_r-(y2_M_k+h2*E2_k))**2) \n",
    "    u1_power = lamda1*np.sum((np.diff(u1[2:]))**2)\n",
    "    u2_power = lamda2*np.sum((np.diff(u2[2:]))**2)\n",
    "    u3_power = lamda3*np.sum((np.diff(u3[2:]))**2)\n",
    "    u4_power = lamda4*np.sum((np.diff(u4[2:]))**2)\n",
    "\n",
    "    # y1_err = y1_percent*np.sum(np.fabs(y1_r-(y1_M_k+h1*E1_k))) \n",
    "    # y2_err = y2_percent*np.sum(np.fabs(y2_r-(y2_M_k+h2*E2_k))) \n",
    "    # u1_power = lamda1*np.sum((np.fabs(np.diff(u1))))\n",
    "    # u2_power = lamda2*np.sum((np.fabs(np.diff(u2))))\n",
    "    # u3_power = lamda3*np.sum((np.fabs(np.diff(u3))))\n",
    "    # u4_power = lamda4*np.sum((np.fabs(np.diff(u4))))\n",
    "    # u5_power = lamda2*np.sum((np.fabs(np.diff(u5))))\n",
    "    # u6_power = lamda3*np.sum((np.fabs(np.diff(u6))))\n",
    "    # u7_power = lamda4*np.sum((np.fabs(np.diff(u7))))\n",
    "\n",
    "    mse = (0\n",
    "            +y1_err\n",
    "            +y2_err\n",
    "            +u1_power\n",
    "            +u2_power\n",
    "            +u3_power\n",
    "            +u4_power\n",
    "            )\n",
    "    \n",
    "    # print('mse {:.7f}'.format(mse))\n",
    "    if isprint==1:\n",
    "        print('mse {:.7f}'.format(mse))\n",
    "        print('1111 {:.7f}'.format(y1_err))\n",
    "        print('2222 {:.7f}'.format(y2_err))\n",
    "        print('1111 {:.7f}'.format(u1_power))\n",
    "        print('2222 {:.7f}'.format(u2_power))\n",
    "        print('3333 {:.7f}'.format(u3_power))\n",
    "        print('4444 {:.7f}'.format(u4_power))\n",
    "\n",
    "\n",
    "\n",
    "    return mse , k_data2, E1_k*h1,  E2_k*h2\n",
    "    # return mse , k_data2, E1_k*h1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import Polygon, box\n",
    "from shapely.geometry import Point\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_predict = copy.deepcopy(model_temp)\n",
    "for name, params in model_predict.named_parameters():\n",
    "    print(name, params.data.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 选择要绘制的多边形\n",
    "# key = ('冷风流量', '热风温度')\n",
    "\n",
    "# # 确保 expanded_hulls_data 中的值是 Polygon 对象的列表\n",
    "# polygons = expanded_hulls_data[key]\n",
    "\n",
    "# # 创建正方形边界（范围 [-1, 1]）\n",
    "# boundary_box = box(0.5, 0.5, 1, 1)\n",
    "\n",
    "# # 创建裁剪后的多边形\n",
    "# clipped_polygons = [polygon.intersection(boundary_box) for polygon in polygons if not polygon.intersection(boundary_box).is_empty]\n",
    "\n",
    "# x_hull, y_hull = polygons[0].exterior.xy\n",
    "# plt.fill(x_hull, y_hull, alpha=0.3, edgecolor='black')\n",
    "# x_hull, y_hull = clipped_polygons[0].exterior.xy\n",
    "# plt.fill(x_hull, y_hull, alpha=0.3, edgecolor='black')\n",
    "# plt.xlim([-1,1])\n",
    "# plt.ylim([-1,1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "penalty_weight = 1.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if_zimotai = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对未来Times周期预测控制\n",
    "max_control = 1.0\n",
    "# 期望设定值\n",
    "set_y1, set_y2, set_y1_trans, set_y2_trans = generate_y_aim_data(Times)\n",
    "\n",
    "# MPC参数\n",
    "P = 3  # 预测时域长度  3\n",
    "M = 3  # 4\n",
    "#生成控制时域的数据格式\n",
    "k_data = generate_k_data(u1_data, u2_data, u3_data, u4_data,\n",
    "                        y1_data, y2_data, num_samples, P)\n",
    "\n",
    "model_predict = copy.deepcopy(model_temp)\n",
    "\n",
    "# MPC控制循环   迭代的只有：k_data\n",
    "all_pred_y1 = []\n",
    "all_pred_y2 = []\n",
    "all_pred_u1 = []\n",
    "all_pred_u2 = []\n",
    "all_pred_u3 = []\n",
    "all_pred_u4 = []\n",
    "all_state = []\n",
    "# 初始化一个最大长度为10的deque\n",
    "gaolu_data_past_x = deque(maxlen=maxlen)\n",
    "gaolu_data_past_y = deque(maxlen=maxlen)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# MPC控制循环4010\n",
    "for k in range(Times):\n",
    "    if iscontrol == False:\n",
    "        break\n",
    "\n",
    "    print(f\"这是对第{k}时刻的最优U1、U2输入求解\")\n",
    "\n",
    "\n",
    "    # 定义优化目标函数\n",
    "    def objective_function(params, *k_data):\n",
    "        mse, k_data2, E1_k_0, E2_k_0 = my_MPC(k_data=k_data[0], params=params, \n",
    "                                M=M, P=P, \n",
    "                                y1_aim = set_y1_trans[k], y2_aim = set_y2_trans[k],\n",
    "                                isprint = 0,ifGAN = ifGAN) \n",
    "        return mse \n",
    "        # return mse\n",
    "\n",
    "\n",
    "\n",
    "    # 初始猜测值[h U1 U2]   定义参数的上下限    设置退出条件\n",
    "    if ifGAN:\n",
    "        params = np.random.randn(z_dim * M)\n",
    "        bounds = [(-max_control, max_control) for _ in range(z_dim * M)]\n",
    "    else:\n",
    "        params = np.concatenate([np.ones(M)*0.9, np.ones(M)*0.9,np.ones(M)*0.9, np.ones(M)*0.9])\n",
    "        # print(params)\n",
    "\n",
    "\n",
    "        if if_zimotai:\n",
    "            # 加入子模态约束\n",
    "            if k == 0:\n",
    "                new_data = np.column_stack([set_y1_trans[k],set_y2_trans[k],0,0,0,0])\n",
    "            else:\n",
    "                new_data = np.column_stack([set_y1_trans[k],set_y2_trans[k],all_pred_u1[-1],all_pred_u2[-1],all_pred_u3[-1],all_pred_u4[-1]])\n",
    "            # print(new_data.shape)\n",
    "            # print(type(new_data))\n",
    "            \n",
    "            label = kmeans.predict(new_data)\n",
    "\n",
    "            print(label)\n",
    "            all_state.append(label)\n",
    "            # print(input_term_limits[label[0]])\n",
    "            # print(input_term_limits[label[0]][input_term[0]][0])\n",
    "            # print(input_term_limits[label[0]][input_term[1]])\n",
    "            # print(input_term_limits[label[0]][input_term[2]])\n",
    "            # print(input_term_limits[label[0]][input_term[3]])\n",
    "\n",
    "\n",
    "\n",
    "            bounds =    [(input_term_limits[label[0]][input_term[0]][0], input_term_limits[label[0]][input_term[0]][1]) for _ in range(M)] + \\\n",
    "                        [(input_term_limits[label[0]][input_term[1]][0], input_term_limits[label[0]][input_term[1]][1]) for _ in range(M)] + \\\n",
    "                        [(input_term_limits[label[0]][input_term[2]][0], input_term_limits[label[0]][input_term[2]][1]) for _ in range(M)] + \\\n",
    "                        [(input_term_limits[label[0]][input_term[3]][0], input_term_limits[label[0]][input_term[3]][1]) for _ in range(M)]\n",
    "\n",
    "        else:\n",
    "            bounds = [(-max_control, max_control) for _ in range(4 * M)]\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    options = {\n",
    "        'maxiter': 1000,      # 最大迭代次数\n",
    "        'disp': True,         # 显示详细的优化过程信息\n",
    "        'factr': 1e-20,       # 调整收敛精度（降低收敛阈值）\n",
    "    }\n",
    "\n",
    "    # 进行优化\n",
    "    result = minimize(objective_function, \n",
    "                    params, \n",
    "                    method='COBYLA', \n",
    "                    args=k_data,\n",
    "                    options=options,\n",
    "                    # constraints=constraints,\n",
    "                    bounds=bounds)  # 添加约束条件\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    if ifGAN:\n",
    "        result_u = series2U_numpy_in_control(np.array(params),M, generated_numpy,\n",
    "                                            scalers_X, scalers,\n",
    "                                            input_term, isprint=False)\n",
    "        U1, U2, U3, U4 =    result_u[0:M], result_u[M:2*M], \\\n",
    "                            result_u[2*M:3*M], result_u[3*M:4*M]\n",
    "    else:\n",
    "        result_u = result.x\n",
    "        U1, U2, U3, U4 =    result_u[0:M], result_u[M:2*M], \\\n",
    "                            result_u[2*M:3*M], result_u[3*M:4*M]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ['富氧流量', '冷风流量', '热风压力', '热风温度']\n",
    "    u1   = k_data[0:3]\n",
    "    u2   = k_data[3:6]\n",
    "    u3   = k_data[6:9]\n",
    "    u4   = k_data[9:12]\n",
    "\n",
    "    y1   = k_data[12:15]\n",
    "    y2   = k_data[15:18]\n",
    "    u1   = np.concatenate((u1,U1,U1[-1]*np.ones(P-M)))\n",
    "    u2   = np.concatenate((u2,U2,U2[-1]*np.ones(P-M)))\n",
    "    u3   = np.concatenate((u3,U3,U3[-1]*np.ones(P-M)))\n",
    "    u4   = np.concatenate((u4,U4,U4[-1]*np.ones(P-M)))\n",
    "    y1   = np.concatenate((y1,np.zeros(P)))\n",
    "    y2   = np.concatenate((y2,np.zeros(P)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # for x, y in zip(U2, U4):\n",
    "    #     point = Point(x, y)\n",
    "    #     if any(point.within(poly) for poly in clipped_polygons):\n",
    "    #         print(f'Point ({x}, {y}) is within the polygons.')\n",
    "    #     else:\n",
    "    #         print(f'Point ({x}, {y}) is outside the polygons.')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # 将控制序列第一个数作用于高炉\n",
    "    j = 1\n",
    "    x = np.column_stack((   u1[j+2],u2[j+2],u3[j+2],u4[j+2],\n",
    "                            u1[j+1],u2[j+1],u3[j+1],u4[j+1],\n",
    "                            y1[j+1],y2[j+1]))\n",
    "    # x = x.reshape((x.shape[0], 1, x.shape[1]))\n",
    "    y1_pred0, y2_pred0 = model_predict.my_predict(x)\n",
    "    if if_gaolu_is_predict:\n",
    "        y1_pred, y2_pred = model_predict.my_predict(x)\n",
    "        if if_add_noise:\n",
    "            # y1_pred = y1_pred+gaussian_noise_TEMP[k].item()\n",
    "            # y2_pred = y2_pred+gaussian_noise_SI[k].item()\n",
    "            y1_pred = y1_pred\n",
    "            y2_pred = y2_pred\n",
    "    else:\n",
    "        y1_pred, y2_pred = model_gaolu.my_predict(x)\n",
    "        if if_update_model:\n",
    "            # if (np.fabs(y2_pred-y2_pred0)<1.5*d_yuansu) & (np.fabs(y1_pred-y1_pred0)<3*d_temp):\n",
    "            #     print('sdgsdegerwh')\n",
    "            model_predict,model_gaolu,gaolu_data_past_x,gaolu_data_past_y = update_model(model_predict,model_gaolu,x,gaolu_data_past_x,gaolu_data_past_y)\n",
    "            # 使用NumPy模型进行预测\n",
    "            model_numpy = MyNeuralNetworkNumpy(model_predict, input_size, hidden_size, output_size,ischuangxin)\n",
    "\n",
    "\n",
    "\n",
    "    # # 更新k_data\n",
    "    \n",
    "    if ifGAN:\n",
    "        params = result.x\n",
    "    else:\n",
    "        params = np.concatenate((U1, U2, U3, U4),axis=0)\n",
    "\n",
    "\n",
    "    mse, k_data2, E1_k_0, E2_k_0 =my_MPC(k_data=k_data,params=np.array(params),\n",
    "                            M=M,P=P, \n",
    "                            y1_aim = set_y1_trans[k], y2_aim = set_y2_trans[k],\n",
    "                            isprint = 0,ifGAN = ifGAN)\n",
    "\n",
    "\n",
    "    print(  '1设定',set_y1_trans[k].round(4),\\\n",
    "            '预测',y1_pred0.round(4),\\\n",
    "            '高炉', y1_pred.round(4),\\\n",
    "            '高炉与设定误差',(set_y1_trans[k]-y1_pred).round(4),(set_y1_trans[k]-y1_pred).round(4)/d_temp,\\\n",
    "            '模型误差',(y1_pred0 - y1_pred).round(4),\\\n",
    "            '校正值',E1_k_0.round(4))\n",
    "    print(  '2设定',set_y2_trans[k].round(4),\\\n",
    "            '预测',y2_pred0.round(4),\\\n",
    "            '高炉', y2_pred.round(4),\\\n",
    "            '高炉与设定误差',(set_y2_trans[k]-y2_pred).round(4),((set_y2_trans[k]-y2_pred).round(4)/d_yuansu)*100,\\\n",
    "            '模型误差',(y2_pred0 - y2_pred).round(4),\\\n",
    "            '校正值',E2_k_0.round(4))\n",
    "\n",
    "\n",
    "\n",
    "    all_pred_y1.append(y1_pred)\n",
    "    all_pred_y2.append(y2_pred)\n",
    "    all_pred_u1.append(U1[0])\n",
    "    all_pred_u2.append(U2[0])\n",
    "    all_pred_u3.append(U3[0])\n",
    "    all_pred_u4.append(U4[0])\n",
    "    k_data2[14] = y1_pred.item()\n",
    "    k_data2[17] = y2_pred.item()\n",
    "    k_data = k_data2\n",
    "    # 进入下一时刻，更新预测时域、控制时域，即k_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "startt = 20\n",
    "endd = startt+150\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "font222 = FontProperties(fname=r\"c:\\windows\\fonts\\simsun.ttc\", size=10)  # 替换为你的中文字体文件路径\n",
    "\n",
    "y1_pred_inverse_transform = scalers[output_term[0]].inverse_transform(np.array(all_pred_y1[startt:endd]).reshape(-1, 1)).flatten()\n",
    "y2_pred_inverse_transform = scalers[output_term[1]].inverse_transform(np.array(all_pred_y2[startt:endd]).reshape(-1, 1)).flatten()\n",
    "all_pred_u1_inverse_transform = scalers[input_term[0]].inverse_transform(np.array(all_pred_u1[startt:endd]).reshape(-1, 1)).flatten()\n",
    "all_pred_u2_inverse_transform = scalers[input_term[1]].inverse_transform(np.array(all_pred_u2[startt:endd]).reshape(-1, 1)).flatten()\n",
    "all_pred_u3_inverse_transform = scalers[input_term[2]].inverse_transform(np.array(all_pred_u3[startt:endd]).reshape(-1, 1)).flatten()\n",
    "all_pred_u4_inverse_transform = scalers[input_term[3]].inverse_transform(np.array(all_pred_u4[startt:endd]).reshape(-1, 1)).flatten()\n",
    "\n",
    "\n",
    "a1 = scalers[input_term[0]].inverse_transform(np.array([1,-1]).reshape(-1, 1)).flatten()\n",
    "a2 = scalers[input_term[1]].inverse_transform(np.array([1,-1]).reshape(-1, 1)).flatten()\n",
    "a3 = scalers[input_term[2]].inverse_transform(np.array([1,-1]).reshape(-1, 1)).flatten()\n",
    "a4 = scalers[input_term[3]].inverse_transform(np.array([1,-1]).reshape(-1, 1)).flatten()\n",
    "print(f'上线分别是：{a1}、{a2}、{a3}、{a4}')\n",
    "\n",
    "\n",
    "rmse_1 = np.mean(np.fabs(set_y1[startt:endd]-y1_pred_inverse_transform))\n",
    "rmse_2 = np.mean(np.fabs(set_y2[startt:endd]-y2_pred_inverse_transform))\n",
    "print('平均误差',rmse_1.round(4))\n",
    "print('平均误差',rmse_2.round(4))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 模型预测控制结果可视化\n",
    "# 创建两个子图，分别绘制每个维度\n",
    "plt.figure(figsize=(8, 13))\n",
    "\n",
    "# 第一个维度的曲线\n",
    "ax = plt.subplot(6, 1, 1)\n",
    "plt.plot(set_y1[startt:endd], 'k.-', label='Set Value')\n",
    "plt.plot(y1_pred_inverse_transform, 'r', label='SC-MLP-U')\n",
    "plt.axvline(x=0, color='gray', linestyle='--', linewidth=1.5)\n",
    "plt.xlim((0,endd-startt))\n",
    "plt.ylabel('MIT/℃', fontproperties=font)  # 使用中文标签\n",
    "plt.legend(prop=font222)\n",
    "plt.grid(linestyle='--', alpha=0.7, color='gray')\n",
    "ax.yaxis.set_label_coords(-0.08, 0.5)  # 固定纵坐标标签在最左边\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "input_term222 =        ['富氧流量/(m\\u00b3/h)', '冷风流量/(m\\u00b3/h)', '热风压力/kPa', '热风温度/℃']\n",
    "output_term222 = ['铁水温度MIT/℃', '铁水硅含量[Si]/%']\n",
    "time_term= '时间戳h'\n",
    "input_term333 =        ['富氧流量', '冷风流量', '热风压力', '热风温度']\n",
    "output_term333 = ['铁水温度MIT', '铁水硅含量[Si]']\n",
    "time_term= '时间戳h'\n",
    "# 用于子图编号的字母序列\n",
    "subplot_labels = ['(a)', '(b)', '(c)', '(d)', '(e)', '(f)', '(g)', '(h)']\n",
    "dtiem = -0.5\n",
    "\n",
    "\n",
    "ax.text(0.5, dtiem, f'{subplot_labels[0]} {output_term333[0]}', \n",
    "            transform=ax.transAxes, ha='center', fontproperties=font)  # 添加每个子图的标题在下方\n",
    "    # 添加横坐标标题\n",
    "ax.set_xlabel('Control cycles', fontproperties=font)  # 只给最下面的子图添加横坐标标签\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 第二个维度的曲线\n",
    "ax = plt.subplot(6, 1, 2)\n",
    "plt.plot(set_y2[startt:endd], 'k.-')\n",
    "plt.plot(y2_pred_inverse_transform, 'r')\n",
    "plt.axvline(x=0, color='gray', linestyle='--', linewidth=1.5)\n",
    "plt.xlim((0,endd-startt))\n",
    "plt.ylabel('[Si]/%', fontproperties=font)  # 使用中文标签\n",
    "plt.grid(linestyle='--', alpha=0.7, color='gray')\n",
    "ax.yaxis.set_label_coords(-0.08, 0.5)  # 固定纵坐标标签在最左边\n",
    "ax.text(0.5, dtiem, f'{subplot_labels[1]} {output_term333[1]}', \n",
    "            transform=ax.transAxes, ha='center', fontproperties=font)  # 添加每个子图的标题在下方\n",
    "    # 添加横坐标标题\n",
    "ax.set_xlabel('Control cycles', fontproperties=font)  # 只给最下面的子图添加横坐标标签\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 第一个维度的u1曲线\n",
    "ax = plt.subplot(6, 1, 3)\n",
    "plt.plot(all_pred_u1_inverse_transform, 'r')\n",
    "plt.axvline(x=0, color='gray', linestyle='--', linewidth=1.5)\n",
    "plt.xlim((0,endd-startt))\n",
    "plt.ylabel(input_term222[0], fontproperties=font)  # 使用中文标签\n",
    "plt.grid(linestyle='--', alpha=0.7, color='gray')\n",
    "ax.yaxis.set_label_coords(-0.08, 0.5)  # 固定纵坐标标签在最左边\n",
    "ax.text(0.5, dtiem, f'{subplot_labels[2]} {input_term333[0]}', \n",
    "            transform=ax.transAxes, ha='center', fontproperties=font)  # 添加每个子图的标题在下方\n",
    "    # 添加横坐标标题\n",
    "ax.set_xlabel('Control cycles', fontproperties=font)  # 只给最下面的子图添加横坐标标签\n",
    "\n",
    "# 第二个维度的u2曲线\n",
    "ax = plt.subplot(6, 1, 4)\n",
    "plt.plot(all_pred_u2_inverse_transform, 'r')\n",
    "plt.axvline(x=0, color='gray', linestyle='--', linewidth=1.5)\n",
    "plt.xlim((0,endd-startt))\n",
    "plt.ylabel(input_term222[1], fontproperties=font)  # 使用中文标签\n",
    "plt.grid(linestyle='--', alpha=0.7, color='gray')\n",
    "ax.yaxis.set_label_coords(-0.08, 0.5)  # 固定纵坐标标签在最左边\n",
    "ax.text(0.5, dtiem, f'{subplot_labels[3]} {input_term333[1]}', \n",
    "            transform=ax.transAxes, ha='center', fontproperties=font)  # 添加每个子图的标题在下方\n",
    "    # 添加横坐标标题\n",
    "ax.set_xlabel('Control cycles', fontproperties=font)  # 只给最下面的子图添加横坐标标签\n",
    "\n",
    "# 第三个维度的u3曲线\n",
    "ax = plt.subplot(6, 1, 5)\n",
    "plt.plot(all_pred_u3_inverse_transform, 'r')  # 修改标签为 'u3'\n",
    "plt.axvline(x=0, color='gray', linestyle='--', linewidth=1.5)\n",
    "plt.xlim((0,endd-startt))\n",
    "plt.ylabel(input_term222[2], fontproperties=font)  # 使用中文标签\n",
    "plt.grid(linestyle='--', alpha=0.7, color='gray')\n",
    "ax.yaxis.set_label_coords(-0.08, 0.5)  # 固定纵坐标标签在最左边\n",
    "ax.text(0.5, dtiem, f'{subplot_labels[4]} {input_term333[2]}', \n",
    "            transform=ax.transAxes, ha='center', fontproperties=font)  # 添加每个子图的标题在下方\n",
    "    # 添加横坐标标题\n",
    "ax.set_xlabel('Control cycles', fontproperties=font)  # 只给最下面的子图添加横坐标标签\n",
    "\n",
    "# 第四个维度的u4曲线\n",
    "ax = plt.subplot(6, 1, 6)\n",
    "plt.plot(all_pred_u4_inverse_transform, 'r')  # 修改标签为 'u4'\n",
    "plt.axvline(x=0, color='gray', linestyle='--', linewidth=1.5)\n",
    "plt.xlim((0,endd-startt))\n",
    "plt.ylabel(input_term222[3], fontproperties=font)  # 使用中文标签\n",
    "plt.grid(linestyle='--', alpha=0.7, color='gray')\n",
    "ax.yaxis.set_label_coords(-0.08, 0.5)  # 固定纵坐标标签在最左边\n",
    "ax.text(0.5, dtiem, f'{subplot_labels[5]} {input_term333[3]}', \n",
    "            transform=ax.transAxes, ha='center', fontproperties=font)  # 添加每个子图的标题在下方\n",
    "    # 添加横坐标标题\n",
    "ax.set_xlabel('Control cycles', fontproperties=font)  # 只给最下面的子图添加横坐标标签\n",
    "\n",
    "\n",
    "\n",
    "# 调整子图布局\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 2))\n",
    "plt.plot(all_state[startt:endd])\n",
    "plt.xlim([0, (endd-startt)])\n",
    "plt.ylabel('子模态标签', fontproperties=font)  # 左侧加注释\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 2))\n",
    "plt.plot(all_state[startt:endd])\n",
    "plt.xlim([0,(endd-startt)])\n",
    "plt.ylabel('子模态标签', fontproperties=font)  # 左侧加注释\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u1_lower, u1_upper = zip(*[(input_term_limits[int(state)][input_term[0]][0],\n",
    "                            input_term_limits[int(state)][input_term[0]][1]) for state in all_state[startt:endd]])\n",
    "u2_lower, u2_upper = zip(*[(input_term_limits[int(state)][input_term[1]][0],\n",
    "                            input_term_limits[int(state)][input_term[1]][1]) for state in all_state[startt:endd]])\n",
    "u3_lower, u3_upper = zip(*[(input_term_limits[int(state)][input_term[2]][0],\n",
    "                            input_term_limits[int(state)][input_term[2]][1]) for state in all_state[startt:endd]])\n",
    "u4_lower, u4_upper = zip(*[(input_term_limits[int(state)][input_term[3]][0],\n",
    "                            input_term_limits[int(state)][input_term[3]][1]) for state in all_state[startt:endd]])\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "time = np.arange(0, endd-startt)\n",
    "\n",
    "# 绘制每个变量的上下限和实际值\n",
    "fig, axes = plt.subplots(4, 1, figsize=(10, 15), sharex=True)\n",
    "\n",
    "# 绘制 u1\n",
    "axes[0].plot(time, all_pred_u1[startt:endd], label='Predicted u1', color='blue')\n",
    "axes[0].fill_between(time, u1_lower, u1_upper, color='lightblue', alpha=0.5, label='u1 Limits')\n",
    "axes[0].set_ylabel('u1 Value')\n",
    "axes[0].legend()\n",
    "\n",
    "# 绘制 u2\n",
    "axes[1].plot(time, all_pred_u2[startt:endd], label='Predicted u2', color='green')\n",
    "axes[1].fill_between(time, u2_lower, u2_upper, color='lightgreen', alpha=0.5, label='u2 Limits')\n",
    "axes[1].set_ylabel('u2 Value')\n",
    "axes[1].legend()\n",
    "\n",
    "# 绘制 u3\n",
    "axes[2].plot(time, all_pred_u3[startt:endd], label='Predicted u3', color='red')\n",
    "axes[2].fill_between(time, u3_lower, u3_upper, color='lightcoral', alpha=0.5, label='u3 Limits')\n",
    "axes[2].set_ylabel('u3 Value')\n",
    "axes[2].legend()\n",
    "\n",
    "# 绘制 u4\n",
    "axes[3].plot(time, all_pred_u4[startt:endd], label='Predicted u4', color='purple')\n",
    "axes[3].fill_between(time, u4_lower, u4_upper, color='plum', alpha=0.5, label='u4 Limits')\n",
    "axes[3].set_ylabel('u4 Value')\n",
    "axes[3].set_xlabel('Time')\n",
    "axes[3].legend()\n",
    "\n",
    "\n",
    "\n",
    "# # 绘制 u4\n",
    "# axes[4].plot(time, all_state[startt:endd], label='Predicted u4', color='purple')\n",
    "# axes[4].set_ylabel('u4 Value')\n",
    "# axes[4].set_xlabel('Time')\n",
    "# axes[4].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "font222 = FontProperties(fname=r\"c:\\windows\\fonts\\simsun.ttc\", size=10)  # 替换为你的中文字体文件路径\n",
    "\n",
    "y1_pred_inverse_transform = scalers[output_term[0]].inverse_transform(np.array(all_pred_y1[startt:endd]).reshape(-1, 1)).flatten()\n",
    "y2_pred_inverse_transform = scalers[output_term[1]].inverse_transform(np.array(all_pred_y2[startt:endd]).reshape(-1, 1)).flatten()\n",
    "all_pred_u1_inverse_transform = scalers[input_term[0]].inverse_transform(np.array(all_pred_u1[startt:endd]).reshape(-1, 1)).flatten()\n",
    "all_pred_u2_inverse_transform = scalers[input_term[1]].inverse_transform(np.array(all_pred_u2[startt:endd]).reshape(-1, 1)).flatten()\n",
    "all_pred_u3_inverse_transform = scalers[input_term[2]].inverse_transform(np.array(all_pred_u3[startt:endd]).reshape(-1, 1)).flatten()\n",
    "all_pred_u4_inverse_transform = scalers[input_term[3]].inverse_transform(np.array(all_pred_u4[startt:endd]).reshape(-1, 1)).flatten()\n",
    "\n",
    "\n",
    "a1 = scalers[input_term[0]].inverse_transform(np.array([1,-1]).reshape(-1, 1)).flatten()\n",
    "a2 = scalers[input_term[1]].inverse_transform(np.array([1,-1]).reshape(-1, 1)).flatten()\n",
    "a3 = scalers[input_term[2]].inverse_transform(np.array([1,-1]).reshape(-1, 1)).flatten()\n",
    "a4 = scalers[input_term[3]].inverse_transform(np.array([1,-1]).reshape(-1, 1)).flatten()\n",
    "print(f'上线分别是：{a1}、{a2}、{a3}、{a4}')\n",
    "\n",
    "\n",
    "rmse_1 = np.mean(np.fabs(set_y1[startt:endd]-y1_pred_inverse_transform))\n",
    "rmse_2 = np.mean(np.fabs(set_y2[startt:endd]-y2_pred_inverse_transform))\n",
    "print('平均误差',rmse_1.round(4))\n",
    "print('平均误差',rmse_2.round(4))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 模型预测控制结果可视化\n",
    "# 创建两个子图，分别绘制每个维度\n",
    "plt.figure(figsize=(8, 13))\n",
    "\n",
    "# 第一个维度的曲线\n",
    "ax = plt.subplot(6, 1, 1)\n",
    "plt.plot(set_y1[startt:endd], 'k.-', label='设定值')\n",
    "plt.plot(y1_pred_inverse_transform, 'r', label='SC-MLP-U')\n",
    "plt.axvline(x=0, color='gray', linestyle='--', linewidth=1.5)\n",
    "plt.xlim((0,endd-startt))\n",
    "plt.ylabel('铁水温度MIT/℃', fontproperties=font)  # 使用中文标签\n",
    "plt.legend(prop=font222)\n",
    "plt.grid(linestyle='--', alpha=0.7, color='gray')\n",
    "ax.yaxis.set_label_coords(-0.07, 0.5)  # 固定纵坐标标签在最左边\n",
    "\n",
    "input_term222 =        ['富氧流量/(m\\u00b3/h)', '冷风流量/(m\\u00b3/h)', '热风压力/kPa', '热风温度/℃']\n",
    "output_term222 = ['铁水温度MIT/℃', '铁水硅含量[Si]/%']\n",
    "time_term= '时间戳h'\n",
    "input_term333 =        ['富氧流量', '冷风流量', '热风压力', '热风温度']\n",
    "output_term333 = ['铁水温度MIT', '铁水硅含量[Si]']\n",
    "time_term= '时间戳h'\n",
    "# 用于子图编号的字母序列\n",
    "subplot_labels = ['(a)', '(b)', '(c)', '(d)', '(e)', '(f)', '(g)', '(h)']\n",
    "dtiem = -0.5\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ax.text(0.5, dtiem, f'{subplot_labels[0]} {output_term333[0]}数据', \n",
    "            transform=ax.transAxes, ha='center', fontproperties=font)  # 添加每个子图的标题在下方\n",
    "    # 添加横坐标标题\n",
    "ax.set_xlabel('控制周期', fontproperties=font)  # 只给最下面的子图添加横坐标标签\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 第二个维度的曲线\n",
    "ax = plt.subplot(6, 1, 2)\n",
    "plt.plot(set_y2[startt:endd], 'k.-')\n",
    "plt.plot(y2_pred_inverse_transform, 'r')\n",
    "plt.axvline(x=0, color='gray', linestyle='--', linewidth=1.5)\n",
    "plt.xlim((0,endd-startt))\n",
    "plt.ylabel('铁水硅含量[Si]/%', fontproperties=font)  # 使用中文标签\n",
    "plt.grid(linestyle='--', alpha=0.7, color='gray')\n",
    "ax.text(0.5, dtiem, f'{subplot_labels[1]} {output_term333[1]}数据', \n",
    "            transform=ax.transAxes, ha='center', fontproperties=font)  # 添加每个子图的标题在下方\n",
    "    # 添加横坐标标题\n",
    "ax.set_xlabel('控制周期', fontproperties=font)  # 只给最下面的子图添加横坐标标签\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 第一个维度的u1曲线\n",
    "ax = plt.subplot(6, 1, 3)\n",
    "plt.plot(all_pred_u1[startt:endd], 'r')\n",
    "plt.axvline(x=0, color='gray', linestyle='--', linewidth=1.5)\n",
    "plt.fill_between(time, u1_lower, u1_upper, color='lightblue', alpha=0.5, label='u1 Limits')\n",
    "plt.xlim((0,endd-startt))\n",
    "plt.ylabel(input_term222[0], fontproperties=font)  # 使用中文标签\n",
    "plt.grid(linestyle='--', alpha=0.7, color='gray')\n",
    "ax.yaxis.set_label_coords(-0.07, 0.5)  # 固定纵坐标标签在最左边\n",
    "ax.text(0.5, dtiem, f'{subplot_labels[2]} {input_term333[0]}数据', \n",
    "            transform=ax.transAxes, ha='center', fontproperties=font)  # 添加每个子图的标题在下方\n",
    "    # 添加横坐标标题\n",
    "ax.set_xlabel('控制周期', fontproperties=font)  # 只给最下面的子图添加横坐标标签\n",
    "\n",
    "# 第二个维度的u2曲线\n",
    "ax = plt.subplot(6, 1, 4)\n",
    "plt.plot(all_pred_u2[startt:endd], 'r')\n",
    "plt.axvline(x=0, color='gray', linestyle='--', linewidth=1.5)\n",
    "plt.fill_between(time, u2_lower, u2_upper, color='lightblue', alpha=0.5, label='u2 Limits')\n",
    "plt.xlim((0,endd-startt))\n",
    "plt.ylabel(input_term222[1], fontproperties=font)  # 使用中文标签\n",
    "plt.grid(linestyle='--', alpha=0.7, color='gray')\n",
    "ax.yaxis.set_label_coords(-0.07, 0.5)  # 固定纵坐标标签在最左边\n",
    "ax.text(0.5, dtiem, f'{subplot_labels[3]} {input_term333[1]}数据', \n",
    "            transform=ax.transAxes, ha='center', fontproperties=font)  # 添加每个子图的标题在下方\n",
    "    # 添加横坐标标题\n",
    "ax.set_xlabel('控制周期', fontproperties=font)  # 只给最下面的子图添加横坐标标签\n",
    "\n",
    "# 第三个维度的u3曲线\n",
    "ax = plt.subplot(6, 1, 5)\n",
    "plt.plot(all_pred_u3[startt:endd], 'r')  # 修改标签为 'u3'\n",
    "plt.axvline(x=0, color='gray', linestyle='--', linewidth=1.5)\n",
    "plt.fill_between(time, u3_lower, u3_upper, color='lightblue', alpha=0.5, label='u3 Limits')\n",
    "plt.xlim((0,endd-startt))\n",
    "plt.ylabel(input_term222[2], fontproperties=font)  # 使用中文标签\n",
    "plt.grid(linestyle='--', alpha=0.7, color='gray')\n",
    "ax.yaxis.set_label_coords(-0.07, 0.5)  # 固定纵坐标标签在最左边\n",
    "ax.text(0.5, dtiem, f'{subplot_labels[4]} {input_term333[2]}数据', \n",
    "            transform=ax.transAxes, ha='center', fontproperties=font)  # 添加每个子图的标题在下方\n",
    "    # 添加横坐标标题\n",
    "ax.set_xlabel('控制周期', fontproperties=font)  # 只给最下面的子图添加横坐标标签\n",
    "\n",
    "# 第四个维度的u4曲线\n",
    "ax = plt.subplot(6, 1, 6)\n",
    "plt.plot(all_pred_u4[startt:endd], 'r')  # 修改标签为 'u4'\n",
    "plt.axvline(x=0, color='gray', linestyle='--', linewidth=1.5)\n",
    "plt.fill_between(time, u4_lower, u4_upper, color='lightblue', alpha=0.5, label='u4 Limits')\n",
    "plt.xlim((0,endd-startt))\n",
    "plt.ylabel(input_term222[3], fontproperties=font)  # 使用中文标签\n",
    "plt.grid(linestyle='--', alpha=0.7, color='gray')\n",
    "ax.yaxis.set_label_coords(-0.07, 0.5)  # 固定纵坐标标签在最左边\n",
    "ax.text(0.5, dtiem, f'{subplot_labels[5]} {input_term333[3]}数据', \n",
    "            transform=ax.transAxes, ha='center', fontproperties=font)  # 添加每个子图的标题在下方\n",
    "    # 添加横坐标标题\n",
    "ax.set_xlabel('控制周期', fontproperties=font)  # 只给最下面的子图添加横坐标标签\n",
    "\n",
    "\n",
    "\n",
    "# 调整子图布局\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frher惹火一天"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(last_input_term)\n",
    "print(output_term)\n",
    "print(input_term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import MeanShift\n",
    "from sklearn.decomposition import KernelPCA\n",
    "# 提取 output_term 和 input_term 的数据\n",
    "def extract_data(data, terms):\n",
    "    return np.column_stack([data[term] for term in terms])\n",
    "\n",
    "# output_data = extract_data(data2_all, output_term+last_output_term)\n",
    "# input_data = extract_data(data2_all, input_term)\n",
    "output_data = extract_data(data2_all, output_term+last_input_term)\n",
    "input_data = extract_data(data2_all, input_term)\n",
    "print(output_data.shape)\n",
    "print(type(output_data))\n",
    "print(f'Output data shape: {output_data.shape}')\n",
    "print(f'Input data shape: {input_data.shape}')\n",
    "\n",
    "\n",
    "\n",
    "# 使用 KernelPCA 将数据映射到高维空间\n",
    "kpca = KernelPCA(n_components=10, kernel='rbf', gamma=0.2)\n",
    "# transformed_data = kpca.fit_transform(output_data)\n",
    "\n",
    "\n",
    "# 使用 K-Means 对 output_term 进行聚类5\n",
    "n_clusters = 12\n",
    "# random_state = 42  # 固定随机数种子\n",
    "kmeans = KMeans(n_clusters=n_clusters)\n",
    "# 使用 K-Means 对 output_term 进行聚类5\n",
    "# n_clusters = len(set(labels))\n",
    "# # random_state = 42  # 固定随机数种子\n",
    "# kmeans = MeanShift()\n",
    "labels = kmeans.fit_predict(output_data)\n",
    "\n",
    "# n_clusters = len(set(labels))\n",
    "\n",
    "# 计算每个子模态的 input_term 空间分布范围\n",
    "def compute_limits(input_data, labels, n_clusters):\n",
    "    limits = {}\n",
    "    for i in range(n_clusters):\n",
    "        cluster_data = input_data[labels == i]\n",
    "        min_limits = cluster_data.min(axis=0)\n",
    "        max_limits = cluster_data.max(axis=0)\n",
    "        print(min_limits)\n",
    "        print(max_limits)\n",
    "        # limits[i] = {input_term[j]: (min_limits[j], max_limits[j]) for j in range(len(input_term))}\n",
    "        limits[i] = {input_term[j]: (min_limits[j], max_limits[j]) for j in range(len(input_term))}\n",
    "    return limits\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "input_term_limits = compute_limits(input_data, labels, n_clusters)\n",
    "\n",
    "# 输出每个子模态的 input_term 限制范围\n",
    "def print_limits(limits):\n",
    "    for mode, terms in limits.items():\n",
    "        print(f\"Mode {mode}:\")\n",
    "        for term, (min_val, max_val) in terms.items():\n",
    "            print(f\"  {term}: Min={min_val}, Max={max_val}\")\n",
    "\n",
    "print_limits(input_term_limits)\n",
    "\n",
    "\n",
    "def plot_clusters_pairwise(output_data, labels, terms):\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    n_clusters = len(set(labels))  # 假设labels是从0开始编号的簇标签\n",
    "    num_terms = len(terms)\n",
    "\n",
    "    # 遍历所有的维度组合（两两配对）\n",
    "    for i in range(num_terms):\n",
    "        for j in range(i+1, num_terms):\n",
    "            plt.subplot(num_terms-1, num_terms-1, i*(num_terms-1) + j)\n",
    "            for k in range(n_clusters):\n",
    "                cluster_data = output_data[labels == k]\n",
    "                plt.scatter(cluster_data[:, i], cluster_data[:, j], s=5, label=f'Mode {k}')\n",
    "            plt.title(f'{terms[i]} vs {terms[j]}', fontproperties=font)\n",
    "            plt.xlabel(terms[i], fontproperties=font)\n",
    "            plt.ylabel(terms[j], fontproperties=font)\n",
    "            # plt.legend()\n",
    "            plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 假设output_term和last_output_term组合在一起\n",
    "# terms = output_term + last_output_term\n",
    "terms = output_term+last_input_term\n",
    "plot_clusters_pairwise(output_data, labels, terms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看 labels 的种类及数量\n",
    "unique_labels = np.unique(labels)\n",
    "print(f\"Unique labels: {unique_labels}\")\n",
    "print(f\"Number of unique labels: {len(unique_labels)}\")\n",
    "# 统计每个标签出现的次数\n",
    "label_counts = np.bincount(labels)\n",
    "for label, count in enumerate(label_counts):\n",
    "    print(f\"Label {label}: {count} points\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from sklearn.cluster import KMeans\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.decomposition import KernelPCA\n",
    "# # 提取 output_term 和 input_term 的数据\n",
    "# def extract_data(data, terms):\n",
    "#     return np.column_stack([data[term] for term in terms])\n",
    "\n",
    "# output_data = extract_data(data2_all, output_term+last_output_term)\n",
    "# input_data = extract_data(data2_all, input_term)\n",
    "# print(output_data.shape)\n",
    "# print(type(output_data))\n",
    "# print(f'Output data shape: {output_data.shape}')\n",
    "# print(f'Input data shape: {input_data.shape}')\n",
    "\n",
    "\n",
    "\n",
    "# # 使用 KernelPCA 将数据映射到高维空间\n",
    "# # kpca = KernelPCA(n_components=10, kernel='rbf', gamma=0.1)\n",
    "# # transformed_data = kpca.fit_transform(output_data)\n",
    "\n",
    "\n",
    "\n",
    "# # 使用 K-Means 对 output_term 进行聚类5\n",
    "# n_clusters = 5\n",
    "# # random_state = 42  # 固定随机数种子\n",
    "# kmeans = KMeans(n_clusters=n_clusters)\n",
    "# labels = kmeans.fit_predict(output_data)\n",
    "\n",
    "# # 计算每个子模态的 input_term 空间分布范围\n",
    "# def compute_limits(input_data, labels, n_clusters):\n",
    "#     limits = {}\n",
    "#     for i in range(n_clusters):\n",
    "#         cluster_data = input_data[labels == i]\n",
    "#         min_limits = cluster_data.min(axis=0)\n",
    "#         max_limits = cluster_data.max(axis=0)\n",
    "#         print(min_limits)\n",
    "#         print(max_limits)\n",
    "#         limits[i] = {input_term[j]: (min_limits[j], max_limits[j]) for j in range(len(input_term))}\n",
    "#     return limits\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# input_term_limits = compute_limits(input_data, labels, n_clusters)\n",
    "\n",
    "# # 输出每个子模态的 input_term 限制范围\n",
    "# def print_limits(limits):\n",
    "#     for mode, terms in limits.items():\n",
    "#         print(f\"Mode {mode}:\")\n",
    "#         for term, (min_val, max_val) in terms.items():\n",
    "#             print(f\"  {term}: Min={min_val}, Max={max_val}\")\n",
    "\n",
    "# print_limits(input_term_limits)\n",
    "\n",
    "\n",
    "# def plot_clusters_pairwise(output_data, labels, terms):\n",
    "#     plt.figure(figsize=(12, 12))\n",
    "#     n_clusters = len(set(labels))  # 假设labels是从0开始编号的簇标签\n",
    "#     num_terms = len(terms)\n",
    "\n",
    "#     # 遍历所有的维度组合（两两配对）\n",
    "#     for i in range(num_terms):\n",
    "#         for j in range(i+1, num_terms):\n",
    "#             plt.subplot(num_terms-1, num_terms-1, i*(num_terms-1) + j)\n",
    "#             for k in range(n_clusters):\n",
    "#                 cluster_data = output_data[labels == k]\n",
    "#                 plt.scatter(cluster_data[:, i], cluster_data[:, j], s=5, label=f'Mode {k}')\n",
    "#             plt.title(f'{terms[i]} vs {terms[j]} space', fontproperties=font)\n",
    "#             plt.xlabel(terms[i], fontproperties=font)\n",
    "#             plt.ylabel(terms[j], fontproperties=font)\n",
    "#             plt.legend()\n",
    "#             plt.grid(True)\n",
    "\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "# # 假设output_term和last_output_term组合在一起\n",
    "# terms = output_term + last_output_term\n",
    "# plot_clusters_pairwise(output_data, labels, terms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# 可视化不同标签下 input_term 数据的二维分布，并绘制边界框\n",
    "def plot_input_term_distribution(ax, input_data, labels, input_term, x_index=0, y_index=1, limits=None, n_clusters=12):\n",
    "    # 使用颜色映射为每个簇生成不同的颜色\n",
    "    colors = plt.cm.get_cmap('tab20', n_clusters)  # 选用 'tab20' 调色板，也可选其他如 'viridis', 'plasma'\n",
    "    \n",
    "    for i in range(n_clusters):\n",
    "        cluster_data = input_data[labels == i]\n",
    "        \n",
    "        # 从 colormap 中获取当前簇的颜色\n",
    "        color = colors(i)\n",
    "        \n",
    "        # 在传入的 ax 上绘制散点\n",
    "        ax.scatter(cluster_data[:, x_index], cluster_data[:, y_index], s=10, label=f'Mode {i}', color=color)\n",
    "        \n",
    "        # 如果提供了边界条件，绘制边界框\n",
    "        if limits is not None and i in limits:\n",
    "            x_min, x_max = limits[i][input_term[x_index]]\n",
    "            y_min, y_max = limits[i][input_term[y_index]]\n",
    "            \n",
    "            # 绘制矩形，颜色与散点颜色一致\n",
    "            rect = plt.Rectangle((x_min, y_min), x_max - x_min, y_max - y_min, \n",
    "                                 linewidth=2, edgecolor=color, facecolor='none', label=f'Boundary {i}')\n",
    "            ax.add_patch(rect)\n",
    "    \n",
    "    ax.set_title(f'{input_term[x_index]} vs {input_term[y_index]}',fontproperties = font)\n",
    "    ax.set_xlabel(input_term[x_index],fontproperties = font)\n",
    "    ax.set_ylabel(input_term[y_index],fontproperties = font)\n",
    "    ax.grid(True)\n",
    "\n",
    "# 获取 input_term 的数量\n",
    "# num_terms = len(input_term)\n",
    "num_terms = len(input_term)\n",
    "\n",
    "# 创建 num_terms x num_terms 的子图\n",
    "fig, axes = plt.subplots(num_terms, num_terms, figsize=(num_terms*5, num_terms*5))\n",
    "fig.tight_layout(pad=3.0)\n",
    "\n",
    "# 遍历所有的 (i, j) 组合，分别绘制在各自的子图上\n",
    "for i in range(num_terms):\n",
    "    for j in range(num_terms):\n",
    "        ax = axes[i, j]  # 获取对应的子图\n",
    "        if i == j :\n",
    "            continue\n",
    "        # plot_input_term_distribution(ax, input_data, labels, input_term, \n",
    "        #                              x_index=i, y_index=j, \n",
    "        #                              limits=input_term_limits, \n",
    "        #                              n_clusters=n_clusters)\n",
    "        plot_input_term_distribution(ax, input_data, labels, input_term, \n",
    "                                     x_index=i, y_index=j, \n",
    "                                     limits=input_term_limits, \n",
    "                                     n_clusters=n_clusters)\n",
    "\n",
    "# 显示所有子图\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(4, 1, figsize=(10, 10))\n",
    "\n",
    "for i in range(len(input_term)):\n",
    "    plt.subplot(4, 1, i + 1)\n",
    "    \n",
    "    # 绘制每个簇的数据\n",
    "    for j in range(n_clusters):\n",
    "        cluster_data = input_data[labels == j]\n",
    "        plt.plot(cluster_data[:, i], label=f'Cluster {j}')  # 使用标签显示簇编号\n",
    "    \n",
    "    # 添加坐标轴标签和标题\n",
    "    plt.xlabel('Index')  # x 轴是索引或时间步长\n",
    "    plt.ylabel(input_term[i],fontproperties = font)  # y 轴是当前的 input_term 项\n",
    "    plt.title(f'{input_term[i]} Distribution Across Clusters',fontproperties = font) # 标题根据 input_term 的名称设定\n",
    "    plt.legend()  # 添加图例\n",
    "    plt.grid(True)  # 显示网格\n",
    "\n",
    "plt.tight_layout()  # 自动调整子图布局以避免重叠\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(4, 1, figsize=(10, 10))\n",
    "for i in range(0,len(input_term)):\n",
    "    plt.subplot(4,1,i+1)\n",
    "    for j in range(0,n_clusters):\n",
    "        cluster_data = input_data[labels == j]\n",
    "        # print(cluster_data[:,0].shape)\n",
    "        plt.plot(cluster_data[:,i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(last_input_term)\n",
    "print(output_term)\n",
    "print(input_term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import MeanShift\n",
    "from sklearn.decomposition import KernelPCA\n",
    "# 提取 output_term 和 input_term 的数据\n",
    "def extract_data(data, terms):\n",
    "    return np.column_stack([data[term] for term in terms])\n",
    "\n",
    "# output_data = extract_data(data2_all, output_term+last_output_term)\n",
    "# input_data = extract_data(data2_all, input_term)\n",
    "output_data = extract_data(data2_all, output_term+last_input_term)\n",
    "input_data = extract_data(data2_all, input_term)\n",
    "print(output_data.shape)\n",
    "print(type(output_data))\n",
    "print(f'Output data shape: {output_data.shape}')\n",
    "print(f'Input data shape: {input_data.shape}')\n",
    "\n",
    "\n",
    "\n",
    "# 使用 KernelPCA 将数据映射到高维空间\n",
    "kpca = KernelPCA(n_components=10, kernel='rbf', gamma=0.2)\n",
    "# transformed_data = kpca.fit_transform(output_data)\n",
    "\n",
    "\n",
    "# 使用 K-Means 对 output_term 进行聚类5\n",
    "n_clusters = 12\n",
    "# random_state = 42  # 固定随机数种子\n",
    "kmeans = KMeans(n_clusters=n_clusters)\n",
    "# 使用 K-Means 对 output_term 进行聚类5\n",
    "# n_clusters = len(set(labels))\n",
    "# # random_state = 42  # 固定随机数种子\n",
    "# kmeans = MeanShift()\n",
    "labels = kmeans.fit_predict(output_data)\n",
    "\n",
    "# n_clusters = len(set(labels))\n",
    "\n",
    "# 计算每个子模态的 input_term 空间分布范围\n",
    "def compute_limits(input_data, labels, n_clusters):\n",
    "    limits = {}\n",
    "    for i in range(n_clusters):\n",
    "        cluster_data = input_data[labels == i]\n",
    "        min_limits = cluster_data.min(axis=0)\n",
    "        max_limits = cluster_data.max(axis=0)\n",
    "        print(min_limits)\n",
    "        print(max_limits)\n",
    "        # limits[i] = {input_term[j]: (min_limits[j], max_limits[j]) for j in range(len(input_term))}\n",
    "        limits[i] = {input_term[j]: (min_limits[j], max_limits[j]) for j in range(len(input_term))}\n",
    "    return limits\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "input_term_limits = compute_limits(input_data, labels, n_clusters)\n",
    "\n",
    "# 输出每个子模态的 input_term 限制范围\n",
    "def print_limits(limits):\n",
    "    for mode, terms in limits.items():\n",
    "        print(f\"Mode {mode}:\")\n",
    "        for term, (min_val, max_val) in terms.items():\n",
    "            print(f\"  {term}: Min={min_val}, Max={max_val}\")\n",
    "\n",
    "print_limits(input_term_limits)\n",
    "\n",
    "\n",
    "def plot_clusters_pairwise(output_data, labels, terms):\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    n_clusters = len(set(labels))  # 假设labels是从0开始编号的簇标签\n",
    "    num_terms = len(terms)\n",
    "\n",
    "    # 遍历所有的维度组合（两两配对）\n",
    "    for i in range(num_terms):\n",
    "        for j in range(i+1, num_terms):\n",
    "            plt.subplot(num_terms-1, num_terms-1, i*(num_terms-1) + j)\n",
    "            for k in range(n_clusters):\n",
    "                cluster_data = output_data[labels == k]\n",
    "                plt.scatter(cluster_data[:, i], cluster_data[:, j], s=5, label=f'Mode {k}')\n",
    "            plt.title(f'{terms[i]} vs {terms[j]}', fontproperties=font)\n",
    "            plt.xlabel(terms[i], fontproperties=font)\n",
    "            plt.ylabel(terms[j], fontproperties=font)\n",
    "            # plt.legend()\n",
    "            plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 假设output_term和last_output_term组合在一起\n",
    "# terms = output_term + last_output_term\n",
    "terms = output_term+last_input_term\n",
    "plot_clusters_pairwise(output_data, labels, terms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看 labels 的种类及数量\n",
    "unique_labels = np.unique(labels)\n",
    "print(f\"Unique labels: {unique_labels}\")\n",
    "print(f\"Number of unique labels: {len(unique_labels)}\")\n",
    "# 统计每个标签出现的次数\n",
    "label_counts = np.bincount(labels)\n",
    "for label, count in enumerate(label_counts):\n",
    "    print(f\"Label {label}: {count} points\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from sklearn.cluster import KMeans\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.decomposition import KernelPCA\n",
    "# # 提取 output_term 和 input_term 的数据\n",
    "# def extract_data(data, terms):\n",
    "#     return np.column_stack([data[term] for term in terms])\n",
    "\n",
    "# output_data = extract_data(data2_all, output_term+last_output_term)\n",
    "# input_data = extract_data(data2_all, input_term)\n",
    "# print(output_data.shape)\n",
    "# print(type(output_data))\n",
    "# print(f'Output data shape: {output_data.shape}')\n",
    "# print(f'Input data shape: {input_data.shape}')\n",
    "\n",
    "\n",
    "\n",
    "# # 使用 KernelPCA 将数据映射到高维空间\n",
    "# # kpca = KernelPCA(n_components=10, kernel='rbf', gamma=0.1)\n",
    "# # transformed_data = kpca.fit_transform(output_data)\n",
    "\n",
    "\n",
    "\n",
    "# # 使用 K-Means 对 output_term 进行聚类5\n",
    "# n_clusters = 5\n",
    "# # random_state = 42  # 固定随机数种子\n",
    "# kmeans = KMeans(n_clusters=n_clusters)\n",
    "# labels = kmeans.fit_predict(output_data)\n",
    "\n",
    "# # 计算每个子模态的 input_term 空间分布范围\n",
    "# def compute_limits(input_data, labels, n_clusters):\n",
    "#     limits = {}\n",
    "#     for i in range(n_clusters):\n",
    "#         cluster_data = input_data[labels == i]\n",
    "#         min_limits = cluster_data.min(axis=0)\n",
    "#         max_limits = cluster_data.max(axis=0)\n",
    "#         print(min_limits)\n",
    "#         print(max_limits)\n",
    "#         limits[i] = {input_term[j]: (min_limits[j], max_limits[j]) for j in range(len(input_term))}\n",
    "#     return limits\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# input_term_limits = compute_limits(input_data, labels, n_clusters)\n",
    "\n",
    "# # 输出每个子模态的 input_term 限制范围\n",
    "# def print_limits(limits):\n",
    "#     for mode, terms in limits.items():\n",
    "#         print(f\"Mode {mode}:\")\n",
    "#         for term, (min_val, max_val) in terms.items():\n",
    "#             print(f\"  {term}: Min={min_val}, Max={max_val}\")\n",
    "\n",
    "# print_limits(input_term_limits)\n",
    "\n",
    "\n",
    "# def plot_clusters_pairwise(output_data, labels, terms):\n",
    "#     plt.figure(figsize=(12, 12))\n",
    "#     n_clusters = len(set(labels))  # 假设labels是从0开始编号的簇标签\n",
    "#     num_terms = len(terms)\n",
    "\n",
    "#     # 遍历所有的维度组合（两两配对）\n",
    "#     for i in range(num_terms):\n",
    "#         for j in range(i+1, num_terms):\n",
    "#             plt.subplot(num_terms-1, num_terms-1, i*(num_terms-1) + j)\n",
    "#             for k in range(n_clusters):\n",
    "#                 cluster_data = output_data[labels == k]\n",
    "#                 plt.scatter(cluster_data[:, i], cluster_data[:, j], s=5, label=f'Mode {k}')\n",
    "#             plt.title(f'{terms[i]} vs {terms[j]} space', fontproperties=font)\n",
    "#             plt.xlabel(terms[i], fontproperties=font)\n",
    "#             plt.ylabel(terms[j], fontproperties=font)\n",
    "#             plt.legend()\n",
    "#             plt.grid(True)\n",
    "\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "# # 假设output_term和last_output_term组合在一起\n",
    "# terms = output_term + last_output_term\n",
    "# plot_clusters_pairwise(output_data, labels, terms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# 可视化不同标签下 input_term 数据的二维分布，并绘制边界框\n",
    "def plot_input_term_distribution(ax, input_data, labels, input_term, x_index=0, y_index=1, limits=None, n_clusters=12):\n",
    "    # 使用颜色映射为每个簇生成不同的颜色\n",
    "    colors = plt.cm.get_cmap('tab20', n_clusters)  # 选用 'tab20' 调色板，也可选其他如 'viridis', 'plasma'\n",
    "    \n",
    "    for i in range(n_clusters):\n",
    "        cluster_data = input_data[labels == i]\n",
    "        \n",
    "        # 从 colormap 中获取当前簇的颜色\n",
    "        color = colors(i)\n",
    "        \n",
    "        # 在传入的 ax 上绘制散点\n",
    "        ax.scatter(cluster_data[:, x_index], cluster_data[:, y_index], s=10, label=f'Mode {i}', color=color)\n",
    "        \n",
    "        # 如果提供了边界条件，绘制边界框\n",
    "        if limits is not None and i in limits:\n",
    "            x_min, x_max = limits[i][input_term[x_index]]\n",
    "            y_min, y_max = limits[i][input_term[y_index]]\n",
    "            \n",
    "            # 绘制矩形，颜色与散点颜色一致\n",
    "            rect = plt.Rectangle((x_min, y_min), x_max - x_min, y_max - y_min, \n",
    "                                 linewidth=2, edgecolor=color, facecolor='none', label=f'Boundary {i}')\n",
    "            ax.add_patch(rect)\n",
    "    \n",
    "    ax.set_title(f'{input_term[x_index]} vs {input_term[y_index]}',fontproperties = font)\n",
    "    ax.set_xlabel(input_term[x_index],fontproperties = font)\n",
    "    ax.set_ylabel(input_term[y_index],fontproperties = font)\n",
    "    ax.grid(True)\n",
    "\n",
    "# 获取 input_term 的数量\n",
    "# num_terms = len(input_term)\n",
    "num_terms = len(input_term)\n",
    "\n",
    "# 创建 num_terms x num_terms 的子图\n",
    "fig, axes = plt.subplots(num_terms, num_terms, figsize=(num_terms*5, num_terms*5))\n",
    "fig.tight_layout(pad=3.0)\n",
    "\n",
    "# 遍历所有的 (i, j) 组合，分别绘制在各自的子图上\n",
    "for i in range(num_terms):\n",
    "    for j in range(num_terms):\n",
    "        ax = axes[i, j]  # 获取对应的子图\n",
    "        if i == j :\n",
    "            continue\n",
    "        # plot_input_term_distribution(ax, input_data, labels, input_term, \n",
    "        #                              x_index=i, y_index=j, \n",
    "        #                              limits=input_term_limits, \n",
    "        #                              n_clusters=n_clusters)\n",
    "        plot_input_term_distribution(ax, input_data, labels, input_term, \n",
    "                                     x_index=i, y_index=j, \n",
    "                                     limits=input_term_limits, \n",
    "                                     n_clusters=n_clusters)\n",
    "\n",
    "# 显示所有子图\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(4, 1, figsize=(10, 10))\n",
    "\n",
    "for i in range(len(input_term)):\n",
    "    plt.subplot(4, 1, i + 1)\n",
    "    \n",
    "    # 绘制每个簇的数据\n",
    "    for j in range(n_clusters):\n",
    "        cluster_data = input_data[labels == j]\n",
    "        plt.plot(cluster_data[:, i], label=f'Cluster {j}')  # 使用标签显示簇编号\n",
    "    \n",
    "    # 添加坐标轴标签和标题\n",
    "    plt.xlabel('Index')  # x 轴是索引或时间步长\n",
    "    plt.ylabel(input_term[i],fontproperties = font)  # y 轴是当前的 input_term 项\n",
    "    plt.title(f'{input_term[i]} Distribution Across Clusters',fontproperties = font) # 标题根据 input_term 的名称设定\n",
    "    plt.legend()  # 添加图例\n",
    "    plt.grid(True)  # 显示网格\n",
    "\n",
    "plt.tight_layout()  # 自动调整子图布局以避免重叠\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(4, 1, figsize=(10, 10))\n",
    "for i in range(0,len(input_term)):\n",
    "    plt.subplot(4,1,i+1)\n",
    "    for j in range(0,n_clusters):\n",
    "        cluster_data = input_data[labels == j]\n",
    "        # print(cluster_data[:,0].shape)\n",
    "        plt.plot(cluster_data[:,i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_result = {\n",
    "    input_term[0]: all_pred_u1[startt:endd],\n",
    "    input_term[1]: all_pred_u2[startt:endd],\n",
    "    input_term[2]: all_pred_u3[startt:endd],\n",
    "    input_term[3]: all_pred_u4[startt:endd],\n",
    "    output_term[0]: all_pred_y1[startt:endd],\n",
    "    output_term[1]: all_pred_y2[startt:endd],\n",
    "    'err1':set_y1[startt:endd]-y1_pred_inverse_transform,\n",
    "    'err2':set_y2[startt:endd]-y2_pred_inverse_transform,\n",
    "}\n",
    "\n",
    "# 将字典转换为 DataFrame\n",
    "df_result = pd.DataFrame(data_result)\n",
    "\n",
    "# # 查看生成的 DataFrame\n",
    "# print(df_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system(r'C:\\Users\\Admin\\Desktop\\11111.mp3')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "优化解的分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    input_term[0]: all_pred_u1[startt:endd],\n",
    "    input_term[1]: all_pred_u2[startt:endd],\n",
    "    input_term[2]: all_pred_u3[startt:endd],\n",
    "    input_term[3]: all_pred_u4[startt:endd],\n",
    "    output_term[0]: all_pred_y1[startt:endd],\n",
    "    output_term[1]: all_pred_y2[startt:endd]\n",
    "}\n",
    "\n",
    "# 将字典转换为 DataFrame\n",
    "df_result = pd.DataFrame(data)\n",
    "\n",
    "# 查看生成的 DataFrame\n",
    "print(df_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(data[output_term[0]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ggg():\n",
    "    i = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = np.column_stack([set_y1_trans[1:],set_y2_trans[1:],set_y1_trans[:-1],set_y2_trans[:-1]])\n",
    "print(new_data.shape)\n",
    "print(type(new_data))\n",
    "new_data\n",
    "label = kmeans.predict(new_data)\n",
    "print(label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = np.column_stack([set_y1_trans,set_y2_trans])\n",
    "print(new_data.shape)\n",
    "print(type(new_data))\n",
    "new_data\n",
    "label = kmeans.predict(new_data)\n",
    "plt.plot(label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(type(output_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# labels = kmeans.predict(y_gaolu_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制散点图矩阵\n",
    "# beilv = 4\n",
    "# plot_scatter_matrix(loaded_data_df_normal, df_result, font=font, figsize=(10*beilv, 8*beilv))\n",
    "\n",
    "# plot_scatter_matrix(X_df_normal, df_result, font=font, figsize=(10*beilv, 8*beilv))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vu与哦i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'C:\\Users\\haokw\\Documents\\GitHub\\gaolu\\up2\\original_data_dict.pkl', 'rb') as f:\n",
    "    loaded_data = pickle.load(f)\n",
    "\n",
    "print(\"Data loaded:\", loaded_data)\n",
    "\n",
    "# 进行归一化\n",
    "loaded_data_dict = {}\n",
    "for column, scaler in scalers.items():\n",
    "    if column in input_term:\n",
    "        loaded_data_dict[column] = scaler.transform(loaded_data[column].reshape(-1, 1)).flatten()\n",
    "\n",
    "loaded_data_df_normal = pd.DataFrame(loaded_data_dict, columns=input_term)\n",
    "\n",
    "loaded_data_df_normal = loaded_data_df_normal.loc[~(loaded_data_df_normal <= -0.9).any(axis=1)].reset_index(drop=True)\n",
    "\n",
    "\n",
    "print(loaded_data_df_normal.shape)\n",
    "\n",
    "loaded_data_df_normal = loaded_data_df_normal.iloc[::80,:].reset_index(drop=True).dropna()#返回第一行\n",
    "check_if_NaN(loaded_data_df_normal)\n",
    "# 绘制散点图矩阵\n",
    "# plot_scatter_matrix(X_df_normal, loaded_data_df_normal, font=font, figsize=(10, 8))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 绘制散点图矩阵\n",
    "# beilv = 1\n",
    "# # plot_scatter_matrix(X_df_normal, df_result, font=font, figsize=(10*beilv, 8*beilv))\n",
    "\n",
    "# plot_scatter_matrix(loaded_data_df_normal, df_result, font=font, figsize=(10*beilv, 8*beilv))\n",
    "# # plot_scatter_matrix(filtered_df, df_result, font=font, figsize=(10*beilv, 8*beilv))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vu与哦i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import DBSCAN\n",
    "from scipy.stats import gaussian_kde\n",
    "from shapely.geometry import MultiPoint, Polygon\n",
    "import matplotlib.font_manager as fm\n",
    "\n",
    "def process_cluster_data(ax, x, y, \n",
    "                         term1, term2, df_result,\n",
    "                         eps=3, min_samples=3, buffer_distance=0.10):\n",
    "    \"\"\"\n",
    "    处理数据，执行聚类，并在指定的轴上绘制聚类结果和扩展边界。\n",
    "\n",
    "    :param ax: matplotlib 的 Axes 对象\n",
    "    :param x: 变量1的值\n",
    "    :param y: 变量2的值\n",
    "    :param eps: DBSCAN 的 eps 参数\n",
    "    :param min_samples: DBSCAN 的 min_samples 参数\n",
    "    :param buffer_distance: 扩展量，用于边界扩展\n",
    "    :param term1: 变量1的名称\n",
    "    :param term2: 变量2的名称\n",
    "    \"\"\"\n",
    "    # 将数据合并为二维数组\n",
    "    data = np.column_stack((x, y))\n",
    "\n",
    "    # 1. 计算每个数据点的密度\n",
    "    kde = gaussian_kde(data.T)\n",
    "    densities = kde(data.T)\n",
    "\n",
    "    # 2. 根据密度排序并选择前100%的数据点\n",
    "    sorted_indices = np.argsort(densities)[::-1]  # 从高到低排序\n",
    "    sorted_data = data[sorted_indices]\n",
    "    sorted_densities = densities[sorted_indices]\n",
    "\n",
    "    # 选择前100%的数据点\n",
    "    threshold_index = int(len(sorted_densities) * 1.00)\n",
    "    top_90_data = sorted_data[:threshold_index]\n",
    "\n",
    "    # 3. 对前90%的数据点进行聚类\n",
    "    dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "    clusters = dbscan.fit_predict(top_90_data)\n",
    "    unique_labels = set(clusters)\n",
    "\n",
    "    # 4. 绘制聚类结果并包含原始数据\n",
    "    # 绘制原始数据\n",
    "    # ax.scatter(data[:, 0], data[:, 1], c='gray', alpha=0.5, label='Original Data')\n",
    "\n",
    "    for label in unique_labels:\n",
    "        if label == -1:\n",
    "            continue  # 忽略噪声点\n",
    "        cluster_points = top_90_data[clusters == label]\n",
    "        # ax.scatter(cluster_points[:, 0], cluster_points[:, 1], label=f\"Original Data\")\n",
    "\n",
    "    # 5. 绘制每个聚类的边界，并在边界上进行外部扩展\n",
    "    hulls = []\n",
    "    # 绘制每个聚类的边界，并在边界上进行外部扩展\n",
    "    for label in unique_labels:\n",
    "        if label == -1:\n",
    "            continue  # 忽略噪声点\n",
    "        cluster_points = top_90_data[clusters == label]\n",
    "        \n",
    "        # 计算聚类的凸包边界\n",
    "        if len(cluster_points) > 2:\n",
    "            convex_hull = MultiPoint(cluster_points).convex_hull\n",
    "        else:\n",
    "            convex_hull = Polygon(cluster_points)\n",
    "            \n",
    "        # 扩展边界（使用 buffer 方法进行外部扩展）\n",
    "        expanded_hull = convex_hull.buffer(buffer_distance)\n",
    "        hulls.append(expanded_hull)\n",
    "        \n",
    "        # 绘制扩展后的边界\n",
    "        x_hull, y_hull = expanded_hull.exterior.xy\n",
    "        ax.fill(x_hull, y_hull, alpha=0.3)\n",
    "        \n",
    "        # 绘制聚类点\n",
    "        # ax.scatter(X_df_normal[term1], X_df_normal[term2], label=f\"Original Data\")\n",
    "        ax.scatter(cluster_points[:, 0], cluster_points[:, 1], label=f\"cluster_points\")\n",
    "        ax.scatter(df_result[term1], df_result[term2], label=f\"Process Data\")\n",
    "        ax.set_xlim([-1, 1])\n",
    "        ax.set_ylim([-1, 1])\n",
    "        \n",
    "\n",
    "    ax.set_xlabel(term1, fontproperties=font)\n",
    "    ax.set_ylabel(term2, fontproperties=font)\n",
    "    ax.set_title(f\"{term1} vs {term2}\", fontproperties=font)\n",
    "    \n",
    "    ax.legend(prop=font)\n",
    "    return {\n",
    "        (term1, term2): hulls\n",
    "    }\n",
    "# {\n",
    "#     ('富氧流量', '设定喷煤量'): [<POLYGON ...>],\n",
    "#     ('富氧流量', '热风压力'): [<POLYGON ...>],\n",
    "#     # 更多数据\n",
    "# }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建一个大的图形\n",
    "fig, axs = plt.subplots(len(input_term), len(input_term), figsize=(10, 10))\n",
    "fig.subplots_adjust(hspace=0.4, wspace=0.4)\n",
    "expanded_hulls_data = {}\n",
    "# 循环处理每一对变量\n",
    "for i in range(len(input_term)):\n",
    "    for j in range(i + 1, len(input_term)):\n",
    "        term1 = input_term[i]\n",
    "        term2 = input_term[j]\n",
    "        \n",
    "        x = loaded_data_df_normal[term1]\n",
    "        y = loaded_data_df_normal[term2]\n",
    "        \n",
    "        ax = axs[i, j]\n",
    "        result = process_cluster_data(ax, x, y, \n",
    "                                      term1, term2, df_result,\n",
    "                                      eps=3, min_samples=3, \n",
    "                                      buffer_distance=0.10)\n",
    "        # 将结果添加到字典中\n",
    "        expanded_hulls_data.update(result)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_scatter_matrix_22222(df_result, df_result, figsize=(10, 8), font=font)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdh而也容易\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded_hulls_data\n",
    "import pickle\n",
    "\n",
    "# 保存 expanded_hulls_data 到文件\n",
    "with open('expanded_hulls_data.pkl', 'wb') as f:\n",
    "    pickle.dump(expanded_hulls_data, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(expanded_hulls_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in expanded_hulls_data.items():\n",
    "    print(f\"Key: {key}, Value: {value}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from shapely.geometry import Polygon, box\n",
    "import numpy as np\n",
    "\n",
    "# 选择要绘制的多边形\n",
    "key = ('冷风流量', '热风温度')\n",
    "\n",
    "# 确保 expanded_hulls_data 中的值是 Polygon 对象的列表\n",
    "polygons = expanded_hulls_data[key]\n",
    "\n",
    "# 创建正方形边界（范围 [-1, 1]）\n",
    "boundary_box = box(-1, -1, 1, 1)\n",
    "\n",
    "\n",
    "# 选择要绘制的多边形\n",
    "key = ('冷风流量', '热风温度')\n",
    "\n",
    "# 确保 expanded_hulls_data 中的值是 Polygon 对象的列表\n",
    "polygons = expanded_hulls_data[key]\n",
    "\n",
    "# 创建正方形边界（范围 [-1, 1]）\n",
    "boundary_box = box(-1, -1, 1, 1)\n",
    "\n",
    "# 创建裁剪后的多边形\n",
    "clipped_polygons = polygon.intersection(boundary_box) for polygon in polygons if not polygon.intersection(boundary_box).is_empty\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 创建图形和坐标轴\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "for polygon in polygons:\n",
    "    if isinstance(polygon, Polygon):\n",
    "        # 计算多边形与正方形的交集\n",
    "        clipped_polygon = polygon.intersection(boundary_box)\n",
    "        \n",
    "        if not clipped_polygon.is_empty:\n",
    "            x_hull, y_hull = clipped_polygon.exterior.xy\n",
    "            ax.fill(x_hull, y_hull, alpha=0.3, edgecolor='black')\n",
    "\n",
    "# 设置坐标轴范围\n",
    "ax.set_xlim([-1, 2])\n",
    "ax.set_ylim([-1, 2])\n",
    "\n",
    "# 设置坐标轴的比例为相等，以防止多边形被扭曲\n",
    "ax.set_aspect('equal')\n",
    "\n",
    "# 设置坐标轴标签和标题\n",
    "plt.xlabel('冷风流量')\n",
    "plt.ylabel('热风温度')\n",
    "plt.title('多边形边界')\n",
    "\n",
    "# 显示图形\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "而我国v热\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 定义列名\n",
    "columns = input_term\n",
    "\n",
    "# 生成随机数据\n",
    "num_samples = 100*10000\n",
    "data = np.random.uniform(-1, 1, size=(num_samples, len(columns)))\n",
    "\n",
    "# 创建 DataFrame\n",
    "random_df = pd.DataFrame(data, columns=columns)\n",
    "\n",
    "# 打印前几行以验证\n",
    "print(random_df.head())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from shapely.geometry import Point\n",
    "\n",
    "# 假设 expanded_hulls_data 是一个包含扩展边界数据的字典\n",
    "# 格式: {'term1_term2': [expanded_hull, ...]}\n",
    "# 示例格式为 {('term1', 'term2'): [expanded_hull, ...]}\n",
    "\n",
    "def filter_points_within_hulls(df, input_terms, expanded_hulls_data):\n",
    "    \"\"\"\n",
    "    根据扩展边界筛选 DataFrame 中的点。\n",
    "\n",
    "    :param df: 包含随机样本的 DataFrame\n",
    "    :param input_terms: 列名列表\n",
    "    :param expanded_hulls_data: 扩展边界数据字典\n",
    "    :return: 筛选后的 DataFrame\n",
    "    \"\"\"\n",
    "    # 创建一个布尔掩码来标记需要保留的行\n",
    "    keep_mask = np.ones(len(df), dtype=bool)\n",
    "    \n",
    "    # 循环处理每一对变量\n",
    "    for i in range(len(input_terms)):\n",
    "        i\n",
    "        for j in range(i + 1, len(input_terms)):\n",
    "            term1 = input_terms[i]\n",
    "            term2 = input_terms[j]\n",
    "            \n",
    "            # 取出数据\n",
    "            x = df[term1]\n",
    "            y = df[term2]\n",
    "            \n",
    "            # 获取对应的扩展边界\n",
    "            hulls = expanded_hulls_data.get((term1, term2), [])\n",
    "            \n",
    "            if not hulls:\n",
    "                continue\n",
    "            \n",
    "            # 计算每个点是否在扩展边界内\n",
    "            def point_in_any_hull(xi, yi):\n",
    "                point = Point(xi, yi)\n",
    "                return any(point.within(hull) for hull in hulls)\n",
    "            \n",
    "            # 应用函数并更新掩码\n",
    "            mask = np.array([point_in_any_hull(xi, yi) for xi, yi in zip(x, y)])\n",
    "            keep_mask &= mask\n",
    "    \n",
    "    # 根据掩码筛选 DataFrame\n",
    "    filtered_df = df[keep_mask]\n",
    "    \n",
    "    return filtered_df\n",
    "\n",
    "# 使用示例\n",
    "filtered_df = filter_points_within_hulls(random_df, input_term, expanded_hulls_data)\n",
    "\n",
    "# 打印筛选后的数据\n",
    "print(filtered_df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存 expanded_hulls_data 到文件\n",
    "with open('filtered_df.pkl', 'wb') as f:\n",
    "    pickle.dump(filtered_df, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from shapely.geometry import Polygon\n",
    "\n",
    "# 函数来绘制边界条件和数据\n",
    "def plot_hulls_and_data(ax, data, term1, term2, expanded_hulls):\n",
    "    \"\"\"\n",
    "    绘制扩展边界和数据点\n",
    "    \n",
    "    :param ax: 绘图的轴对象\n",
    "    :param data: 数据点\n",
    "    :param term1: 变量1的名称\n",
    "    :param term2: 变量2的名称\n",
    "    :param expanded_hulls: 扩展边界的字典\n",
    "    \"\"\"\n",
    "    # 绘制数据点\n",
    "    ax.scatter(data[term1], data[term2], c='blue', label='Data Points', alpha=0.5)\n",
    "    \n",
    "    # 绘制扩展边界\n",
    "    hulls = expanded_hulls.get((term1, term2), [])\n",
    "    for hull in hulls:\n",
    "        if isinstance(hull, Polygon):\n",
    "            x_hull, y_hull = hull.exterior.xy\n",
    "            ax.fill(x_hull, y_hull, alpha=0.3)\n",
    "\n",
    "    ax.set_xlabel(term1, fontproperties=font)\n",
    "    ax.set_ylabel(term2, fontproperties=font)\n",
    "    ax.set_title(f'{term1} vs {term2}', fontproperties=font)\n",
    "    ax.legend()\n",
    "\n",
    "# 创建一个大的图像和子图\n",
    "num_vars = len(input_term)\n",
    "fig, axs = plt.subplots(num_vars, num_vars, figsize=(10, 10))\n",
    "\n",
    "# 遍历每一对变量\n",
    "for i in range(num_vars):\n",
    "    for j in range(num_vars):\n",
    "        if i != j:\n",
    "            term1 = input_term[i]\n",
    "            term2 = input_term[j]\n",
    "\n",
    "            ax = axs[i, j]\n",
    "            \n",
    "            # 过滤数据点在扩展边界内\n",
    "            filtered_df = filtered_df[(filtered_df[term1] >= -1) & (filtered_df[term1] <= 1) &\n",
    "                                        (filtered_df[term2] >= -1) & (filtered_df[term2] <= 1)]\n",
    "\n",
    "            # 绘制边界条件和数据\n",
    "            plot_hulls_and_data(ax, filtered_df, term1, term2, expanded_hulls_data)\n",
    "\n",
    "        else:\n",
    "            axs[i, j].axis('off')  # 关闭对角线上的子图\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GAN超参数  训练数据\n",
    "\n",
    "# 将历史数据转换为 PyTorch 张量\n",
    "data_item = random_df\n",
    "# data_item = data_test\n",
    "\n",
    "data = torch.tensor(data_item.values, dtype=torch.float32)\n",
    "df_GAN = data_item\n",
    "test_sample_num = data.shape[0]\n",
    "\n",
    "\n",
    "\n",
    "z_dim = 4  # 随机噪声维度\n",
    "data_dim = data.shape[1]  # 数据维度，4\n",
    "learning_rate = 0.0002\n",
    "num_epochs = 200\n",
    "n_critic = 5  # 每次更新生成器前，更新 Critic 的次数\n",
    "\n",
    "print_piture_d = 10\n",
    "\n",
    "# 设置数据加载器\n",
    "batch_size = 512\n",
    "data_loader = torch.utils.data.DataLoader(data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# W-GAN模型定义和训练\n",
    "# WGAN的生成器\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(256, output_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# WGAN的Critic\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Critic, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# 权重裁剪\n",
    "def weight_clipping(critic, clip_value=0.01):\n",
    "    for param in critic.parameters():\n",
    "        param.data.clamp_(-clip_value, clip_value)\n",
    "\n",
    "# WGAN训练\n",
    "def train_wgan(generator, critic, data_loader, num_epochs, z_dim, clip_value, n_critic, optimizer_G, optimizer_C, output_dir):\n",
    "    \n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    for epoch in range(num_epochs):\n",
    "        for real_data in data_loader:\n",
    "            batch_size = real_data.size(0)\n",
    "\n",
    "            # 更新 Critic\n",
    "            for _ in range(n_critic):\n",
    "                z = torch.randn(batch_size, z_dim)\n",
    "                fake_data = generator(z)\n",
    "\n",
    "                # 计算 Critic 的损失（Wasserstein 距离）\n",
    "                real_output = critic(real_data)\n",
    "                fake_output = critic(fake_data.detach())\n",
    "                c_loss = -torch.mean(real_output) + torch.mean(fake_output)\n",
    "\n",
    "                optimizer_C.zero_grad()\n",
    "                c_loss.backward()\n",
    "                optimizer_C.step()\n",
    "\n",
    "                # 对 Critic 权重进行裁剪\n",
    "                weight_clipping(critic, clip_value)\n",
    "\n",
    "            # 更新生成器\n",
    "            z = torch.randn(batch_size, z_dim)\n",
    "            fake_data = generator(z)\n",
    "            fake_output = critic(fake_data)\n",
    "            g_loss = -torch.mean(fake_output)\n",
    "\n",
    "            optimizer_G.zero_grad()\n",
    "            g_loss.backward()\n",
    "            optimizer_G.step()\n",
    "\n",
    "        print(f\"WGAN_training Epoch [{epoch}/{num_epochs}], c_loss: {c_loss.item():.4f}, g_loss: {g_loss.item():.4f}\")\n",
    "\n",
    "        if epoch % print_piture_d == print_piture_d-1:\n",
    "            # 生成一些数据\n",
    "            z = torch.randn(test_sample_num, z_dim)\n",
    "            generated_data = generator(z).detach().numpy()\n",
    "\n",
    "            # 将生成的数据转换为 DataFrame\n",
    "            generated_df = pd.DataFrame(generated_data, columns=df_GAN.columns)\n",
    "\n",
    "            # 获取当前时间\n",
    "            current_time = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "            # 构建保存路径和文件名\n",
    "            filename = f\"{current_time}_WGAN_Epoch_{epoch+1}_c_loss_{c_loss.item():.4f}_g_loss_{g_loss.item():.4f}.png\"\n",
    "            save_path = os.path.join(output_dir, filename)\n",
    "\n",
    "            # 可视化生成的数据分布（你可以实现自己的可视化函数）\n",
    "            plot_scatter_matrix(df_GAN, generated_df, figsize=(10, 8), font=font, save_path=save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化WGAN  训练\n",
    "wgan_generator = Generator(input_dim=z_dim, output_dim=data_dim)\n",
    "wgan_critic = Critic(input_dim=data_dim)\n",
    "# 初始化优化器\n",
    "optimizer_G = optim.Adam(wgan_generator.parameters(), lr=learning_rate, betas=(0.2, 0.999))\n",
    "optimizer_C = optim.Adam(wgan_critic.parameters(), lr=learning_rate, betas=(0.2, 0.999))\n",
    "\n",
    "# 训练WGAN\n",
    "train_wgan(wgan_generator, wgan_critic, data_loader, \n",
    "                num_epochs=num_epochs, z_dim=z_dim, clip_value=0.01, \n",
    "                n_critic = 5, \n",
    "                optimizer_G=optimizer_G, optimizer_C=optimizer_C, \n",
    "                output_dir = r\"data\\train_output_picture\\WGAN_all_data\")\n",
    "\n",
    "\n",
    "\n",
    "# 保存模型参数\n",
    "def save_model(generator, discriminator, output_dir, epoch):\n",
    "        # 创建保存目录（如果不存在）\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        # 定义文件名\n",
    "        generator_filename = os.path.join(output_dir, f\"generator.pth\")\n",
    "        discriminator_filename = os.path.join(output_dir, f\"discriminator.pth\")\n",
    "\n",
    "        # 保存生成器的模型参数\n",
    "        torch.save(generator.state_dict(), generator_filename)\n",
    "        print(f\"Generator model saved to {generator_filename}\")\n",
    "\n",
    "        # 保存判别器的模型参数\n",
    "        torch.save(discriminator.state_dict(), discriminator_filename)\n",
    "        print(f\"Discriminator model saved to {discriminator_filename}\")\n",
    "\n",
    "save_model(wgan_generator, wgan_critic, \n",
    "        output_dir=r\"data\\model_params\\wgan_model_all_data\", \n",
    "        epoch=num_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成样本测试\n",
    "z = torch.randn(test_sample_num, z_dim)\n",
    "print(z.shape)\n",
    "\n",
    "generated_data = wgan_generator(z).detach().numpy()\n",
    "print(generated_data.shape)\n",
    "\n",
    "# 将生成的数据转换为 DataFrame\n",
    "generated_df = pd.DataFrame(generated_data, columns=df_GAN.columns)\n",
    "\n",
    "# 可视化生成的数据分布（你可以实现自己的可视化函数）\n",
    "plot_scatter_matrix(df_GAN, generated_df, figsize=(10, 8), font=font)\n",
    "plot_scatter_matrix(generated_df, df_result, figsize=(10, 8), font=font)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LS-GAN模型定义和训练\n",
    "class LSGANGenerator(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LSGANGenerator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            # nn.ReLU(True),\n",
    "            nn.Linear(128, 256),\n",
    "            # nn.ReLU(True),\n",
    "            nn.Linear(256, output_dim),\n",
    "            # nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# LSGAN的判别器\n",
    "class LSGANDiscriminator(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(LSGANDiscriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# # LSGAN的损失函数\n",
    "# def generator_loss(fake_output):\n",
    "#     return torch.mean((fake_output - 1) ** 2)\n",
    "\n",
    "# def discriminator_loss(real_output, fake_output):\n",
    "#     real_loss = torch.mean((real_output - 1) ** 2)\n",
    "#     fake_loss = torch.mean(fake_output ** 2)\n",
    "#     return real_loss + fake_loss\n",
    "\n",
    "\n",
    "# 定义LSGAN损失函数\n",
    "def generator_loss(fake_output, generated_data, real_data):\n",
    "    # LS-GAN 原始损失\n",
    "    lsgan_loss = torch.mean((fake_output - 1) ** 2)\n",
    "    # 生成数据与真实数据的MSE匹配损失\n",
    "    mse_loss = torch.mean((generated_data - real_data) ** 2)\n",
    "    return lsgan_loss + mse_loss\n",
    "\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = torch.mean((real_output - 1) ** 2)\n",
    "    fake_loss = torch.mean(fake_output ** 2)\n",
    "    return real_loss + fake_loss\n",
    "\n",
    "\n",
    "\n",
    "# LSGAN训练\n",
    "def train_lsgan(generator, discriminator, data_loader, num_epochs, z_dim, optimizer_G, optimizer_D, output_dir):\n",
    "    \n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    for epoch in range(num_epochs):\n",
    "        for real_data in data_loader:\n",
    "            batch_size = real_data.size(0)\n",
    "\n",
    "            # 生成数据\n",
    "            z = torch.randn(batch_size, z_dim)\n",
    "            fake_data = generator(z)\n",
    "\n",
    "            # 判别器前向传播\n",
    "            real_output = discriminator(real_data)\n",
    "            fake_output = discriminator(fake_data.detach())\n",
    "\n",
    "            # 计算判别器损失\n",
    "            d_loss = discriminator_loss(real_output, fake_output)\n",
    "            optimizer_D.zero_grad()\n",
    "            d_loss.backward()\n",
    "            optimizer_D.step()\n",
    "\n",
    "            # 生成器前向传播\n",
    "            fake_output = discriminator(fake_data)\n",
    "\n",
    "            # 计算生成器损失\n",
    "            g_loss = generator_loss(fake_output, fake_data, real_data)\n",
    "            optimizer_G.zero_grad()\n",
    "            g_loss.backward()\n",
    "            optimizer_G.step()\n",
    "\n",
    "        print(f\"LS-GAN_training Epoch [{epoch}/{num_epochs}], D_loss: {d_loss.item():.4f}, G_loss: {g_loss.item():.4f}\")\n",
    "\n",
    "        if epoch % print_piture_d == print_piture_d-1:\n",
    "            # 生成数据test_sample_num\n",
    "            z = torch.randn(test_sample_num, z_dim)\n",
    "            generated_data = generator(z).detach().numpy()\n",
    "\n",
    "            # 将生成的数据转换为 DataFrame\n",
    "            generated_df = pd.DataFrame(generated_data, columns=df_GAN.columns)\n",
    "\n",
    "            # 获取当前时间\n",
    "            current_time = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "            # 构建保存路径和文件名\n",
    "            filename = f\"{current_time}_LSGAN_Epoch_{epoch+1}_D_loss_{d_loss.item():.4f}_G_loss_{g_loss.item():.4f}.png\"\n",
    "            save_path = os.path.join(output_dir, filename)\n",
    "\n",
    "            # 可视化生成的数据分布（你可以实现自己的可视化函数）\n",
    "            plot_scatter_matrix(df_GAN, generated_df, figsize=(10, 8), font=font, save_path=save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化LSGAN优化器  训练\n",
    "lsgan_generator = LSGANGenerator(input_dim=z_dim, output_dim=data_dim)\n",
    "lsgan_discriminator = LSGANDiscriminator(input_dim=data_dim)\n",
    "\n",
    "optimizer_G = optim.Adam(lsgan_generator.parameters(), lr=learning_rate, betas=(0.2, 0.999))\n",
    "optimizer_D = optim.Adam(lsgan_discriminator.parameters(), lr=learning_rate, betas=(0.2, 0.999))\n",
    "\n",
    "# 使用WGAN生成的数据作为输入进行LSGAN训练\n",
    "train_lsgan(lsgan_generator, lsgan_discriminator, data_loader, \n",
    "                num_epochs=num_epochs, z_dim=z_dim, \n",
    "                optimizer_G=optimizer_G, optimizer_D=optimizer_D, \n",
    "                output_dir = r\"data\\train_output_picture\\LS-GAN_all_data\")\n",
    "\n",
    "\n",
    "\n",
    "# 保存模型参数\n",
    "def save_model(generator, discriminator, output_dir, epoch):\n",
    "        # 创建保存目录（如果不存在）\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        # 定义文件名\n",
    "        generator_filename = os.path.join(output_dir, f\"generator.pth\")\n",
    "        discriminator_filename = os.path.join(output_dir, f\"discriminator.pth\")\n",
    "\n",
    "        # 保存生成器的模型参数\n",
    "        torch.save(generator.state_dict(), generator_filename)\n",
    "        print(f\"Generator model saved to {generator_filename}\")\n",
    "\n",
    "        # 保存判别器的模型参数\n",
    "        torch.save(discriminator.state_dict(), discriminator_filename)\n",
    "        print(f\"Discriminator model saved to {discriminator_filename}\")\n",
    "save_model(lsgan_generator, lsgan_discriminator, \n",
    "        output_dir=r\"data\\model_params\\lsgan_model_all_data\", \n",
    "        epoch=num_epochs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system(r'C:\\Users\\haokw\\Desktop\\11111.mp3')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_path = r\"C:\\Users\\haokw\\Documents\\GitHub\\gaolu\\up2\\旧框架\\data\\model_params\\lsgan_model_all_data\\generator.pth\"\n",
    "# generator_path = r\"data\\model_params\\lsgan_model\\generator.pth\"\n",
    "lsgan_generator.load_state_dict(torch.load(generator_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成样本测试\n",
    "z = torch.randn(test_sample_num, z_dim)\n",
    "print(z.shape)\n",
    "\n",
    "generated_data = lsgan_generator(z).detach().numpy()\n",
    "print(generated_data.shape)\n",
    "\n",
    "# 将生成的数据转换为 DataFrame\n",
    "generated_df = pd.DataFrame(generated_data, columns=df_GAN.columns)\n",
    "\n",
    "# 可视化生成的数据分布（你可以实现自己的可视化函数）\n",
    "\n",
    "\n",
    "# plot_scatter_matrix(df_GAN, generated_df, figsize=(10, 8), font=font)\n",
    "# plot_scatter_matrix(generated_df, df_result, figsize=(10, 8), font=font)\n",
    "plot_scatter_matrix_22222(df_GAN, generated_df, figsize=(10, 8), font=font)\n",
    "plot_scatter_matrix_22222(generated_df, df_result, figsize=(10, 8), font=font)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39_tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
