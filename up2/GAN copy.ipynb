{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基础库\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import sys\n",
    "import os\n",
    "import datetime\n",
    "import pickle\n",
    "# 机器学习库\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "# 数据归一化、逆归一化\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# 优化相关库\n",
    "from skopt import gp_minimize\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# 深度学习库\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# 忽略警告\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# 中文字体设置\n",
    "from matplotlib.font_manager import FontProperties\n",
    "font = FontProperties(fname=r\"c:\\windows\\fonts\\simsun.ttc\", size=12)  # 替换为你的中文字体文件路径\n",
    "\n",
    "# 其他路径设置\n",
    "sys.path.append(r\"C:\\Users\\haokw\\Documents\\GitHub\\gaolu\\MPC\\高炉\")\n",
    "\n",
    "# 自定义模块\n",
    "import base \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 检查 DataFrame 中是否包含 NaN 值\n",
    "def check_if_NaN(data):\n",
    "    print(data.shape)\n",
    "    contains_nan = data.isna().any().any()\n",
    "    if contains_nan:\n",
    "        print(\"数据包含 NaN 值\")\n",
    "    else:\n",
    "        print(\"数据不包含 NaN 值\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取Excel文件\n",
    "# excel_path = f'C:\\\\Users\\\\haokw\\\\Documents\\\\GitHub\\\\gaolu\\\\up2\\\\data\\\\data.xlsx'\n",
    "# df_sheet_X = pd.read_excel(excel_path, sheet_name='X') \n",
    "# check_if_NaN(df_sheet_X)\n",
    "\n",
    "\n",
    "excel_path = f'C:\\\\Users\\\\haokw\\\\Documents\\\\GitHub\\\\gaolu\\\\up2\\\\data\\\\data.xlsx'\n",
    "df_sheet_Y = pd.read_excel(excel_path, sheet_name='Y') \n",
    "\n",
    "\n",
    "check_if_NaN(df_sheet_Y)\n",
    "\n",
    "\n",
    "\n",
    "# 读取Excel文件\n",
    "excel_path = f'C:\\\\Users\\\\haokw\\\\Documents\\\\GitHub\\\\gaolu\\\\up2\\\\data\\\\GAN_all_data.xlsx'\n",
    "df_sheet_params1 = pd.read_excel(excel_path, sheet_name='Sheet1') \n",
    "df_sheet_params2 = pd.read_excel(excel_path, sheet_name='Sheet2') \n",
    "df_sheet_params3 = pd.read_excel(excel_path, sheet_name='Sheet3') \n",
    "df_sheet_params4 = pd.read_excel(excel_path, sheet_name='Sheet4') \n",
    "df_sheet_params = pd.concat([df_sheet_params1,df_sheet_params2,df_sheet_params3,df_sheet_params4],axis=0)\n",
    "print('加载数据完成')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sheet_X = df_sheet_params.iloc[::20,:].reset_index()#返回第一行\n",
    "check_if_NaN(df_sheet_X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########\n",
    "os.system(r'C:\\Users\\haokw\\Desktop\\11111.mp3')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_term =  ['富氧流量', '设定喷煤量', '热风压力', '热风温度']\n",
    "output_term = ['铁水温度[MIT]', '铁水硅含量[SI]']\n",
    "time_term=  '时间戳h'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建数据框副本以避免修改原始数据\n",
    "df_sheet_X_process = df_sheet_X.copy()\n",
    "df_sheet_Y_process = df_sheet_Y.copy()\n",
    "\n",
    "\n",
    "def IQR_process(df_IQR, columns):\n",
    "    df_IQR = df_IQR\n",
    "    columns = columns\n",
    "\n",
    "    print(columns)      # 获取数据框的所有列名\n",
    "    outlier_indices = set()  # 用于存储异常值的行索引\n",
    "\n",
    "    # 1. 分别处理每个变量\n",
    "    for column in columns:\n",
    "        # 计算描述性统计\n",
    "        stats = df_IQR[column].describe()\n",
    "\n",
    "        # 计算IQR（四分位距）以及上下须的范围\n",
    "        Q1 = stats['25%']\n",
    "        Q3 = stats['75%']\n",
    "        IQR = Q3 - Q1\n",
    "        lower_whisker = Q1 - 1.5 * IQR\n",
    "        upper_whisker = Q3 + 1.5 * IQR\n",
    "\n",
    "        # # 绘制箱线图\n",
    "        # plt.figure(figsize=(8, 6))\n",
    "        # sns.boxplot(data=df_IQR[column])\n",
    "        # plt.title(f'Boxplot of {column}', fontproperties=font)\n",
    "        # plt.xlabel('Feature', fontproperties=font)\n",
    "        # plt.ylabel('Value', fontproperties=font)\n",
    "        # plt.show()\n",
    "\n",
    "        # 查找异常值的索引\n",
    "        outliers = df_IQR[(df_IQR[column] < lower_whisker) | \n",
    "                            (df_IQR[column] > upper_whisker)].index\n",
    "        outlier_indices.update(outliers)\n",
    "\n",
    "        # # 打印统计信息和异常值范围\n",
    "        # print(f\"列: {column}\")\n",
    "        # print(f\"第一四分位数 (Q1): {Q1}\")\n",
    "        # print(f\"第三四分位数 (Q3): {Q3}\")\n",
    "        # print(f\"下须 (lower whisker): {lower_whisker}\")\n",
    "        # print(f\"上须 (upper whisker): {upper_whisker}\")\n",
    "        # print(f\"找到的异常值索引: {list(outliers)}\")\n",
    "\n",
    "        \n",
    "        # print(f\"异常值数量: {len(outliers)}\")\n",
    "        # print(f\"总数: {len(df_IQR[column])}\")\n",
    "\n",
    "        # print(f\"异常值比例: {len(outliers)/len(df_IQR[column])}\\n\")\n",
    "\n",
    "    # 2. 删除所有异常值\n",
    "    df_cleaned = df_IQR.drop(index=outlier_indices)\n",
    "    # 重新设置索引，使索引从 0 开始，并丢弃旧索引\n",
    "    df_cleaned.reset_index(drop=True, inplace=True)\n",
    "    # 输出处理后的数据框信息\n",
    "    print(f\"原始数据行数: {df_IQR.shape[0]}\")\n",
    "    print(f\"删除异常值后的数据行数: {df_cleaned.shape[0]}\")\n",
    "\n",
    "    # 你可以继续对 df_cleaned 进行后续处理\n",
    "\n",
    "\n",
    "\n",
    "    return df_cleaned\n",
    "\n",
    "\n",
    "df_cleaned_X = IQR_process(df_sheet_X_process, input_term)\n",
    "df_cleaned_Y = IQR_process(df_sheet_Y_process, output_term)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 画出数据\n",
    "def plot_subplot(data_x_yuan,data_y_yuan,data_x,data_y,column):\n",
    "    plt.plot(data_x_yuan,data_y_yuan,'r.')\n",
    "    plt.plot(data_x,data_y,'m.')\n",
    "    # plt.xlabel(time_term, fontproperties=font)  # 使用中文标签\n",
    "    plt.ylabel(column, fontproperties=font)  # 使用中文标签\n",
    "    # 使用中文标签\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15, 4))\n",
    "for idx, column in enumerate(input_term):\n",
    "    plt.subplot(len(input_term), 1, idx+1)\n",
    "    plot_subplot(   df_sheet_X[time_term].values,   df_sheet_X[column].values, \n",
    "                    df_cleaned_X[time_term].values, df_cleaned_X[column].values,column\n",
    "                )\n",
    "\n",
    "plt.figure(figsize=(15, 2))\n",
    "for idx, column in enumerate(output_term):\n",
    "    plt.subplot(len(output_term), 1, idx+1)\n",
    "    plot_subplot(   df_sheet_Y[time_term].values,   df_sheet_Y[column].values, \n",
    "                    df_cleaned_Y[time_term].values, df_cleaned_Y[column].values,column\n",
    "                )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 画出选取的数据\n",
    "def plot_subplot(data_x,data_y,column,index_predict,index_gaolu):\n",
    "    plt.plot(data_x,data_y,'-', label='origin_data')\n",
    "    plt.plot(data_x[index_gaolu],data_y[index_gaolu],'r-', label='gaolu_data')\n",
    "    plt.plot(data_x[index_predict],data_y[index_predict],'g-', label='predict_data')\n",
    "    plt.legend()\n",
    "    # plt.xlabel(time_term, fontproperties=font)  # 使用中文标签\n",
    "    plt.ylabel(column, fontproperties=font)  # 使用中文标签\n",
    "\n",
    "# 6509\n",
    "\n",
    "length1 = 400\n",
    "start1 = 0\n",
    "\n",
    "length2 = 1\n",
    "start2 = 6507\n",
    "\n",
    "\n",
    "index_gaolu   = range(start1, start1+length1+1, 1)\n",
    "index_predict     = range(start2, start2+length2+1, 1)\n",
    "# index = range(1, 7572, 1)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15, 6))\n",
    "for idx, column in enumerate(input_term):\n",
    "    plt.subplot(len(input_term+output_term), 1, idx+1)\n",
    "    plot_subplot(df_cleaned_X[time_term].values, df_cleaned_X[column].values, column, index_predict, index_gaolu)\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15, 6))\n",
    "for idx, column in enumerate(output_term):\n",
    "    plt.subplot(len(input_term+output_term), 1, idx+1)\n",
    "    plot_subplot(df_cleaned_Y[time_term].values,df_cleaned_Y[column].values,column,index_predict,index_gaolu)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将数据存储为字典，每个键对应一列数据\n",
    "X_dict_original = {\n",
    "    input_term[0]:   df_cleaned_X[input_term[0]].values,\n",
    "    input_term[1]:   df_cleaned_X[input_term[1]].values,\n",
    "    input_term[2]:   df_cleaned_X[input_term[2]].values,\n",
    "    input_term[3]:   df_cleaned_X[input_term[3]].values\n",
    "}\n",
    "Y_dict_original = {\n",
    "    output_term[0]:  df_cleaned_Y[output_term[0]].values,\n",
    "    output_term[1]:  df_cleaned_Y[output_term[1]].values\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X_dict_original_index_gaolu = {\n",
    "    input_term[0]:   df_cleaned_X[input_term[0]][index_gaolu].values,\n",
    "    input_term[1]:   df_cleaned_X[input_term[1]][index_gaolu].values,\n",
    "    input_term[2]:   df_cleaned_X[input_term[2]][index_gaolu].values,\n",
    "    input_term[3]:   df_cleaned_X[input_term[3]][index_gaolu].values\n",
    "}\n",
    "Y_dict_original_index_gaolu = {\n",
    "    output_term[0]:  df_cleaned_Y[output_term[0]][index_gaolu].values,\n",
    "    output_term[1]:  df_cleaned_Y[output_term[1]][index_gaolu].values\n",
    "}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化缩放器\n",
    "scalers_X = {}\n",
    "scalers_Y = {}\n",
    "\n",
    "# 进行拟合\n",
    "for column, data in X_dict_original.items():\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    scaler.fit(data.reshape(-1, 1))  # 保证数据是列向量\n",
    "    scalers_X[column] = scaler\n",
    "\n",
    "for column, data in Y_dict_original.items():\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    scaler.fit(data.reshape(-1, 1))  # 保证数据是列向量\n",
    "    scalers_Y[column] = scaler\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 进行归一化\n",
    "X_dict_normal = {}\n",
    "Y_dict_normal = {}\n",
    "for column, scaler in scalers_X.items():\n",
    "    X_dict_normal[column] = scaler.transform(X_dict_original[column].reshape(-1, 1)).flatten()\n",
    "for column, scaler in scalers_Y.items():\n",
    "    Y_dict_normal[column] = scaler.transform(Y_dict_original[column].reshape(-1, 1)).flatten()\n",
    "\n",
    "# 转换为DataFrame\n",
    "print('全部数据')\n",
    "X_df_normal = pd.DataFrame(X_dict_normal)\n",
    "check_if_NaN(X_df_normal)\n",
    "Y_df_normal = pd.DataFrame(Y_dict_normal)\n",
    "check_if_NaN(Y_df_normal)\n",
    "\n",
    "\n",
    "\n",
    "# 高炉部分数据\n",
    "# 进行归一化\n",
    "X_dict_normal_index_gaolu = {}\n",
    "Y_dict_normal_index_gaolu = {}\n",
    "for column, scaler in scalers_X.items():\n",
    "    X_dict_normal_index_gaolu[column] = scaler.transform(X_dict_original_index_gaolu[column].reshape(-1, 1)).flatten()\n",
    "for column, scaler in scalers_Y.items():\n",
    "    Y_dict_normal_index_gaolu[column] = scaler.transform(Y_dict_original_index_gaolu[column].reshape(-1, 1)).flatten()\n",
    "\n",
    "# 转换为DataFrame\n",
    "print('高炉部分数据')\n",
    "X_df_normal_index_gaolu = pd.DataFrame(X_dict_normal_index_gaolu)\n",
    "check_if_NaN(X_df_normal_index_gaolu)\n",
    "Y_df_normal_index_gaolu = pd.DataFrame(Y_dict_normal_index_gaolu)\n",
    "check_if_NaN(Y_df_normal_index_gaolu)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制叠加的散点图矩阵。\n",
    "\n",
    "\n",
    "def plot_scatter_matrix(X_df_normal, X_df_normal_index_gaolu, figsize=(10, 8),font=font, save_path=None):\n",
    "    \"\"\"\n",
    "    绘制叠加的散点图矩阵。\n",
    "\n",
    "    参数:\n",
    "    X_df_normal (DataFrame): 第一组数据。\n",
    "    X_df_normal_index_gaolu (DataFrame): 第二组数据。\n",
    "    font (FontProperties, optional): 字体属性，用于设置标签的字体。\n",
    "    \"\"\"\n",
    "    # 设置颜色和标记\n",
    "    color_left = 'blue'\n",
    "    color_right = 'red'\n",
    "    marker_left = '.'\n",
    "    marker_right = '.'\n",
    "\n",
    "    # 设置数据\n",
    "    df_left = X_df_normal  # 第一组数据\n",
    "    df_right = X_df_normal_index_gaolu  # 第二组数据\n",
    "\n",
    "    # 绘制叠加的散点图矩阵\n",
    "    plt.figure(figsize = figsize)\n",
    "    num_cols = len(df_left.columns)\n",
    "    \n",
    "    for i, col1 in enumerate(df_left.columns):\n",
    "        for j, col2 in enumerate(df_left.columns):\n",
    "            ax = plt.subplot(num_cols, num_cols, i * num_cols + j + 1)\n",
    "            \n",
    "            if i != j:\n",
    "                ax.scatter(df_left[col1], df_left[col2], color=color_left, alpha=0.5, marker=marker_left, label='Left Data' if i == 0 and j == 1 else \"\")\n",
    "                ax.scatter(df_right[col1], df_right[col2], color=color_right, alpha=0.5, marker=marker_right, label='Right Data' if i == 0 and j == 1 else \"\")\n",
    "                ax.set_xlim([-1, 1])\n",
    "                ax.set_ylim([-1, 1])\n",
    "            else:\n",
    "                ax.hist(df_left[col1], bins=50, alpha=0.5, color=color_left)\n",
    "                ax.hist(df_right[col1], bins=50, alpha=0.5, color=color_right)\n",
    "                ax.set_xlim([-1, 1])\n",
    "\n",
    "            if i == num_cols - 1:\n",
    "                ax.set_xlabel(col2, fontproperties=font)\n",
    "            if j == 0:\n",
    "                ax.set_ylabel(col1, fontproperties=font)\n",
    "\n",
    "    # # 添加图例\n",
    "    # plt.legend(loc='upper right', bbox_to_anchor=(1.5, 1))\n",
    "    # 手动添加图例\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    ax.legend(handles, labels, loc='upper right', bbox_to_anchor=(1.5, 1))\n",
    "\n",
    "    # 添加标题并调整布局\n",
    "    plt.suptitle('Overlaid Scatter Matrix of Features', y=1.02)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # 如果提供了保存路径，则保存图像\n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "        plt.close()  # 关闭当前的图像，以节省内存\n",
    "    else:\n",
    "        plt.show()  # 否则显示图像\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制散点图矩阵\n",
    "plot_scatter_matrix(X_df_normal, X_df_normal_index_gaolu, font=font, figsize=(10, 8))\n",
    "# 绘制散点图矩阵\n",
    "plot_scatter_matrix(Y_df_normal, Y_df_normal_index_gaolu, font=font, figsize=(5, 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 转换为DataFrame\n",
    "print('高炉部分数据')\n",
    "X_df_normal_index_gaolu = pd.DataFrame(X_dict_normal_index_gaolu)\n",
    "check_if_NaN(X_df_normal_index_gaolu)\n",
    "Y_df_normal_index_gaolu = pd.DataFrame(Y_dict_normal_index_gaolu)\n",
    "check_if_NaN(Y_df_normal_index_gaolu)\n",
    "\n",
    "\n",
    "\n",
    "# 转换为DataFrame\n",
    "print('全部数据')\n",
    "X_df_normal = pd.DataFrame(X_dict_normal)\n",
    "check_if_NaN(X_df_normal)\n",
    "Y_df_normal = pd.DataFrame(Y_dict_normal)\n",
    "check_if_NaN(Y_df_normal)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(min(X_df_normal.shape[0],Y_df_normal.shape[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_df_normal.shape)\n",
    "print(Y_df_normal.shape)\n",
    "print(df_cleaned_X.info())\n",
    "print(df_cleaned_Y.info())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isShuffle = True\n",
    "isShuffle = False\n",
    "time_steps = 2\n",
    "test_size = 0.15\n",
    "val_size = 0.15\n",
    "train_size = 1-val_size-test_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 组合训练数据--拆分训练、测试集\n",
    "\n",
    "# 定义时间步数和特征数\n",
    "\n",
    "# 构成    \n",
    "# X = [X(t),X(t-1),Y(t-1)]\n",
    "# Y = [Y(t)]\n",
    "def make_data(X_df_normal,Y_df_normal,index_fanwei,ifprint):\n",
    "    X_modified = []\n",
    "    y_modified = []\n",
    "\n",
    "\n",
    "    for i in range(0, min(X_df_normal.shape[0],Y_df_normal.shape[0])):\n",
    "        # print(i)\n",
    "        if i in index_fanwei:\n",
    "            # print(i)\n",
    "\n",
    "            Y_time = df_cleaned_Y[time_term][i]\n",
    "            # print('输出时间：',df_cleaned_Y[time_term][i])\n",
    "\n",
    "            closest_10 = df_cleaned_X[df_cleaned_X[time_term] <= Y_time].nlargest(time_steps, time_term)\n",
    "            # print(closest_10)\n",
    "            # 检查 closest_10 是否为空\n",
    "            if closest_10.empty:\n",
    "                print(\"No closest values found. closest_10 is empty.\")\n",
    "                continue\n",
    "                \n",
    "            index = closest_10.index\n",
    "            # print(list(index))\n",
    "            # print(closest_10.iloc[-1][time_term])\n",
    "            # print(Y_time - time_steps + 1 )\n",
    "\n",
    "            if closest_10.iloc[-1][time_term] != Y_time - time_steps + 1 :\n",
    "                # print(i,Y_time)\n",
    "                print(i,',t',Y_time,',||||  t',closest_10.iloc[0][time_term],',t-time_steps',closest_10.iloc[-1][time_term],'index',index[0],index[-1],'errloss')\n",
    "            else:\n",
    "                # print(X_df_normal.loc[index])\n",
    "                # 拼接行数据 (axis=0 表示纵向拼接)\n",
    "                new_x_sample = np.concatenate([X_df_normal.loc[i, :].values for i in index], axis=0)\n",
    "                # print(new_x_sample)\n",
    "                y_last = Y_df_normal.loc[i-1]\n",
    "                \n",
    "                # print(y_last, 'y_last time : ',df_cleaned_Y[time_term][i-1])\n",
    "\n",
    "                new_x_sample = np.concatenate([new_x_sample,y_last],axis=0)\n",
    "                # print(new_x_sample)\n",
    "                y_sample = Y_df_normal.loc[i]\n",
    "                # print(y_sample)\n",
    "                X_modified.append(new_x_sample)\n",
    "                y_modified.append(y_sample)\n",
    "                print(i,',t',Y_time,',t',closest_10.iloc[0][time_term],',t-time_steps',closest_10.iloc[-1][time_term],'index',index[0],index[-1])\n",
    "\n",
    "\n",
    "            # break\n",
    "    \n",
    "    # 将列表转换为 NumPy 数组\n",
    "    \n",
    "    # 查看二维列表的形状\n",
    "    rows = len(X_modified)\n",
    "    columns = len(X_modified[0]) if rows > 0 else 0\n",
    "    print(f\"二维列表的形状: ({rows}, {columns})\")\n",
    "\n",
    "\n",
    "\n",
    "    X_modified = np.array(X_modified)\n",
    "    y_modified = np.array(y_modified)\n",
    "    X_reshaped = X_modified.reshape((X_modified.shape[0], X_modified.shape[1]))\n",
    "\n",
    "    # 打印新数据的形状\n",
    "    print(\"Modified Input Shape:\", X_reshaped.shape)\n",
    "    print(\"Modified Output Shape:\", y_modified.shape)\n",
    "\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_reshaped, y_modified, \n",
    "                                                        test_size=test_size, \n",
    "                                                        random_state=42, \n",
    "                                                        shuffle=isShuffle)\n",
    "\n",
    "    # 将剩余的70%训练数据再次拆分成训练数据和验证数据（20%验证数据，50%训练数据）\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, \n",
    "                                                        test_size=val_size/(train_size+val_size), \n",
    "                                                        random_state=42, \n",
    "                                                        shuffle=isShuffle)\n",
    "\n",
    "    print('训练数量：',X_train.shape,y_train.shape)\n",
    "    print('验证数量：',X_val.shape,y_val.shape)\n",
    "    print('测试数量：',X_test.shape,y_test.shape)\n",
    "\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('高炉模型数据')\n",
    "X_gaolu_train, X_gaolu_val, X_gaolu_test,\\\n",
    "y_gaolu_train, y_gaolu_val, y_gaolu_test = make_data(X_df_normal,Y_df_normal,\n",
    "                                                    index_gaolu,ifprint = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在三角形内部生成随机点\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Polygon\n",
    "from matplotlib.path import Path\n",
    "\n",
    "# 设置随机种子\n",
    "np.random.seed(42)\n",
    "\n",
    "def generate_triangle_data(vertices, num_samples):\n",
    "    # 创建三角形的路径\n",
    "    triangle_path = Path(vertices)\n",
    "    \n",
    "    # 初始化点\n",
    "    points = []\n",
    "    \n",
    "    # 在三角形内部生成随机点\n",
    "    while len(points) < num_samples:\n",
    "        # 生成随机点\n",
    "        random_point = np.random.rand(2)\n",
    "        \n",
    "        # 检查点是否在三角形内部\n",
    "        if triangle_path.contains_points([random_point]):\n",
    "            points.append(random_point)\n",
    "    \n",
    "    return np.array(points)\n",
    "\n",
    "# 定义三角形顶点\n",
    "vertices = np.array([\n",
    "    [0, 0],  # 顶点A\n",
    "    [1, 0],  # 顶点B\n",
    "    [0.5, 1]  # 顶点C\n",
    "])\n",
    "\n",
    "# 生成数据\n",
    "num_samples = 10000\n",
    "data_test = generate_triangle_data(vertices, num_samples)\n",
    "\n",
    "# # 可视化数据\n",
    "# plt.figure(figsize=(8, 8))\n",
    "# plt.fill(vertices[:, 0], vertices[:, 1], 'lightgray', edgecolor='black', alpha=0.5)  # 填充三角形\n",
    "# plt.scatter(data_test[:, 0], data_test[:, 1], c='blue', alpha=0.5, s=1)\n",
    "# plt.title('Random Points Inside a Triangle')\n",
    "# plt.xlabel('X')\n",
    "# plt.ylabel('Y')\n",
    "# plt.axis('equal')\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# 将数据转换为 DataFrame\n",
    "data_test = pd.DataFrame(data_test, columns=['X', 'Y'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_df_normal' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 将历史数据转换为 PyTorch 张量\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m data_item \u001b[38;5;241m=\u001b[39m \u001b[43mX_df_normal\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# data_item = data_test\u001b[39;00m\n\u001b[0;32m      5\u001b[0m data \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(data_item\u001b[38;5;241m.\u001b[39mvalues, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_df_normal' is not defined"
     ]
    }
   ],
   "source": [
    "# 将历史数据转换为 PyTorch 张量\n",
    "data_item = X_df_normal\n",
    "# data_item = data_test\n",
    "\n",
    "data = torch.tensor(data_item.values, dtype=torch.float32)\n",
    "df_WGAN = data_item\n",
    "test_sample_num = data.shape[0]\n",
    "\n",
    "\n",
    "\n",
    "# 超参数\n",
    "z_dim = 5  # 随机噪声维度\n",
    "data_dim = data.shape[1]  # 数据维度，4\n",
    "learning_rate = 0.0002\n",
    "num_epochs = 200\n",
    "n_critic = 5  # 每次更新生成器前，更新 Critic 的次数\n",
    "\n",
    "print_piture_d = 10\n",
    "\n",
    "# 设置数据加载器\n",
    "batch_size = 512\n",
    "data_loader = torch.utils.data.DataLoader(data, batch_size=batch_size, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# W-GAN模型定义和训练\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# WGAN的生成器\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(256, output_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# WGAN的Critic\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Critic, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# 权重裁剪\n",
    "def weight_clipping(critic, clip_value=0.01):\n",
    "    for param in critic.parameters():\n",
    "        param.data.clamp_(-clip_value, clip_value)\n",
    "\n",
    "# WGAN训练\n",
    "def train_wgan(generator, critic, data_loader, num_epochs, z_dim, clip_value, n_critic, optimizer_G, optimizer_C, output_dir):\n",
    "    \n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    for epoch in range(num_epochs):\n",
    "        for real_data in data_loader:\n",
    "            batch_size = real_data.size(0)\n",
    "\n",
    "            # 更新 Critic\n",
    "            for _ in range(n_critic):\n",
    "                z = torch.randn(batch_size, z_dim)\n",
    "                fake_data = generator(z)\n",
    "\n",
    "                # 计算 Critic 的损失（Wasserstein 距离）\n",
    "                real_output = critic(real_data)\n",
    "                fake_output = critic(fake_data.detach())\n",
    "                c_loss = -torch.mean(real_output) + torch.mean(fake_output)\n",
    "\n",
    "                optimizer_C.zero_grad()\n",
    "                c_loss.backward()\n",
    "                optimizer_C.step()\n",
    "\n",
    "                # 对 Critic 权重进行裁剪\n",
    "                weight_clipping(critic, clip_value)\n",
    "\n",
    "            # 更新生成器\n",
    "            z = torch.randn(batch_size, z_dim)\n",
    "            fake_data = generator(z)\n",
    "            fake_output = critic(fake_data)\n",
    "            g_loss = -torch.mean(fake_output)\n",
    "\n",
    "            optimizer_G.zero_grad()\n",
    "            g_loss.backward()\n",
    "            optimizer_G.step()\n",
    "\n",
    "        print(f\"WGAN_training Epoch [{epoch}/{num_epochs}], c_loss: {c_loss.item():.4f}, g_loss: {g_loss.item():.4f}\")\n",
    "\n",
    "        if epoch % print_piture_d == print_piture_d-1:\n",
    "            # 生成一些数据\n",
    "            z = torch.randn(test_sample_num, z_dim)\n",
    "            generated_data = generator(z).detach().numpy()\n",
    "\n",
    "            # 将生成的数据转换为 DataFrame\n",
    "            generated_df = pd.DataFrame(generated_data, columns=df_WGAN.columns)\n",
    "\n",
    "            # 获取当前时间\n",
    "            current_time = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "            # 构建保存路径和文件名\n",
    "            filename = f\"{current_time}_WGAN_Epoch_{epoch+1}_c_loss_{c_loss.item():.4f}_g_loss_{g_loss.item():.4f}.png\"\n",
    "            save_path = os.path.join(output_dir, filename)\n",
    "\n",
    "            # 可视化生成的数据分布（你可以实现自己的可视化函数）\n",
    "            plot_scatter_matrix(df_WGAN, generated_df, figsize=(10, 8), font=font, save_path=save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LS-GAN模型定义和训练\n",
    "class LSGANGenerator(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LSGANGenerator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(256, output_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# LSGAN的判别器\n",
    "class LSGANDiscriminator(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(LSGANDiscriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# LSGAN的损失函数\n",
    "def generator_loss(fake_output):\n",
    "    return torch.mean((fake_output - 1) ** 2)\n",
    "\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = torch.mean((real_output - 1) ** 2)\n",
    "    fake_loss = torch.mean(fake_output ** 2)\n",
    "    return real_loss + fake_loss\n",
    "\n",
    "# LSGAN训练\n",
    "def train_lsgan(generator, discriminator, data_loader, num_epochs, z_dim, optimizer_G, optimizer_D, output_dir):\n",
    "    \n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    for epoch in range(num_epochs):\n",
    "        for real_data in data_loader:\n",
    "            batch_size = real_data.size(0)\n",
    "\n",
    "            # 生成数据\n",
    "            z = torch.randn(batch_size, z_dim)\n",
    "            fake_data = generator(z)\n",
    "\n",
    "            # 判别器前向传播\n",
    "            real_output = discriminator(real_data)\n",
    "            fake_output = discriminator(fake_data.detach())\n",
    "\n",
    "            # 计算判别器损失\n",
    "            d_loss = discriminator_loss(real_output, fake_output)\n",
    "            optimizer_D.zero_grad()\n",
    "            d_loss.backward()\n",
    "            optimizer_D.step()\n",
    "\n",
    "            # 生成器前向传播\n",
    "            fake_output = discriminator(fake_data)\n",
    "\n",
    "            # 计算生成器损失\n",
    "            g_loss = generator_loss(fake_output)\n",
    "            optimizer_G.zero_grad()\n",
    "            g_loss.backward()\n",
    "            optimizer_G.step()\n",
    "\n",
    "        print(f\"LS-GAN_training Epoch [{epoch}/{num_epochs}], D_loss: {d_loss.item():.4f}, G_loss: {g_loss.item():.4f}\")\n",
    "\n",
    "        if epoch % print_piture_d == print_piture_d-1:\n",
    "            # 生成数据test_sample_num\n",
    "            z = torch.randn(test_sample_num, z_dim)\n",
    "            generated_data = generator(z).detach().numpy()\n",
    "\n",
    "            # 将生成的数据转换为 DataFrame\n",
    "            generated_df = pd.DataFrame(generated_data, columns=df_WGAN.columns)\n",
    "\n",
    "            # 获取当前时间\n",
    "            current_time = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "            # 构建保存路径和文件名\n",
    "            filename = f\"{current_time}_LSGAN_Epoch_{epoch+1}_D_loss_{d_loss.item():.4f}_G_loss_{g_loss.item():.4f}.png\"\n",
    "            save_path = os.path.join(output_dir, filename)\n",
    "\n",
    "            # 可视化生成的数据分布（你可以实现自己的可视化函数）\n",
    "            plot_scatter_matrix(df_WGAN, generated_df, figsize=(10, 8), font=font, save_path=save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化WGAN生成器和Critic\n",
    "wgan_generator = Generator(input_dim=z_dim, output_dim=data_dim)\n",
    "wgan_critic = Critic(input_dim=data_dim)\n",
    "# 初始化优化器\n",
    "optimizer_G = optim.Adam(wgan_generator.parameters(), lr=learning_rate, betas=(0.2, 0.999))\n",
    "optimizer_C = optim.Adam(wgan_critic.parameters(), lr=learning_rate, betas=(0.2, 0.999))\n",
    "output_dir = r\"data\\train_output_picture\\WGAN_training_output_all_data\"\n",
    "# 训练WGAN\n",
    "train_wgan(wgan_generator, wgan_critic, data_loader, \n",
    "                num_epochs=num_epochs, z_dim=z_dim, clip_value=0.01, \n",
    "                n_critic = 5, \n",
    "                optimizer_G=optimizer_G, optimizer_C=optimizer_C, \n",
    "                output_dir=output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########\n",
    "os.system(r'C:\\Users\\haokw\\Desktop\\11111.mp3')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化LSGAN优化器\n",
    "# # 使用WGAN生成的数据初始化LSGAN\n",
    "lsgan_generator = LSGANGenerator(input_dim=z_dim, output_dim=data_dim)\n",
    "lsgan_discriminator = LSGANDiscriminator(input_dim=data_dim)\n",
    "\n",
    "optimizer_G = optim.Adam(lsgan_generator.parameters(), lr=learning_rate, betas=(0.2, 0.999))\n",
    "optimizer_D = optim.Adam(lsgan_discriminator.parameters(), lr=learning_rate, betas=(0.2, 0.999))\n",
    "output_dir = r\"data\\train_output_picture\\LS-GAN_training_output_all_data\"\n",
    "# 使用WGAN生成的数据作为输入进行LSGAN训练\n",
    "train_lsgan(lsgan_generator, lsgan_discriminator, data_loader, \n",
    "                num_epochs=num_epochs, z_dim=z_dim, \n",
    "                optimizer_G=optimizer_G, optimizer_D=optimizer_D, \n",
    "                output_dir=output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成样本测试\n",
    "z = torch.randn(10000, z_dim)\n",
    "print(z.shape)\n",
    "\n",
    "generated_data = lsgan_generator(z).detach().numpy()\n",
    "print(generated_data.shape)\n",
    "\n",
    "# 将生成的数据转换为 DataFrame\n",
    "generated_df = pd.DataFrame(generated_data, columns=df_WGAN.columns)\n",
    "\n",
    "# 可视化生成的数据分布（你可以实现自己的可视化函数）\n",
    "plot_scatter_matrix(df_WGAN, generated_df, figsize=(10, 8), font=font)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 各种参数、信息数据的整理保存\n",
    "\n",
    "# 缩放器\n",
    "with open(r'data\\scalers\\scalers_X.pkl', 'wb') as f:\n",
    "    pickle.dump(scalers_X, f)\n",
    "with open(r'data\\scalers\\scalers_Y.pkl', 'wb') as f:\n",
    "    pickle.dump(scalers_Y, f)\n",
    "\n",
    "\n",
    "# 保存模型参数\n",
    "def save_model(generator, discriminator, output_dir, epoch):\n",
    "    # 创建保存目录（如果不存在）\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # 定义文件名\n",
    "    generator_filename = os.path.join(output_dir, f\"generator.pth\")\n",
    "    discriminator_filename = os.path.join(output_dir, f\"discriminator.pth\")\n",
    "\n",
    "    # 保存生成器的模型参数\n",
    "    torch.save(generator.state_dict(), generator_filename)\n",
    "    print(f\"Generator model saved to {generator_filename}\")\n",
    "\n",
    "    # 保存判别器的模型参数\n",
    "    torch.save(discriminator.state_dict(), discriminator_filename)\n",
    "    print(f\"Discriminator model saved to {discriminator_filename}\")\n",
    "\n",
    "save_model(lsgan_generator, lsgan_discriminator, \n",
    "        output_dir=r\"data\\model_params\\lsgan_model\", \n",
    "        epoch=num_epochs)\n",
    "\n",
    "save_model(wgan_generator, wgan_critic, \n",
    "        output_dir=r\"data\\model_params\\wgan_model\", \n",
    "        epoch=num_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSGAN的生成器   初始化   加载生成器参数  获取 PyTorch 模型参数  构建numpy版本  初始化numpy生成器\n",
    "class LSGANGenerator_item(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LSGANGenerator_item, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(256, output_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "\n",
    "# 假设输入维度和输出维度如下\n",
    "data_dim = 4  # 数据维度\n",
    "\n",
    "# 初始化生成器和判别器\n",
    "lsgan_generator_item = LSGANGenerator_item(input_dim=z_dim, output_dim=data_dim)\n",
    "\n",
    "\n",
    "\n",
    "# 加载生成器参数\n",
    "generator_path = r\"data\\model_params\\lsgan_model\\lsgan_generator_epoch_200.pth\"\n",
    "lsgan_generator_item.load_state_dict(torch.load(generator_path))\n",
    "lsgan_generator_item.eval()  # 切换到评估模式\n",
    "print(f\"Generator model loaded from {generator_path}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 假设 lsgan_generator_item 已经是训练好的模型\n",
    "def extract_pytorch_params(model):\n",
    "    params = {}\n",
    "    for name, param in model.named_parameters():\n",
    "        params[name] = param.data.numpy()\n",
    "    return params\n",
    "# 获取 PyTorch 模型参数\n",
    "lsgan_generator_item_params = extract_pytorch_params(lsgan_generator_item)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 构建numpy版本\n",
    "class LSGANGeneratorNumpy:\n",
    "    def __init__(self, input_dim, output_dim, params):\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        # 使用从 PyTorch 模型中提取的参数初始化\n",
    "        self.W1 = params['model.0.weight'].T  # 转置以匹配 numpy 矩阵乘法\n",
    "        self.b1 = params['model.0.bias']\n",
    "        \n",
    "        self.W2 = params['model.2.weight'].T\n",
    "        self.b2 = params['model.2.bias']\n",
    "        \n",
    "        self.W3 = params['model.4.weight'].T\n",
    "        self.b3 = params['model.4.bias']\n",
    "    \n",
    "    def relu(self, x):\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    def tanh(self, x):\n",
    "        return np.tanh(x)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        z1 = np.dot(x, self.W1) + self.b1\n",
    "        a1 = self.relu(z1)\n",
    "        \n",
    "        z2 = np.dot(a1, self.W2) + self.b2\n",
    "        a2 = self.relu(z2)\n",
    "        \n",
    "        z3 = np.dot(a2, self.W3) + self.b3\n",
    "        output = self.tanh(z3)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "# 初始化numpy生成器\n",
    "lsgan_generator_numpy = LSGANGeneratorNumpy(z_dim, data_dim, lsgan_generator_item_params)\n",
    "\n",
    "print('参数已迁移')\n",
    "\n",
    "\n",
    "\n",
    "# 从文件中加载 scalers_X 和 scalers_Y\n",
    "with open(r'data\\scalers\\scalers_X.pkl', 'rb') as f:\n",
    "    scalers_X = pickle.load(f)\n",
    "\n",
    "with open(r'data\\scalers\\scalers_Y.pkl', 'rb') as f:\n",
    "    scalers_Y = pickle.load(f)\n",
    "\n",
    "print('scalers参数已迁移')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 验证环节\n",
    "def series2U(z, M, scalers_X, input_term, isprint = True):\n",
    "\n",
    "    if(isprint):print(z.shape)\n",
    "\n",
    "    # 将 z 重新整理成 (M, z_dim) 的二维数组\n",
    "    z_reshaped = z.reshape(M, z_dim)\n",
    "\n",
    "    # 将 numpy 数组转换为 tensor，指定 dtype 为 float32\n",
    "    z_tensor = torch.from_numpy(z_reshaped).float()\n",
    "    if(isprint):print(z_tensor.shape)\n",
    "\n",
    "    # z_tensor = torch.randn(2, z_dim)\n",
    "    generated_data = lsgan_generator(z_tensor).detach().numpy()\n",
    "\n",
    "    if(isprint):print(generated_data.shape)\n",
    "    if(isprint):print(generated_data)\n",
    "\n",
    "    # 分别提取 U1, U2, U3, U4\n",
    "    U1 = generated_data[:, 0]\n",
    "    U2 = generated_data[:, 1]\n",
    "    U3 = generated_data[:, 2]\n",
    "    U4 = generated_data[:, 3]\n",
    "\n",
    "    # 将 U1, U2, U3, U4 连接成一个序列\n",
    "    sequence = np.concatenate((U1, U2, U3, U4))\n",
    "\n",
    "    if(isprint):print(\"U1:\", U1)\n",
    "    if(isprint):print(\"U2:\", U2)\n",
    "    if(isprint):print(\"U3:\", U3)\n",
    "    if(isprint):print(\"U4:\", U4)\n",
    "    if(isprint):print(\"Connected sequence:\", sequence)\n",
    "\n",
    "\n",
    "    U1_inverse = scalers_X[input_term[0]].inverse_transform(U1.reshape(-1, 1)).flatten()\n",
    "    U2_inverse = scalers_X[input_term[1]].inverse_transform(U2.reshape(-1, 1)).flatten()\n",
    "    U3_inverse = scalers_X[input_term[2]].inverse_transform(U3.reshape(-1, 1)).flatten()\n",
    "    U4_inverse = scalers_X[input_term[3]].inverse_transform(U4.reshape(-1, 1)).flatten()\n",
    "\n",
    "    if(isprint):print(\"U1_inverse:\", U1_inverse)\n",
    "    if(isprint):print(\"U2_inverse:\", U2_inverse)\n",
    "    if(isprint):print(\"U3_inverse:\", U3_inverse)\n",
    "    if(isprint):print(\"U4_inverse:\", U4_inverse)\n",
    "\n",
    "    return sequence,U1_inverse,U2_inverse,U3_inverse,U4_inverse\n",
    "\n",
    "\n",
    "def series2U_numpy(z, M, scalers_X, input_term, isprint = True):\n",
    "\n",
    "    if(isprint):print(z.shape)\n",
    "\n",
    "    # 生成一些随机数据进行测试\n",
    "    x =  z.reshape(M, z_dim)\n",
    "\n",
    "    # 前向传播\n",
    "    generated_data = lsgan_generator_numpy.forward(x)\n",
    "\n",
    "    if(isprint):print(\"Generated data shape:\", generated_data.shape)\n",
    "    if(isprint):print(\"Generated data:\\n\", generated_data)\n",
    "\n",
    "\n",
    "    # 分别提取 U1, U2, U3, U4\n",
    "    U1 = generated_data[:, 0]\n",
    "    U2 = generated_data[:, 1]\n",
    "    U3 = generated_data[:, 2]\n",
    "    U4 = generated_data[:, 3]\n",
    "\n",
    "    # 将 U1, U2, U3, U4 连接成一个序列\n",
    "    sequence = np.concatenate((U1, U2, U3, U4))\n",
    "\n",
    "    if(isprint):print(\"U1:\", U1)\n",
    "    if(isprint):print(\"U2:\", U2)\n",
    "    if(isprint):print(\"U3:\", U3)\n",
    "    if(isprint):print(\"U4:\", U4)\n",
    "    if(isprint):print(\"Connected sequence:\", sequence)\n",
    "\n",
    "    U1_inverse = scalers_X[input_term[0]].inverse_transform(U1.reshape(-1, 1)).flatten()\n",
    "    U2_inverse = scalers_X[input_term[1]].inverse_transform(U2.reshape(-1, 1)).flatten()\n",
    "    U3_inverse = scalers_X[input_term[2]].inverse_transform(U3.reshape(-1, 1)).flatten()\n",
    "    U4_inverse = scalers_X[input_term[3]].inverse_transform(U4.reshape(-1, 1)).flatten()\n",
    "\n",
    "    if(isprint):print(\"U1_inverse:\", U1_inverse)\n",
    "    if(isprint):print(\"U2_inverse:\", U2_inverse)\n",
    "    if(isprint):print(\"U3_inverse:\", U3_inverse)\n",
    "    if(isprint):print(\"U4_inverse:\", U4_inverse)\n",
    "\n",
    "    return sequence,U1_inverse,U2_inverse,U3_inverse,U4_inverse\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 定义 M 和 z_dim\n",
    "M = 3      # 样本数量\n",
    "\n",
    "\n",
    "# 使用 numpy 生成标准正态分布的随机数，形状为 (z_dim * M,)\n",
    "z = np.random.randn(z_dim * M)\n",
    "# generated_data = series2U(z,M,isprint=True)\n",
    "# generated_data_numpy = series2U_numpy(z,M,isprint=True)\n",
    "\n",
    "# print(\"验证原模型与numpy模型的输出是否一致：\")\n",
    "# result_d_state = np.fabs(generated_data-generated_data_numpy)<1e-6\n",
    "# # print(result_d_state)\n",
    "# print('错误数量：',np.sum(result_d_state==False),'，正确数量：',np.sum(result_d_state==True))\n",
    "\n",
    "\n",
    "generated_data,U1_inverse,U2_inverse,U3_inverse,U4_inverse = series2U_numpy(z,M, scalers_X, input_term,isprint=True)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39_tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
