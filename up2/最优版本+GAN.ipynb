{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 库文件\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import os\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from skopt import gp_minimize\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.font_manager import FontProperties\n",
    "# 设置中文字体\n",
    "font = FontProperties(fname=r\"c:\\windows\\fonts\\simsun.ttc\", size=12)  # 替换为你的中文字体文件路径\n",
    "import sys\n",
    "sys.path.append(r\"C:\\Users\\haokw\\Documents\\GitHub\\gaolu\\MPC\\高炉\")\n",
    "from collections import deque\n",
    "\n",
    "import base \n",
    "# 基础库\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import sys\n",
    "import os\n",
    "import datetime\n",
    "import pickle\n",
    "# 机器学习库\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "# 数据归一化、逆归一化\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# 优化相关库\n",
    "from skopt import gp_minimize\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# 深度学习库\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# 忽略警告\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# 中文字体设置\n",
    "from matplotlib.font_manager import FontProperties\n",
    "font = FontProperties(fname=r\"c:\\windows\\fonts\\simsun.ttc\", size=12)  # 替换为你的中文字体文件路径\n",
    "\n",
    "# 其他路径设置\n",
    "sys.path.append(r\"C:\\Users\\haokw\\Documents\\GitHub\\gaolu\\MPC\\高炉\")\n",
    "\n",
    "# 自定义模块\n",
    "import base \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取Excel文件\n",
    "excel_path = f'C:\\\\Users\\\\haokw\\\\Documents\\\\GitHub\\\\gaolu\\\\up2\\\\data\\\\data.xlsx'\n",
    "df_sheet_params = pd.read_excel(excel_path, sheet_name='X') \n",
    "\n",
    "\n",
    "excel_path = f'C:\\\\Users\\\\haokw\\\\Documents\\\\GitHub\\\\gaolu\\\\up2\\\\data\\\\data.xlsx'\n",
    "df_sheet_yuansu = pd.read_excel(excel_path, sheet_name='Y') \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 检查 DataFrame 中是否包含 NaN 值\n",
    "def check_if_NaN(data):\n",
    "    print(data.shape)\n",
    "    contains_nan = data.isna().any().any()\n",
    "    if contains_nan:\n",
    "        print(\"数据包含 NaN 值\")\n",
    "    else:\n",
    "        print(\"数据不包含 NaN 值\")\n",
    "check_if_NaN(df_sheet_yuansu)\n",
    "check_if_NaN(df_sheet_params)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_term =        ['富氧流量', '设定喷煤量', '热风压力', '热风温度']\n",
    "output_term = ['铁水温度[MIT]', '铁水硅含量[SI]']\n",
    "time_term= '时间戳h'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 异常数据处理-处理前后对比\n",
    "# 创建数据框副本以避免修改原始数据\n",
    "df_sheet_yuansu_process = df_sheet_yuansu.copy()\n",
    "df_sheet_params_process = df_sheet_params.copy()\n",
    "# 定义一个函数，用前后两个值的差值按照距离进行加权替换异常值\n",
    "def replace_outliers_with_weighted_diff(x, y):\n",
    "    # 计算列的中位数\n",
    "    median_value = y.median()\n",
    "    # 检测异常值的索引\n",
    "    outliers_index = (y - median_value).abs() > 2.5 * y.std()\n",
    "    \n",
    "    # 遍历异常值的索引\n",
    "    for idx in outliers_index[outliers_index].index:\n",
    "        # 获取异常值前一个和后一个值的索引\n",
    "        prev_idx = idx - 1 if idx - 1 >= 0 else idx\n",
    "        next_idx = idx + 1 if idx + 1 < len(y) else idx\n",
    "        # 计算当前 x 与前后两个 x 的距离\n",
    "        dist_prev = abs(x[idx] - x[prev_idx])\n",
    "        dist_next = abs(x[next_idx] - x[idx])\n",
    "        total_dist = dist_prev + dist_next\n",
    "        # 计算权重\n",
    "        weight_prev = dist_next / total_dist\n",
    "        weight_next = dist_prev / total_dist\n",
    "        # 计算前后两个值的差值\n",
    "        diff = y[next_idx] - y[prev_idx]\n",
    "        # 根据权重进行插值\n",
    "        interpolated_value = y[prev_idx] + weight_prev * diff\n",
    "        # 用插值结果替代异常值\n",
    "        y[idx] = interpolated_value\n",
    "\n",
    "# 画出数据\n",
    "def plot_subplot(data_x,data_y_yuan,data_y,column):\n",
    "    plt.plot(data_x,data_y_yuan,'r-')\n",
    "    plt.plot(data_x,data_y,'m-')\n",
    "    # plt.xlabel(time_term, fontproperties=font)  # 使用中文标签\n",
    "    plt.ylabel(column, fontproperties=font)  # 使用中文标签\n",
    "    # 使用中文标签\n",
    "\n",
    "\n",
    "# 对指定列应用替代异常值的函数\n",
    "# 对指定列应用替代异常值的函数\n",
    "replace_outliers_with_weighted_diff(df_sheet_params_process[time_term], df_sheet_params_process[input_term[0]])\n",
    "replace_outliers_with_weighted_diff(df_sheet_params_process[time_term], df_sheet_params_process[input_term[1]])\n",
    "replace_outliers_with_weighted_diff(df_sheet_params_process[time_term], df_sheet_params_process[input_term[2]])\n",
    "replace_outliers_with_weighted_diff(df_sheet_params_process[time_term], df_sheet_params_process[input_term[3]])\n",
    "# replace_outliers_with_weighted_diff(df_sheet_params_process[time_term], df_sheet_params_process[input_term[4]])\n",
    "# replace_outliers_with_weighted_diff(df_sheet_params_process[time_term], df_sheet_params_process[input_term[5]])\n",
    "# replace_outliers_with_weighted_diff(df_sheet_params_process[time_term], df_sheet_params_process[input_term[6]])\n",
    "\n",
    "# replace_outliers_with_weighted_diff(df_sheet_yuansu_process[time_term], df_sheet_yuansu_process[output_term[0]])\n",
    "# replace_outliers_with_weighted_diff(df_sheet_yuansu_process[time_term], df_sheet_yuansu_process[output_term[1]])\n",
    "\n",
    "plt.figure(figsize=(15, 4))\n",
    "for idx, column in enumerate(input_term):\n",
    "    plt.subplot(len(input_term), 1, idx+1)\n",
    "    plot_subplot(df_sheet_params_process[time_term].values,df_sheet_params[column].values,df_sheet_params_process[column].values,column)\n",
    "\n",
    "plt.figure(figsize=(15, 2))\n",
    "for idx, column in enumerate(output_term):\n",
    "    plt.subplot(len(output_term), 1, idx+1)\n",
    "    plot_subplot(df_sheet_yuansu_process[time_term].values,df_sheet_yuansu[column].values,df_sheet_yuansu_process[column].values,column)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 画出选取的数据\n",
    "def plot_subplot(data_x,data_y,column,index_predict,index_gaolu):\n",
    "    plt.plot(data_x,data_y,'-', label='origin_data')\n",
    "    plt.plot(data_x[index_gaolu],data_y[index_gaolu],'r.', label='gaolu_data')\n",
    "    plt.plot(data_x[index_predict],data_y[index_predict],'g-', label='predict_data')\n",
    "    plt.legend()\n",
    "    # plt.xlabel(time_term, fontproperties=font)  # 使用中文标签\n",
    "    plt.ylabel(column, fontproperties=font)  # 使用中文标签\n",
    "\n",
    "\n",
    "\n",
    "length1 = 400\n",
    "start1 = 0\n",
    "length2 = 400\n",
    "start2 = 400\n",
    "\n",
    "\n",
    "index_gaolu   = range(start1, start1+length1+1, 1)\n",
    "index_predict     = range(start2, start2+length2+1, 1)\n",
    "# index = range(1, 7572, 1)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15, 6))\n",
    "for idx, column in enumerate(output_term):\n",
    "    plt.subplot(len(input_term+output_term), 1, idx+1)\n",
    "    plot_subplot(df_sheet_yuansu_process[time_term].values,df_sheet_yuansu_process[column].values,column,index_predict,index_gaolu)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据归一化、逆归一化\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# 将数据存储为字典，每个键对应一列数据\n",
    "original_data_dict = {\n",
    "    input_term[0]:   df_sheet_params_process[input_term[0]].values,\n",
    "    input_term[1]:   df_sheet_params_process[input_term[1]].values,\n",
    "    input_term[2]:   df_sheet_params_process[input_term[2]].values,\n",
    "    input_term[3]:   df_sheet_params_process[input_term[3]].values,\n",
    "    # input_term[4]:   df_sheet_params_process[input_term[4]].values,\n",
    "    # input_term[5]:   df_sheet_params_process[input_term[5]].values,\n",
    "    # input_term[6]:   df_sheet_params_process[input_term[6]].values,\n",
    "    output_term[0]:  df_sheet_yuansu_process[output_term[0]].values,\n",
    "    output_term[1]:  df_sheet_yuansu_process[output_term[1]].values\n",
    "}\n",
    "\n",
    "# 初始化缩放器\n",
    "scalers = {}\n",
    "\n",
    "# 进行拟合\n",
    "for column, data in original_data_dict.items():\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    scaler.fit(data.reshape(-1, 1))  # 保证数据是列向量\n",
    "    scalers[column] = scaler\n",
    "\n",
    "# 进行归一化\n",
    "normalized_data_dict = {}\n",
    "for column, scaler in scalers.items():\n",
    "    normalized_data_dict[column] = scaler.transform(original_data_dict[column].reshape(-1, 1)).flatten()\n",
    "\n",
    "# 进行反归一化\n",
    "original_data_dict = {}\n",
    "for column, scaler in scalers.items():\n",
    "    original_data_dict[column] = scaler.inverse_transform(normalized_data_dict[column].reshape(-1, 1)).flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 标定归一化前后数据\n",
    "data_point = np.array([1500]).reshape(-1, 1)\n",
    "data1 = scalers[output_term[0]].transform(data_point).flatten()\n",
    "\n",
    "data_point = np.array(data1).reshape(-1, 1)\n",
    "data2 = scalers[output_term[0]].inverse_transform(data_point).flatten()\n",
    "\n",
    "data_point = np.array([1510]).reshape(-1, 1)\n",
    "data3 = scalers[output_term[0]].transform(data_point).flatten()\n",
    "\n",
    "data_point = np.array(data3).reshape(-1, 1)\n",
    "data4 = scalers[output_term[0]].inverse_transform(data_point).flatten()\n",
    "\n",
    "print(data1)\n",
    "print(data2)\n",
    "print(data3)\n",
    "print(data4)\n",
    "d_temp = (data3-data1)/(data4-data2)\n",
    "print('每摄氏度的输出差：',d_temp)\n",
    "\n",
    "\n",
    "\n",
    "data_point = np.array([0.50]).reshape(-1, 1)\n",
    "data1 = scalers[output_term[1]].transform(data_point).flatten()\n",
    "\n",
    "data_point = np.array(data1).reshape(-1, 1)\n",
    "data2 = scalers[output_term[1]].inverse_transform(data_point).flatten()\n",
    "\n",
    "data_point = np.array([0.60]).reshape(-1, 1)\n",
    "data3 = scalers[output_term[1]].transform(data_point).flatten()\n",
    "\n",
    "data_point = np.array(data3).reshape(-1, 1)\n",
    "data4 = scalers[output_term[1]].inverse_transform(data_point).flatten()\n",
    "\n",
    "print(data1)\n",
    "print(data2)\n",
    "print(data3)\n",
    "print(data4)\n",
    "d_yuansu = (data3-data1)/(data4-data2)\n",
    "print('每浓度的输出差：',(data3-data1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isShuffle = True\n",
    "isShuffle = False\n",
    "time_steps = 2\n",
    "test_size = 0.15\n",
    "val_size = 0.15\n",
    "train_size = 1-val_size-test_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 组合训练数据--拆分训练、测试集\n",
    "\n",
    "# 定义时间步数和特征数\n",
    "\n",
    "# 构成    \n",
    "# X = [X(t),X(t-1),Y(t-1)]\n",
    "# Y = [Y(t)]\n",
    "def make_data(u1_data,u2_data,u3_data,u4_data,y1_data,y2_data,index_fanwei):\n",
    "    X = np.column_stack((u1_data,u2_data,u3_data,u4_data))\n",
    "    y = np.column_stack((y1_data, y2_data))\n",
    "\n",
    "    X_modified = []\n",
    "    y_modified = []\n",
    "    \n",
    "    for i in range(3,len(y1_data)):\n",
    "        if i in index_fanwei:\n",
    "            # print(i)\n",
    "            # print(df_sheet_yuansu[time_term][i])\n",
    "            yuansu_time = df_sheet_yuansu[time_term][i]\n",
    "            closest_10 = df_sheet_params[df_sheet_params[time_term] <= yuansu_time].nlargest(time_steps, time_term)\n",
    "            # print(closest_10)\n",
    "            \n",
    "            index = closest_10.index\n",
    "            # print(index)\n",
    "            # print(closest_10.iloc[-1][time_term])\n",
    "            if closest_10.iloc[-1][time_term] != yuansu_time - time_steps + 1:\n",
    "                print(i,yuansu_time,'errloss')\n",
    "            else:\n",
    "\n",
    "                # print(X[index, :])\n",
    "                new_x_sample = np.concatenate([X[i, :] for i in index],axis=0)\n",
    "                # print(new_x_sample)\n",
    "                y_last = y[i-1, :]\n",
    "                # print(y_last, 'y_last time : ',df_sheet_yuansu[time_term][i-1])\n",
    "                new_x_sample = np.concatenate([new_x_sample,y_last],axis=0)\n",
    "                # print(new_x_sample)\n",
    "                y_sample = y[i, :]  \n",
    "                X_modified.append(new_x_sample)\n",
    "                y_modified.append(y_sample)\n",
    "                print(i,yuansu_time,index[0],index[-1], end='\\r')\n",
    "                # break\n",
    "\n",
    "    # 将列表转换为 NumPy 数组\n",
    "    X_modified = np.array(X_modified)\n",
    "    y_modified = np.array(y_modified)\n",
    "    X_reshaped = X_modified.reshape((X_modified.shape[0], X_modified.shape[1]))\n",
    "\n",
    "    # 打印新数据的形状\n",
    "    print(\"Modified Input Shape:\", X_reshaped.shape)\n",
    "    print(\"Modified Output Shape:\", y_modified.shape)\n",
    "\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_reshaped, y_modified, \n",
    "                                                        test_size=test_size, \n",
    "                                                        random_state=42, \n",
    "                                                        shuffle=isShuffle)\n",
    "\n",
    "    # 将剩余的70%训练数据再次拆分成训练数据和验证数据（20%验证数据，50%训练数据）\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, \n",
    "                                                        test_size=val_size/(train_size+val_size), \n",
    "                                                        random_state=42, \n",
    "                                                        shuffle=isShuffle)\n",
    "\n",
    "    print('训练数量：',X_train.shape,y_train.shape)\n",
    "    print('验证数量：',X_val.shape,y_val.shape)\n",
    "    print('测试数量：',X_test.shape,y_test.shape)\n",
    "\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def symmetrical_moving_average(data, N):\n",
    "    \"\"\"\n",
    "    使用对称的移动平均滤波，当前值由其自身及其前后的值决定。\n",
    "    \n",
    "    :param data: 输入的数据序列，一般为列表或者NumPy数组。\n",
    "    :return: 经过滤波处理的数据序列。\n",
    "    \"\"\"\n",
    "    filtered_data = []\n",
    "    N = 9\n",
    "    percent = 0.8\n",
    "    # 遍历数据，从索引1开始到倒数第二个元素结束\n",
    "    for i in range(1, len(data) - 1):\n",
    "        # 计算当前值及其前后值的平均\n",
    "        average = (data[i - 1]*(1-percent)/2 + data[i]*percent + data[i + 1]*(1-percent)/2)\n",
    "        filtered_data.append(average)\n",
    "    \n",
    "    # 对于序列的第一个和最后一个元素，直接使用原始值\n",
    "    # 或者可以使用其他边界处理策略\n",
    "    filtered_data.insert(0, data[0])\n",
    "    filtered_data.append(data[-1])\n",
    "    \n",
    "    return np.array(filtered_data)\n",
    "\n",
    "# 示例数据\n",
    "data = [2, 4, 6, 8, 10, 12, 14]\n",
    "filtered_data = symmetrical_moving_average(data,9)\n",
    "print(filtered_data)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 高炉模型列数据\n",
    "u1_data = normalized_data_dict[input_term[0]]\n",
    "u2_data = normalized_data_dict[input_term[1]]\n",
    "u3_data = normalized_data_dict[input_term[2]]\n",
    "u4_data = normalized_data_dict[input_term[3]]\n",
    "y1_data = normalized_data_dict[output_term[0]]\n",
    "y2_data = normalized_data_dict[output_term[1]]\n",
    "num_samples = y2_data.shape[0]\n",
    "\n",
    "# filter_windows = 2\n",
    "# u1_data = symmetrical_moving_average(u1_data, filter_windows)\n",
    "# u2_data = symmetrical_moving_average(u2_data, filter_windows)\n",
    "# u3_data = symmetrical_moving_average(u3_data, filter_windows)\n",
    "# u4_data = symmetrical_moving_average(u4_data, filter_windows)\n",
    "# u5_data = symmetrical_moving_average(u5_data, filter_windows)\n",
    "# u6_data = symmetrical_moving_average(u6_data, filter_windows)\n",
    "# u7_data = symmetrical_moving_average(u7_data, filter_windows)\n",
    "# y1_data = symmetrical_moving_average(y1_data, filter_windows)\n",
    "# y2_data = symmetrical_moving_average(y2_data, filter_windows)\n",
    "\n",
    "print('高炉模型数据')\n",
    "X_gaolu_train, X_gaolu_val, X_gaolu_test,\\\n",
    "y_gaolu_train, y_gaolu_val, y_gaolu_test = make_data(u1_data,u2_data,u3_data,u4_data,\n",
    "                                                            y1_data,y2_data,\n",
    "                                                            index_fanwei=index_gaolu)\n",
    "\n",
    "\n",
    "\n",
    "# 预测模型列数据\n",
    "u1_data = normalized_data_dict[input_term[0]]\n",
    "u2_data = normalized_data_dict[input_term[1]]\n",
    "u3_data = normalized_data_dict[input_term[2]]\n",
    "u4_data = normalized_data_dict[input_term[3]]\n",
    "y1_data = normalized_data_dict[output_term[0]]\n",
    "y2_data = normalized_data_dict[output_term[1]]\n",
    "num_samples = y2_data.shape[0]\n",
    "\n",
    "# filter_windows = 2\n",
    "# u1_data = symmetrical_moving_average(u1_data, filter_windows)\n",
    "# u2_data = symmetrical_moving_average(u2_data, filter_windows)\n",
    "# u3_data = symmetrical_moving_average(u3_data, filter_windows)\n",
    "# u4_data = symmetrical_moving_average(u4_data, filter_windows)\n",
    "# u5_data = symmetrical_moving_average(u5_data, filter_windows)\n",
    "# u6_data = symmetrical_moving_average(u6_data, filter_windows)\n",
    "# u7_data = symmetrical_moving_average(u7_data, filter_windows)\n",
    "# y1_data = symmetrical_moving_average(y1_data, filter_windows)\n",
    "# y2_data = symmetrical_moving_average(y2_data, filter_windows)\n",
    "print('预测模型数据')\n",
    "X_predict_train, X_predict_val, X_predict_test,\\\n",
    "y_predict_train, y_predict_val, y_predict_test = make_data(u1_data,u2_data,u3_data,u4_data,\n",
    "                                                            y1_data,y2_data,\n",
    "                                                            index_fanwei=index_predict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_once_time = 50\n",
    "ischuangxin = True\n",
    "# ischuangxin = False\n",
    "cengshu = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义模型\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class MyNeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, if_chuangxin = False,gamma = 0.1):\n",
    "        self.if_chuangxin = if_chuangxin\n",
    "        super(MyNeuralNetwork, self).__init__()\n",
    "        if cengshu == 3:    \n",
    "            if self.if_chuangxin:            \n",
    "                self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "                self.relu = nn.ReLU()\n",
    "                self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "                self.fc3 = nn.Linear(hidden_size, hidden_size)\n",
    "                self.fc4 = nn.Linear(hidden_size, output_size)\n",
    "            else:\n",
    "                self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "                self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "                self.fc3 = nn.Linear(hidden_size, hidden_size)\n",
    "                self.fc4 = nn.Linear(hidden_size, output_size)\n",
    "                self.relu = nn.ReLU()\n",
    "        elif cengshu == 2:  \n",
    "            if self.if_chuangxin:            \n",
    "                self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "                self.relu = nn.ReLU()\n",
    "                self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "                self.fc3 = nn.Linear(hidden_size, output_size)\n",
    "            else:\n",
    "                self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "                self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "                self.fc3 = nn.Linear(hidden_size, output_size)\n",
    "                self.relu = nn.ReLU()\n",
    "\n",
    "\n",
    "    def forward(self, x0):\n",
    "        if cengshu == 3:    \n",
    "            if self.if_chuangxin:\n",
    "\n",
    "                x = self.fc1(x0)\n",
    "                x = self.relu(x)\n",
    "\n",
    "                x2 = self.fc2(x)\n",
    "                x2 = self.relu(x2)\n",
    "\n",
    "                x3 = self.fc3(x2)\n",
    "                x3 = self.relu(x3)\n",
    "\n",
    "                x4 = x + x2 + x3\n",
    "                output = self.fc4(x4)\n",
    "            else:\n",
    "                x = self.fc1(x0)\n",
    "                x = self.relu(x)\n",
    "\n",
    "                x2 = self.fc2(x)\n",
    "                x2 = self.relu(x2)\n",
    "\n",
    "                x3 = self.fc3(x2)\n",
    "                x3 = self.relu(x3)\n",
    "\n",
    "                output = self.fc4(x3)\n",
    "        elif cengshu == 2:  \n",
    "            if self.if_chuangxin:\n",
    "\n",
    "                x = self.fc1(x0)\n",
    "                x = self.relu(x)\n",
    "\n",
    "                x2 = self.fc2(x)\n",
    "                x2 = self.relu(x2)\n",
    "\n",
    "                x3 = x + x2\n",
    "                output = self.fc3(x3)\n",
    "            else:\n",
    "                x = self.fc1(x0)\n",
    "                x = self.relu(x)\n",
    "\n",
    "                x2 = self.fc2(x)\n",
    "                x2 = self.relu(x2)\n",
    "\n",
    "                output = self.fc3(x2)\n",
    "        return output\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    def custom_loss(self, y_true, y_pred):\n",
    "\n",
    "        squared_diff = torch.pow(y_true - y_pred, 2)\n",
    "        sum_squared_diff = torch.sum(squared_diff)\n",
    "        mse = sum_squared_diff / len(y_true)\n",
    "        return mse\n",
    "    \n",
    "\n",
    "    def my_fit(self, \n",
    "                X_train, y_train, \n",
    "                X_val, y_val, \n",
    "                train_loss_list,val_loss_list,\n",
    "                epochs=1, batch_size=32, lr=0.001):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            epoch_loss = 0\n",
    "            for i in range(0, len(X_train), batch_size):\n",
    "                x_batch = torch.tensor(X_train[i:i+batch_size], dtype=torch.float32)\n",
    "                y_batch = torch.tensor(y_train[i:i+batch_size], dtype=torch.float32)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                y_pred = self(x_batch)\n",
    "                loss = self.custom_loss(y_batch, y_pred)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "            average_epoch_train_loss = epoch_loss / (len(X_train) / batch_size)\n",
    "            # 验证集评估\n",
    "            self.eval()\n",
    "            with torch.no_grad():\n",
    "                val_loss = 0\n",
    "                for i in range(0, len(X_val), batch_size):\n",
    "                    x_batch_val = torch.tensor(X_val[i:i+batch_size], dtype=torch.float32)\n",
    "                    y_batch_val = torch.tensor(y_val[i:i+batch_size], dtype=torch.float32)\n",
    "\n",
    "                    y_pred_val = self(x_batch_val)\n",
    "                    val_loss += self.custom_loss(y_batch_val, y_pred_val).item()\n",
    "\n",
    "                average_epoch_val_loss = val_loss / (len(X_val) / batch_size)\n",
    "\n",
    "            print(f'第 {epoch + 1}/{epochs} 轮, 训练误差: {average_epoch_train_loss:.4f}, 验证误差: {average_epoch_val_loss:.4f}', end='\\r')\n",
    "            train_loss_list.append(average_epoch_train_loss)\n",
    "            val_loss_list.append(average_epoch_val_loss)\n",
    "\n",
    "        return train_loss_list,val_loss_list\n",
    "    \n",
    "    def model_update(self, \n",
    "                X_train, y_train, \n",
    "                epochs=1, batch_size=32, lr=0.001,ifprint = False):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "        for epoch in range(epochs):\n",
    "            epoch_loss = 0\n",
    "            for i in range(0, len(X_train), batch_size):\n",
    "                x_batch = torch.tensor(X_train[i:i+batch_size], dtype=torch.float32)\n",
    "                y_batch = torch.tensor(y_train[i:i+batch_size], dtype=torch.float32)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                y_pred = self(x_batch)\n",
    "                loss = self.custom_loss(y_batch, y_pred)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "            average_epoch_train_loss = epoch_loss / (len(X_train) / batch_size)\n",
    "            if(ifprint):print(f'第 {epoch + 1}/{epochs} 轮, 训练误差: {average_epoch_train_loss:.4f}')\n",
    "            \n",
    "            \n",
    "        return 0\n",
    "    \n",
    "    \n",
    "\n",
    "    def my_predict(self, X_test):\n",
    "        # 设置模型为评估模式，这会关闭 dropout 等层\n",
    "        self.eval()\n",
    "        # 将输入数据转换为张量，并设置 requires_grad=True\n",
    "        x_tensor = torch.tensor(X_test, dtype=torch.float32, requires_grad=True)\n",
    "        \n",
    "        # 获取模型的预测输出\n",
    "        y_pred = self(x_tensor)\n",
    "        # 保留预测值的梯度信息\n",
    "        y_pred.retain_grad()\n",
    "        # 返回预测结果和包含梯度信息的张量\n",
    "        return y_pred[:,0].detach().numpy(),y_pred[:,1].detach().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立高炉模型实例\n",
    "input_size = 10  # 输入特征大小\n",
    "hidden_size = 16  # 32\n",
    "output_size = 2  # 输出大小\n",
    "# 设置随机种子\n",
    "torch.manual_seed(0)\n",
    "model_gaolu = MyNeuralNetwork(input_size, \n",
    "                            hidden_size,\n",
    "                            output_size,\n",
    "                            ischuangxin,\n",
    "                            gamma = 0.1)\n",
    "epoch_sum_gaolu = 0\n",
    "gaolu_train_loss_list = []\n",
    "gaolu_val_loss_list = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 高炉模型训练\n",
    "epoch_once = epoch_once_time\n",
    "epoch_sum_gaolu = epoch_sum_gaolu + epoch_once\n",
    "gaolu_train_loss_list,gaolu_val_loss_list = model_gaolu.my_fit(X_gaolu_train, y_gaolu_train,\n",
    "                                    X_gaolu_val, y_gaolu_val, \n",
    "                                    gaolu_train_loss_list, gaolu_val_loss_list,\n",
    "                                    epochs=epoch_once, \n",
    "                                    batch_size=32,\n",
    "                                    lr = 0.002)\n",
    "\n",
    "print('\\nepoch_sum:',epoch_sum_gaolu)\n",
    "\n",
    "# 绘制训练和验证损失曲线\n",
    "plt.plot(gaolu_train_loss_list, label='Train Loss')\n",
    "plt.plot(gaolu_val_loss_list, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用于子图编号的字母序列\n",
    "subplot_labels = ['(a)', '(b)', '(c)', '(d)', '(e)', '(f)', '(g)', '(h)']\n",
    "input_term333 =        ['富氧流量', '设定喷煤量', '热风压力', '热风温度']\n",
    "output_term333 = ['铁水温度MIT', '铁水硅含量[Si]']\n",
    "time_term= '时间戳h'\n",
    "print(input_term333)\n",
    "print(output_term333)\n",
    "input_term222 =        ['富氧流量/(m\\u00b3/h)', '设定喷煤量/(t/h)', '热风压力/kPa', '热风温度/℃']\n",
    "output_term222 = ['铁水温度MIT/℃', '铁水硅含量[Si]/%']\n",
    "time_term= '时间戳h'\n",
    "print(input_term222)\n",
    "print(output_term222)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ditem = -0.65\n",
    "def double_control_train_test_result(scalers, output_term, y_test, y_pred_0, y_pred_1, y_test_2, y_pred_0_2, y_pred_1_2,ditem,a,b):\n",
    "    y_test_0 = scalers[output_term[0]].inverse_transform((y_test[:, 0]).reshape(-1, 1)).flatten()\n",
    "    y_test_1 = scalers[output_term[1]].inverse_transform((y_test[:, 1]).reshape(-1, 1)).flatten()\n",
    "    y_pred_0_inverse_transform = scalers[output_term[0]].inverse_transform((y_pred_0).reshape(-1, 1)).flatten()\n",
    "    y_pred_1_inverse_transform = scalers[output_term[1]].inverse_transform((y_pred_1).reshape(-1, 1)).flatten()\n",
    "\n",
    "    output0 = y_test_0 - y_pred_0_inverse_transform\n",
    "    output1 = y_test_1 - y_pred_1_inverse_transform\n",
    "\n",
    "    plt.figure(figsize=(9, 8))\n",
    "    \n",
    "    \n",
    "    ax = plt.subplot(4, 1, 1)\n",
    "    plt.plot(y_test_0,'k', label=\"真实值\")\n",
    "    plt.plot(y_pred_0_inverse_transform,'r--', label=\"预测值\")\n",
    "    ax.legend(prop=font, loc='upper right', bbox_to_anchor=(a, b), ncol=4) \n",
    "    \n",
    "    plt.ylabel(output_term222[0], fontproperties=font)  \n",
    "    ax.yaxis.set_label_coords(-0.07, 0.5)  \n",
    "    ax.text(0.5, ditem, f'{subplot_labels[0]} {output_term333[0]}建模效果', \n",
    "            transform=ax.transAxes, ha='center', fontproperties=font)  \n",
    "    ax.set_xlabel('训练样本', fontproperties=font)  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ax = plt.subplot(4, 1, 2)\n",
    "    plt.plot(y_test_1,'k')\n",
    "    plt.plot(y_pred_1_inverse_transform,'r--')\n",
    "    plt.ylabel(output_term222[1], fontproperties=font) \n",
    "\n",
    "    ax.yaxis.set_label_coords(-0.07, 0.5)  \n",
    "    ax.text(0.5, ditem, f'{subplot_labels[1]} {output_term333[1]}建模效果', \n",
    "            transform=ax.transAxes, ha='center', fontproperties=font)  \n",
    "    \n",
    "    ax.set_xlabel('训练样本', fontproperties=font)  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    y_test_0 = scalers[output_term[0]].inverse_transform((y_test_2[:, 0]).reshape(-1, 1)).flatten()\n",
    "    y_test_1 = scalers[output_term[1]].inverse_transform((y_test_2[:, 1]).reshape(-1, 1)).flatten()\n",
    "    y_pred_0_inverse_transform = scalers[output_term[0]].inverse_transform((y_pred_0_2).reshape(-1, 1)).flatten()\n",
    "    y_pred_1_inverse_transform = scalers[output_term[1]].inverse_transform((y_pred_1_2).reshape(-1, 1)).flatten()\n",
    "\n",
    "    output0 = y_test_0 - y_pred_0_inverse_transform\n",
    "    output1 = y_test_1 - y_pred_1_inverse_transform\n",
    "\n",
    "    ax = plt.subplot(4, 1, 3)\n",
    "    plt.plot(y_test_0,'k', label=\"真实值\")\n",
    "    plt.plot(y_pred_0_inverse_transform,'r--', label=\"预测值\")\n",
    "    plt.ylabel(output_term222[0], fontproperties=font)  \n",
    "    ax.yaxis.set_label_coords(-0.07, 0.5)  \n",
    "    ax.text(0.5, ditem, f'{subplot_labels[2]} {output_term333[0]}预测效果', \n",
    "            transform=ax.transAxes, ha='center', fontproperties=font)  \n",
    "    ax.set_xlabel('测试样本', fontproperties=font)  \n",
    "\n",
    "\n",
    "\n",
    "    ax = plt.subplot(4, 1, 4)\n",
    "    plt.plot(y_test_1,'k')\n",
    "    plt.plot(y_pred_1_inverse_transform,'r--')\n",
    "    plt.ylabel(output_term222[1], fontproperties=font)  \n",
    "    ax.yaxis.set_label_coords(-0.07, 0.5)  \n",
    "    ax.text(0.5, ditem, f'{subplot_labels[3]} {output_term333[1]}预测效果', \n",
    "            transform=ax.transAxes, ha='center', fontproperties=font)  \n",
    "    ax.set_xlabel('测试样本', fontproperties=font)  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    plt.subplots_adjust(hspace=0.6)  # 调整子图之间的垂直间距\n",
    "    plt.tight_layout()  # 自动调整子图布局\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 高炉模型建模效果\n",
    "y_train_pred_0,y_train_pred_1 = model_gaolu.my_predict(X_gaolu_train)\n",
    "y_test_pred_0,y_test_pred_1 = model_gaolu.my_predict(X_gaolu_test)\n",
    "\n",
    "double_control_train_test_result(scalers,  output_term,\n",
    "                                        y_gaolu_train,  y_train_pred_0, y_train_pred_1,\n",
    "                                        y_gaolu_test ,   y_test_pred_0,  y_test_pred_1,\n",
    "                                        ditem = -0.65   ,a = 0.63, b = 0.38 )\n",
    "\n",
    "# base.double_control_train_test_result(scalers,  output_term,\n",
    "#                                         y_gaolu_train[:-1],  y_train_pred_0[1:], y_train_pred_1[1:],\n",
    "#                                         y_gaolu_test[:-1] ,   y_test_pred_0[1:],  y_test_pred_1[1:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建预测模型实例\n",
    "# 设置随机种子\n",
    "torch.manual_seed(0)\n",
    "model_predict = MyNeuralNetwork(input_size, hidden_size, output_size,ischuangxin)\n",
    "epoch_sum_predict = 0\n",
    "predict_train_loss_list = []\n",
    "predict_val_loss_list = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预测模型训练\n",
    "epoch_once = epoch_once_time\n",
    "epoch_sum = epoch_sum_predict + epoch_once\n",
    "predict_train_loss_list, predict_val_loss_list = model_predict.my_fit(X_predict_train, y_predict_train,\n",
    "                                    X_predict_val, y_predict_val, \n",
    "                                    predict_train_loss_list, predict_val_loss_list,\n",
    "                                    epochs=epoch_once, \n",
    "                                    batch_size=64,\n",
    "                                    lr = 0.002)\n",
    "\n",
    "print('\\nepoch_sum:',epoch_sum_predict)\n",
    "\n",
    "\n",
    "# 绘制训练和验证损失曲线\n",
    "plt.plot(predict_train_loss_list, label='Train Loss')\n",
    "plt.plot(predict_val_loss_list, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预测模型建模效果\n",
    "y_train_pred_0,y_train_pred_1 = model_predict.my_predict(X_predict_train)\n",
    "\n",
    "\n",
    "y_test_pred_0,y_test_pred_1 = model_predict.my_predict(X_predict_test)\n",
    "\n",
    "double_control_train_test_result(scalers,  output_term,\n",
    "                                        y_predict_train,  y_train_pred_0, y_train_pred_1,\n",
    "                                        y_predict_test,   y_test_pred_0,  y_test_pred_1,\n",
    "                                        ditem = -0.65   ,a = 0.31, b = 0.38 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base.gaolu_predict_raw(scalers,output_term,model_predict,model_gaolu,X_predict_test,y_predict_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用NumPy重新构建神经网络架构\n",
    "class MyNeuralNetworkNumpy:\n",
    "    def __init__(self, model, input_size, hidden_size, output_size,ifchuangxin):\n",
    "        self.ifchuangxin = ifchuangxin\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        params = {name: param.detach().numpy() for name, param in model.state_dict().items()}\n",
    "        if cengshu == 3:    \n",
    "            self.weights_fc1 = params['fc1.weight']\n",
    "            self.bias_fc1 = params['fc1.bias']\n",
    "            self.weights_fc2 = params['fc2.weight']\n",
    "            self.bias_fc2 = params['fc2.bias']\n",
    "            self.weights_fc3 = params['fc3.weight']\n",
    "            self.bias_fc3 = params['fc3.bias']\n",
    "            self.weights_fc4 = params['fc4.weight']\n",
    "            self.bias_fc4 = params['fc4.bias']\n",
    "        elif cengshu == 2:  \n",
    "            self.weights_fc1 = params['fc1.weight']\n",
    "            self.bias_fc1 = params['fc1.bias']\n",
    "            self.weights_fc2 = params['fc2.weight']\n",
    "            self.bias_fc2 = params['fc2.bias']\n",
    "            self.weights_fc3 = params['fc3.weight']\n",
    "            self.bias_fc3 = params['fc3.bias']\n",
    "        \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    def relu(self, x):\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if cengshu == 3:    \n",
    "            if self.ifchuangxin:\n",
    "                hidden1 = np.dot(x, self.weights_fc1.T) + self.bias_fc1\n",
    "                hidden1 = self.relu(hidden1)\n",
    "\n",
    "                hidden2 = np.dot(hidden1, self.weights_fc2.T) + self.bias_fc2\n",
    "                hidden2 = self.relu(hidden2)\n",
    "\n",
    "                hidden3 = np.dot(hidden2, self.weights_fc3.T) + self.bias_fc3\n",
    "                hidden3 = self.relu(hidden3)\n",
    "\n",
    "                hidden4 = hidden1 + hidden2 + hidden3\n",
    "                output = np.dot(hidden4, self.weights_fc4.T) + self.bias_fc4\n",
    "\n",
    "            else:\n",
    "                hidden1 = np.dot(x, self.weights_fc1.T) + self.bias_fc1\n",
    "                hidden1 = self.relu(hidden1)\n",
    "\n",
    "                hidden2 = np.dot(hidden1, self.weights_fc2.T) + self.bias_fc2\n",
    "                hidden2 = self.relu(hidden2)\n",
    "\n",
    "                hidden3 = np.dot(hidden2, self.weights_fc3.T) + self.bias_fc3\n",
    "                hidden3 = self.relu(hidden3)\n",
    "\n",
    "                output = np.dot(hidden3, self.weights_fc4.T) + self.bias_fc4\n",
    "        elif cengshu == 2:  \n",
    "            if self.ifchuangxin:\n",
    "                hidden1 = np.dot(x, self.weights_fc1.T) + self.bias_fc1\n",
    "                hidden1 = self.relu(hidden1)\n",
    "\n",
    "                hidden2 = np.dot(hidden1, self.weights_fc2.T) + self.bias_fc2\n",
    "                hidden2 = self.relu(hidden2)\n",
    "\n",
    "                hidden3 = hidden1 + hidden2\n",
    "                output = np.dot(hidden3, self.weights_fc3.T) + self.bias_fc3\n",
    "                # hidden2 = self.relu(hidden2)\n",
    "\n",
    "                # hidden3 = np.concatenate([x, hidden1, hidden2], axis=1)  # 按列连接\n",
    "                # output = np.dot(hidden3, self.weights_fc3.T) + self.bias_fc3\n",
    "\n",
    "            else:\n",
    "                hidden1 = np.dot(x, self.weights_fc1.T) + self.bias_fc1\n",
    "                hidden1 = self.relu(hidden1)\n",
    "\n",
    "                hidden2 = np.dot(hidden1, self.weights_fc2.T) + self.bias_fc2\n",
    "                hidden2 = self.relu(hidden2)\n",
    "\n",
    "                output = np.dot(hidden2, self.weights_fc3.T) + self.bias_fc3\n",
    "\n",
    "\n",
    "            \n",
    "        return output\n",
    "    \n",
    "    \n",
    "    def my_predict(self, data_input):\n",
    "        input = data_input  # 随机初始化一个输入序列\n",
    "        output_prediction = model_numpy.forward(input)\n",
    "        return output_prediction[:,0], output_prediction[:,1]\n",
    "\n",
    "# 使用NumPy模型进行预测\n",
    "model_numpy = MyNeuralNetworkNumpy(model_predict, input_size, hidden_size, output_size,ischuangxin)\n",
    "\n",
    "\n",
    "y_pred_0, y_pred_1 = model_numpy.my_predict(X_predict_test)\n",
    "\n",
    "# 计算 RMSE、MRE\n",
    "y_test = y_predict_test\n",
    "\n",
    "\n",
    "base.double_control_predict_result(scalers,output_term,y_test,y_pred_0,y_pred_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制叠加的散点图矩阵。\n",
    "def plot_scatter_matrix(X_df_normal, X_df_normal_index_gaolu, figsize=(10, 8),font=font, save_path=None):\n",
    "    \"\"\"\n",
    "    绘制叠加的散点图矩阵。\n",
    "\n",
    "    参数:\n",
    "    X_df_normal (DataFrame): 第一组数据。\n",
    "    X_df_normal_index_gaolu (DataFrame): 第二组数据。\n",
    "    font (FontProperties, optional): 字体属性，用于设置标签的字体。\n",
    "    \"\"\"\n",
    "    # 设置颜色和标记\n",
    "    color_left = 'blue'\n",
    "    color_right = 'red'\n",
    "    marker_left = '.'\n",
    "    marker_right = '.'\n",
    "\n",
    "    # 设置数据\n",
    "    df_left = X_df_normal  # 第一组数据\n",
    "    df_right = X_df_normal_index_gaolu  # 第二组数据\n",
    "\n",
    "    # 绘制叠加的散点图矩阵\n",
    "    plt.figure(figsize = figsize)\n",
    "    num_cols = len(df_left.columns)\n",
    "    \n",
    "    for i, col1 in enumerate(df_left.columns):\n",
    "        for j, col2 in enumerate(df_left.columns):\n",
    "            ax = plt.subplot(num_cols, num_cols, i * num_cols + j + 1)\n",
    "            \n",
    "            if i != j:\n",
    "                ax.scatter(df_left[col1], df_left[col2], color=color_left, alpha=0.5, marker=marker_left, label='Left Data' if i == 0 and j == 1 else \"\")\n",
    "                ax.scatter(df_right[col1], df_right[col2], color=color_right, alpha=0.5, marker=marker_right, label='Right Data' if i == 0 and j == 1 else \"\")\n",
    "                ax.set_xlim([-1, 1])\n",
    "                ax.set_ylim([-1, 1])\n",
    "            else:\n",
    "                ax.hist(df_left[col1], bins=50, alpha=0.5, color=color_left)\n",
    "                ax.hist(df_right[col1], bins=50, alpha=0.5, color=color_right)\n",
    "                ax.set_xlim([-1, 1])\n",
    "\n",
    "            if i == num_cols - 1:\n",
    "                ax.set_xlabel(col2, fontproperties=font)\n",
    "            if j == 0:\n",
    "                ax.set_ylabel(col1, fontproperties=font)\n",
    "\n",
    "    # # 添加图例\n",
    "    # plt.legend(loc='upper right', bbox_to_anchor=(1.5, 1))\n",
    "    # 手动添加图例\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    ax.legend(handles, labels, loc='upper right', bbox_to_anchor=(1.5, 1))\n",
    "\n",
    "    # 添加标题并调整布局\n",
    "    plt.suptitle('Overlaid Scatter Matrix of Features', y=1.02)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # 如果提供了保存路径，则保存图像\n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "        plt.close()  # 关闭当前的图像，以节省内存\n",
    "    else:\n",
    "        plt.show()  # 否则显示图像\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制散点图矩阵\n",
    "# 转换为DataFrame\n",
    "print('高炉部分数据')\n",
    "\n",
    "\n",
    "# 转换为DataFrame\n",
    "print('全部数据')\n",
    "X_df_normal = pd.DataFrame(normalized_data_dict, columns=input_term)\n",
    "print(X_df_normal.shape)\n",
    "# check_if_NaN(X_df_normal)\n",
    "Y_df_normal = pd.DataFrame(normalized_data_dict, columns=output_term)\n",
    "# check_if_NaN(Y_df_normal)\n",
    "\n",
    "\n",
    "# 绘制散点图矩阵\n",
    "plot_scatter_matrix(X_df_normal, X_df_normal.loc[index_gaolu], font=font, figsize=(10, 8))\n",
    "# # 绘制散点图矩阵\n",
    "plot_scatter_matrix(Y_df_normal, Y_df_normal.loc[index_gaolu], font=font, figsize=(5, 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 51\u001b[0m\n\u001b[0;32m     37\u001b[0m data_test \u001b[38;5;241m=\u001b[39m generate_triangle_data(vertices, num_samples)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# # 可视化数据\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# plt.figure(figsize=(8, 8))\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# plt.fill(vertices[:, 0], vertices[:, 1], 'lightgray', edgecolor='black', alpha=0.5)  # 填充三角形\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     49\u001b[0m \n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# 将数据转换为 DataFrame\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m data_test \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mDataFrame(data_test, columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mY\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# 在三角形内部生成随机点\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Polygon\n",
    "from matplotlib.path import Path\n",
    "\n",
    "# 设置随机种子\n",
    "np.random.seed(42)\n",
    "\n",
    "def generate_triangle_data(vertices, num_samples):\n",
    "    # 创建三角形的路径\n",
    "    triangle_path = Path(vertices)\n",
    "    \n",
    "    # 初始化点\n",
    "    points = []\n",
    "    \n",
    "    # 在三角形内部生成随机点\n",
    "    while len(points) < num_samples:\n",
    "        # 生成随机点\n",
    "        random_point = np.random.rand(2)\n",
    "        \n",
    "        # 检查点是否在三角形内部\n",
    "        if triangle_path.contains_points([random_point]):\n",
    "            points.append(random_point)\n",
    "    \n",
    "    return np.array(points)\n",
    "\n",
    "# 定义三角形顶点\n",
    "vertices = np.array([\n",
    "    [0, 0],  # 顶点A\n",
    "    [1, 0],  # 顶点B\n",
    "    [0.5, 1]  # 顶点C\n",
    "])\n",
    "\n",
    "# 生成数据\n",
    "num_samples = 10000\n",
    "data_test = generate_triangle_data(vertices, num_samples)\n",
    "\n",
    "# # 可视化数据\n",
    "# plt.figure(figsize=(8, 8))\n",
    "# plt.fill(vertices[:, 0], vertices[:, 1], 'lightgray', edgecolor='black', alpha=0.5)  # 填充三角形\n",
    "# plt.scatter(data_test[:, 0], data_test[:, 1], c='blue', alpha=0.5, s=1)\n",
    "# plt.title('Random Points Inside a Triangle')\n",
    "# plt.xlabel('X')\n",
    "# plt.ylabel('Y')\n",
    "# plt.axis('equal')\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# 将数据转换为 DataFrame\n",
    "data_test = pd.DataFrame(data_test, columns=['X', 'Y'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GAN超参数  训练数据\n",
    "\n",
    "\n",
    "# 将历史数据转换为 PyTorch 张量\n",
    "data_item = X_df_normal\n",
    "# data_item = data_test\n",
    "\n",
    "data = torch.tensor(data_item.values, dtype=torch.float32)\n",
    "df_GAN = data_item\n",
    "test_sample_num = data.shape[0]\n",
    "\n",
    "\n",
    "\n",
    "z_dim = 5  # 随机噪声维度\n",
    "data_dim = data.shape[1]  # 数据维度，4\n",
    "learning_rate = 0.0002\n",
    "num_epochs = 200\n",
    "n_critic = 5  # 每次更新生成器前，更新 Critic 的次数\n",
    "\n",
    "print_piture_d = 10\n",
    "\n",
    "# 设置数据加载器\n",
    "batch_size = 512\n",
    "data_loader = torch.utils.data.DataLoader(data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# W-GAN模型定义和训练\n",
    "# WGAN的生成器\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(256, output_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# WGAN的Critic\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Critic, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# 权重裁剪\n",
    "def weight_clipping(critic, clip_value=0.01):\n",
    "    for param in critic.parameters():\n",
    "        param.data.clamp_(-clip_value, clip_value)\n",
    "\n",
    "# WGAN训练\n",
    "def train_wgan(generator, critic, data_loader, num_epochs, z_dim, clip_value, n_critic, optimizer_G, optimizer_C, output_dir):\n",
    "    output_dir = r\"data\\train_output_picture\\WGAN_training_output2\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    for epoch in range(num_epochs):\n",
    "        for real_data in data_loader:\n",
    "            batch_size = real_data.size(0)\n",
    "\n",
    "            # 更新 Critic\n",
    "            for _ in range(n_critic):\n",
    "                z = torch.randn(batch_size, z_dim)\n",
    "                fake_data = generator(z)\n",
    "\n",
    "                # 计算 Critic 的损失（Wasserstein 距离）\n",
    "                real_output = critic(real_data)\n",
    "                fake_output = critic(fake_data.detach())\n",
    "                c_loss = -torch.mean(real_output) + torch.mean(fake_output)\n",
    "\n",
    "                optimizer_C.zero_grad()\n",
    "                c_loss.backward()\n",
    "                optimizer_C.step()\n",
    "\n",
    "                # 对 Critic 权重进行裁剪\n",
    "                weight_clipping(critic, clip_value)\n",
    "\n",
    "            # 更新生成器\n",
    "            z = torch.randn(batch_size, z_dim)\n",
    "            fake_data = generator(z)\n",
    "            fake_output = critic(fake_data)\n",
    "            g_loss = -torch.mean(fake_output)\n",
    "\n",
    "            optimizer_G.zero_grad()\n",
    "            g_loss.backward()\n",
    "            optimizer_G.step()\n",
    "\n",
    "        print(f\"WGAN_training Epoch [{epoch}/{num_epochs}], c_loss: {c_loss.item():.4f}, g_loss: {g_loss.item():.4f}\")\n",
    "\n",
    "        if epoch % print_piture_d == print_piture_d-1:\n",
    "            # 生成一些数据\n",
    "            z = torch.randn(test_sample_num, z_dim)\n",
    "            generated_data = generator(z).detach().numpy()\n",
    "\n",
    "            # 将生成的数据转换为 DataFrame\n",
    "            generated_df = pd.DataFrame(generated_data, columns=df_GAN.columns)\n",
    "\n",
    "            # 获取当前时间\n",
    "            current_time = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "            # 构建保存路径和文件名\n",
    "            filename = f\"{current_time}_WGAN_Epoch_{epoch+1}_c_loss_{c_loss.item():.4f}_g_loss_{g_loss.item():.4f}.png\"\n",
    "            save_path = os.path.join(output_dir, filename)\n",
    "\n",
    "            # 可视化生成的数据分布（你可以实现自己的可视化函数）\n",
    "            plot_scatter_matrix(df_GAN, generated_df, figsize=(10, 8), font=font, save_path=save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化WGAN  训练\n",
    "wgan_generator = Generator(input_dim=z_dim, output_dim=data_dim)\n",
    "wgan_critic = Critic(input_dim=data_dim)\n",
    "# 初始化优化器\n",
    "optimizer_G = optim.Adam(wgan_generator.parameters(), lr=learning_rate, betas=(0.2, 0.999))\n",
    "optimizer_C = optim.Adam(wgan_critic.parameters(), lr=learning_rate, betas=(0.2, 0.999))\n",
    "\n",
    "# 训练WGAN\n",
    "train_wgan(wgan_generator, wgan_critic, data_loader, \n",
    "                num_epochs=num_epochs, z_dim=z_dim, clip_value=0.01, \n",
    "                n_critic = 5, \n",
    "                optimizer_G=optimizer_G, optimizer_C=optimizer_C, \n",
    "                output_dir=\"wgan_outputs\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存模型参数\n",
    "def save_model(generator, discriminator, output_dir, epoch):\n",
    "        # 创建保存目录（如果不存在）\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        # 定义文件名\n",
    "        generator_filename = os.path.join(output_dir, f\"wgan_generator_epoch_{epoch}.pth\")\n",
    "        discriminator_filename = os.path.join(output_dir, f\"wgan_discriminator_epoch_{epoch}.pth\")\n",
    "\n",
    "        # 保存生成器的模型参数\n",
    "        torch.save(generator.state_dict(), generator_filename)\n",
    "        print(f\"Generator model saved to {generator_filename}\")\n",
    "\n",
    "        # 保存判别器的模型参数\n",
    "        torch.save(discriminator.state_dict(), discriminator_filename)\n",
    "        print(f\"Discriminator model saved to {discriminator_filename}\")\n",
    "\n",
    "save_model(wgan_generator, wgan_critic, \n",
    "        output_dir=r\"data\\model_params\\wgan_model_2\", \n",
    "        epoch=num_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成样本测试\n",
    "z = torch.randn(test_sample_num, z_dim)\n",
    "print(z.shape)\n",
    "\n",
    "generated_data = wgan_generator(z).detach().numpy()\n",
    "print(generated_data.shape)\n",
    "\n",
    "# 将生成的数据转换为 DataFrame\n",
    "generated_df = pd.DataFrame(generated_data, columns=df_GAN.columns)\n",
    "\n",
    "# 可视化生成的数据分布（你可以实现自己的可视化函数）\n",
    "plot_scatter_matrix(df_GAN, generated_df, figsize=(10, 8), font=font)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LS-GAN模型定义和训练\n",
    "class LSGANGenerator(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LSGANGenerator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(256, output_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# LSGAN的判别器\n",
    "class LSGANDiscriminator(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(LSGANDiscriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# # LSGAN的损失函数\n",
    "# def generator_loss(fake_output):\n",
    "#     return torch.mean((fake_output - 1) ** 2)\n",
    "\n",
    "# def discriminator_loss(real_output, fake_output):\n",
    "#     real_loss = torch.mean((real_output - 1) ** 2)\n",
    "#     fake_loss = torch.mean(fake_output ** 2)\n",
    "#     return real_loss + fake_loss\n",
    "\n",
    "\n",
    "# 定义LSGAN损失函数\n",
    "def generator_loss(fake_output, generated_data, real_data):\n",
    "    # LS-GAN 原始损失\n",
    "    lsgan_loss = torch.mean((fake_output - 1) ** 2)\n",
    "    # 生成数据与真实数据的MSE匹配损失\n",
    "    mse_loss = torch.mean((generated_data - real_data) ** 2)\n",
    "    return lsgan_loss + mse_loss\n",
    "\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = torch.mean((real_output - 1) ** 2)\n",
    "    fake_loss = torch.mean(fake_output ** 2)\n",
    "    return real_loss + fake_loss\n",
    "\n",
    "\n",
    "\n",
    "# LSGAN训练\n",
    "def train_lsgan(generator, discriminator, data_loader, num_epochs, z_dim, optimizer_G, optimizer_D, output_dir):\n",
    "    output_dir = r\"data\\train_output_picture\\LS-GAN_training_output2\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    for epoch in range(num_epochs):\n",
    "        for real_data in data_loader:\n",
    "            batch_size = real_data.size(0)\n",
    "\n",
    "            # 生成数据\n",
    "            z = torch.randn(batch_size, z_dim)\n",
    "            fake_data = generator(z)\n",
    "\n",
    "            # 判别器前向传播\n",
    "            real_output = discriminator(real_data)\n",
    "            fake_output = discriminator(fake_data.detach())\n",
    "\n",
    "            # 计算判别器损失\n",
    "            d_loss = discriminator_loss(real_output, fake_output)\n",
    "            optimizer_D.zero_grad()\n",
    "            d_loss.backward()\n",
    "            optimizer_D.step()\n",
    "\n",
    "            # 生成器前向传播\n",
    "            fake_output = discriminator(fake_data)\n",
    "\n",
    "            # 计算生成器损失\n",
    "            g_loss = generator_loss(fake_output, fake_data, real_data)\n",
    "            optimizer_G.zero_grad()\n",
    "            g_loss.backward()\n",
    "            optimizer_G.step()\n",
    "\n",
    "        print(f\"LS-GAN_training Epoch [{epoch}/{num_epochs}], D_loss: {d_loss.item():.4f}, G_loss: {g_loss.item():.4f}\")\n",
    "\n",
    "        if epoch % print_piture_d == print_piture_d-1:\n",
    "            # 生成数据test_sample_num\n",
    "            z = torch.randn(test_sample_num, z_dim)\n",
    "            generated_data = generator(z).detach().numpy()\n",
    "\n",
    "            # 将生成的数据转换为 DataFrame\n",
    "            generated_df = pd.DataFrame(generated_data, columns=df_GAN.columns)\n",
    "\n",
    "            # 获取当前时间\n",
    "            current_time = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "            # 构建保存路径和文件名\n",
    "            filename = f\"{current_time}_LSGAN_Epoch_{epoch+1}_D_loss_{d_loss.item():.4f}_G_loss_{g_loss.item():.4f}.png\"\n",
    "            save_path = os.path.join(output_dir, filename)\n",
    "\n",
    "            # 可视化生成的数据分布（你可以实现自己的可视化函数）\n",
    "            plot_scatter_matrix(df_GAN, generated_df, figsize=(10, 8), font=font, save_path=save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化LSGAN优化器  训练\n",
    "lsgan_generator = LSGANGenerator(input_dim=z_dim, output_dim=data_dim)\n",
    "lsgan_discriminator = LSGANDiscriminator(input_dim=data_dim)\n",
    "\n",
    "optimizer_G = optim.Adam(lsgan_generator.parameters(), lr=learning_rate, betas=(0.2, 0.999))\n",
    "optimizer_D = optim.Adam(lsgan_discriminator.parameters(), lr=learning_rate, betas=(0.2, 0.999))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用WGAN生成的数据作为输入进行LSGAN训练\n",
    "train_lsgan(lsgan_generator, lsgan_discriminator, data_loader, \n",
    "                num_epochs=num_epochs, z_dim=z_dim, \n",
    "                optimizer_G=optimizer_G, optimizer_D=optimizer_D, \n",
    "                output_dir=\"lsgan_outputs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存模型参数\n",
    "def save_model(generator, discriminator, output_dir, epoch):\n",
    "        # 创建保存目录（如果不存在）\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        # 定义文件名\n",
    "        generator_filename = os.path.join(output_dir, f\"lsgan_generator_epoch_{epoch}.pth\")\n",
    "        discriminator_filename = os.path.join(output_dir, f\"lsgan_discriminator_epoch_{epoch}.pth\")\n",
    "\n",
    "        # 保存生成器的模型参数\n",
    "        torch.save(generator.state_dict(), generator_filename)\n",
    "        print(f\"Generator model saved to {generator_filename}\")\n",
    "\n",
    "        # 保存判别器的模型参数\n",
    "        torch.save(discriminator.state_dict(), discriminator_filename)\n",
    "        print(f\"Discriminator model saved to {discriminator_filename}\")\n",
    "save_model(lsgan_generator, lsgan_discriminator, \n",
    "        output_dir=r\"data\\model_params\\lsgan_model_2\", \n",
    "        epoch=num_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成样本测试\n",
    "z = torch.randn(test_sample_num, z_dim)\n",
    "print(z.shape)\n",
    "\n",
    "generated_data = lsgan_generator(z).detach().numpy()\n",
    "print(generated_data.shape)\n",
    "\n",
    "# 将生成的数据转换为 DataFrame\n",
    "generated_df = pd.DataFrame(generated_data, columns=df_GAN.columns)\n",
    "\n",
    "# 可视化生成的数据分布（你可以实现自己的可视化函数）\n",
    "plot_scatter_matrix(df_GAN, generated_df, figsize=(10, 8), font=font)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WGAN的生成器   初始化   加载生成器参数  获取 PyTorch 模型参数  构建numpy版本  初始化numpy生成器\n",
    "\n",
    "# 初始化生成器和判别器\n",
    "wgan_generator_item = Generator(input_dim=z_dim, output_dim=data_dim)\n",
    "\n",
    "\n",
    "# 加载生成器参数\n",
    "generator_path = r\"data\\model_params\\wgan_model\\generator.pth\"\n",
    "wgan_generator_item.load_state_dict(torch.load(generator_path))\n",
    "wgan_generator_item.eval()  # 切换到评估模式\n",
    "print(f\"Generator model loaded from {generator_path}\")\n",
    "\n",
    "\n",
    "# 假设 lsgan_generator_item 已经是训练好的模型\n",
    "def extract_pytorch_params(model):\n",
    "    params = {}\n",
    "    for name, param in model.named_parameters():\n",
    "        params[name] = param.data.numpy()\n",
    "    return params\n",
    "# 获取 PyTorch 模型参数\n",
    "wgan_generator_item_params = extract_pytorch_params(wgan_generator_item)\n",
    "\n",
    "\n",
    "\n",
    "# 构建numpy版本\n",
    "class WGANGeneratorNumpy:\n",
    "    def __init__(self, input_dim, output_dim, params):\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        # 使用从 PyTorch 模型中提取的参数初始化\n",
    "        self.W1 = params['model.0.weight'].T  # 转置以匹配 numpy 矩阵乘法\n",
    "        self.b1 = params['model.0.bias']\n",
    "        \n",
    "        self.W2 = params['model.2.weight'].T\n",
    "        self.b2 = params['model.2.bias']\n",
    "        \n",
    "        self.W3 = params['model.4.weight'].T\n",
    "        self.b3 = params['model.4.bias']\n",
    "    \n",
    "    def relu(self, x):\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    def tanh(self, x):\n",
    "        return np.tanh(x)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        z1 = np.dot(x, self.W1) + self.b1\n",
    "        a1 = self.relu(z1)\n",
    "        \n",
    "        z2 = np.dot(a1, self.W2) + self.b2\n",
    "        a2 = self.relu(z2)\n",
    "        \n",
    "        z3 = np.dot(a2, self.W3) + self.b3\n",
    "        output = self.tanh(z3)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "\n",
    "\n",
    "# 初始化numpy生成器\n",
    "wgan_generator_numpy = WGANGeneratorNumpy(z_dim, data_dim, wgan_generator_item_params)\n",
    "\n",
    "\n",
    "\n",
    "print('参数已迁移')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSGAN的生成器   初始化   加载生成器参数  获取 PyTorch 模型参数  构建numpy版本  初始化numpy生成器\n",
    "\n",
    "# 初始化生成器和判别器\n",
    "lsgan_generator_item = LSGANGenerator(input_dim=z_dim, output_dim=data_dim)\n",
    "\n",
    "\n",
    "# 加载生成器参数\n",
    "generator_path = r\"data\\model_params\\lsgan_model\\generator.pth\"\n",
    "lsgan_generator_item.load_state_dict(torch.load(generator_path))\n",
    "lsgan_generator_item.eval()  # 切换到评估模式\n",
    "print(f\"Generator model loaded from {generator_path}\")\n",
    "\n",
    "\n",
    "# 假设 lsgan_generator_item 已经是训练好的模型\n",
    "def extract_pytorch_params(model):\n",
    "    params = {}\n",
    "    for name, param in model.named_parameters():\n",
    "        params[name] = param.data.numpy()\n",
    "    return params\n",
    "# 获取 PyTorch 模型参数\n",
    "lsgan_generator_item_params = extract_pytorch_params(lsgan_generator_item)\n",
    "\n",
    "\n",
    "\n",
    "# 构建numpy版本\n",
    "class LSGANGeneratorNumpy:\n",
    "    def __init__(self, input_dim, output_dim, params):\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        # 使用从 PyTorch 模型中提取的参数初始化\n",
    "        self.W1 = params['model.0.weight'].T  # 转置以匹配 numpy 矩阵乘法\n",
    "        self.b1 = params['model.0.bias']\n",
    "        \n",
    "        self.W2 = params['model.2.weight'].T\n",
    "        self.b2 = params['model.2.bias']\n",
    "        \n",
    "        self.W3 = params['model.4.weight'].T\n",
    "        self.b3 = params['model.4.bias']\n",
    "    \n",
    "    def relu(self, x):\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    def tanh(self, x):\n",
    "        return np.tanh(x)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        z1 = np.dot(x, self.W1) + self.b1\n",
    "        a1 = self.relu(z1)\n",
    "        \n",
    "        z2 = np.dot(a1, self.W2) + self.b2\n",
    "        a2 = self.relu(z2)\n",
    "        \n",
    "        z3 = np.dot(a2, self.W3) + self.b3\n",
    "        output = self.tanh(z3)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "\n",
    "\n",
    "# 初始化numpy生成器\n",
    "lsgan_generator_numpy = LSGANGeneratorNumpy(z_dim, data_dim, lsgan_generator_item_params)\n",
    "\n",
    "\n",
    "\n",
    "print('参数已迁移')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generated_numpy = wgan_generator_numpy\n",
    "# generated_torch = wgan_generator\n",
    "\n",
    "generated_numpy = lsgan_generator_numpy\n",
    "generated_torch = lsgan_generator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 验证环节\n",
    "def series2U(z, M, generator, scalers_X, input_term, isprint = True):\n",
    "    if(isprint):print(z.shape)\n",
    "\n",
    "    # 将 z 重新整理成 (M, z_dim) 的二维数组\n",
    "    z_reshaped = z.reshape(M, z_dim)\n",
    "\n",
    "    # 将 numpy 数组转换为 tensor，指定 dtype 为 float32\n",
    "    z_tensor = torch.from_numpy(z_reshaped).float()\n",
    "    if(isprint):print(z_tensor.shape)\n",
    "\n",
    "    # z_tensor = torch.randn(2, z_dim)\n",
    "    generated_data = generator(z_tensor).detach().numpy()\n",
    "\n",
    "    if(isprint):print(generated_data.shape)\n",
    "    if(isprint):print(generated_data)\n",
    "\n",
    "    # 分别提取 U1, U2, U3, U4\n",
    "    U1 = generated_data[:, 0]\n",
    "    U2 = generated_data[:, 1]\n",
    "    U3 = generated_data[:, 2]\n",
    "    U4 = generated_data[:, 3]\n",
    "\n",
    "    # 将 U1, U2, U3, U4 连接成一个序列\n",
    "    sequence = np.concatenate((U1, U2, U3, U4))\n",
    "\n",
    "    if(isprint):print(\"U1:\", U1)\n",
    "    if(isprint):print(\"U2:\", U2)\n",
    "    if(isprint):print(\"U3:\", U3)\n",
    "    if(isprint):print(\"U4:\", U4)\n",
    "    if(isprint):print(\"Connected sequence:\", sequence)\n",
    "\n",
    "\n",
    "    U1_inverse = scalers_X[input_term[0]].inverse_transform(U1.reshape(-1, 1)).flatten()\n",
    "    U2_inverse = scalers_X[input_term[1]].inverse_transform(U2.reshape(-1, 1)).flatten()\n",
    "    U3_inverse = scalers_X[input_term[2]].inverse_transform(U3.reshape(-1, 1)).flatten()\n",
    "    U4_inverse = scalers_X[input_term[3]].inverse_transform(U4.reshape(-1, 1)).flatten()\n",
    "\n",
    "    if(isprint):print(\"U1_inverse:\", U1_inverse)\n",
    "    if(isprint):print(\"U2_inverse:\", U2_inverse)\n",
    "    if(isprint):print(\"U3_inverse:\", U3_inverse)\n",
    "    if(isprint):print(\"U4_inverse:\", U4_inverse)\n",
    "\n",
    "    return sequence\n",
    "\n",
    "\n",
    "def series2U_numpy(z, M, generator, scalers_X, input_term, isprint = True):\n",
    "\n",
    "    if(isprint):print(z.shape)\n",
    "\n",
    "    # 生成一些随机数据进行测试\n",
    "    x =  z.reshape(M, z_dim)\n",
    "\n",
    "    # 前向传播\n",
    "    generated_data = generator.forward(x)\n",
    "\n",
    "    if(isprint):print(\"Generated data shape:\", generated_data.shape)\n",
    "    if(isprint):print(\"Generated data:\\n\", generated_data)\n",
    "\n",
    "\n",
    "    # 分别提取 U1, U2, U3, U4\n",
    "    U1 = generated_data[:, 0]\n",
    "    U2 = generated_data[:, 1]\n",
    "    U3 = generated_data[:, 2]\n",
    "    U4 = generated_data[:, 3]\n",
    "\n",
    "    # 将 U1, U2, U3, U4 连接成一个序列\n",
    "    sequence = np.concatenate((U1, U2, U3, U4))\n",
    "\n",
    "    if(isprint):print(\"U1:\", U1)\n",
    "    if(isprint):print(\"U2:\", U2)\n",
    "    if(isprint):print(\"U3:\", U3)\n",
    "    if(isprint):print(\"U4:\", U4)\n",
    "    if(isprint):print(\"Connected sequence:\", sequence)\n",
    "\n",
    "    U1_inverse = scalers_X[input_term[0]].inverse_transform(U1.reshape(-1, 1)).flatten()\n",
    "    U2_inverse = scalers_X[input_term[1]].inverse_transform(U2.reshape(-1, 1)).flatten()\n",
    "    U3_inverse = scalers_X[input_term[2]].inverse_transform(U3.reshape(-1, 1)).flatten()\n",
    "    U4_inverse = scalers_X[input_term[3]].inverse_transform(U4.reshape(-1, 1)).flatten()\n",
    "\n",
    "    if(isprint):print(\"U1_inverse:\", U1_inverse)\n",
    "    if(isprint):print(\"U2_inverse:\", U2_inverse)\n",
    "    if(isprint):print(\"U3_inverse:\", U3_inverse)\n",
    "    if(isprint):print(\"U4_inverse:\", U4_inverse)\n",
    "\n",
    "    return sequence\n",
    "\n",
    "\n",
    "# 定义 M 和 z_dim\n",
    "test_size_generated_data = 3      # 样本数量\n",
    "\n",
    "# 使用 numpy 生成标准正态分布的随机数，形状为 (z_dim * M,)\n",
    "z = np.random.randn(z_dim * test_size_generated_data)\n",
    "generated_data          = series2U      (z,test_size_generated_data,generated_torch, scalers, input_term, isprint=False)\n",
    "generated_data_numpy    = series2U_numpy(z,test_size_generated_data,generated_numpy ,scalers, input_term, isprint=False)\n",
    "\n",
    "print(\"验证原模型与numpy模型的输出是否一致：\")\n",
    "result_d_state = np.fabs(generated_data-generated_data_numpy)<1e-6\n",
    "# print(result_d_state)\n",
    "print('总数量：',test_size_generated_data*4,',错误数量：',np.sum(result_d_state==False),'，正确数量：',np.sum(result_d_state==True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从文件中加载 scalers_X 和 scalers_Y\n",
    "with open(r'data\\scalers\\scalers_X.pkl', 'rb') as f:\n",
    "    scalers_X_GAN = pickle.load(f)\n",
    "\n",
    "with open(r'data\\scalers\\scalers_Y.pkl', 'rb') as f:\n",
    "    scalers_Y = pickle.load(f)\n",
    "\n",
    "print('scalers参数已迁移')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def series2U_numpy_in_control(z, M, generator_numpy,scalers_X, scalers, input_term, isprint = True):\n",
    "\n",
    "    if(isprint):print(z.shape)\n",
    "\n",
    "    # 生成一些随机数据进行测试\n",
    "    x =  z.reshape(M, z_dim)\n",
    "\n",
    "    # 前向传播\n",
    "    generated_data = generator_numpy.forward(x)\n",
    "\n",
    "    if(isprint):print(\"Generated data shape:\", generated_data.shape)\n",
    "    if(isprint):print(\"Generated data:\\n\", generated_data)\n",
    "\n",
    "\n",
    "    # 分别提取 U1, U2, U3, U4\n",
    "    U1 = generated_data[:, 0]\n",
    "    U2 = generated_data[:, 1]\n",
    "    U3 = generated_data[:, 2]\n",
    "    U4 = generated_data[:, 3]\n",
    "\n",
    "    if(isprint):print(\"U1:\", U1)\n",
    "    if(isprint):print(\"U2:\", U2)\n",
    "    if(isprint):print(\"U3:\", U3)\n",
    "    if(isprint):print(\"U4:\", U4)\n",
    "\n",
    "    U1_inverse = scalers_X[input_term[0]].inverse_transform(U1.reshape(-1, 1)).flatten()\n",
    "    U2_inverse = scalers_X[input_term[1]].inverse_transform(U2.reshape(-1, 1)).flatten()\n",
    "    U3_inverse = scalers_X[input_term[2]].inverse_transform(U3.reshape(-1, 1)).flatten()\n",
    "    U4_inverse = scalers_X[input_term[3]].inverse_transform(U4.reshape(-1, 1)).flatten()\n",
    "\n",
    "    if(isprint):print(\"U1_inverse:\", U1_inverse)\n",
    "    if(isprint):print(\"U2_inverse:\", U2_inverse)\n",
    "    if(isprint):print(\"U3_inverse:\", U3_inverse)\n",
    "    if(isprint):print(\"U4_inverse:\", U4_inverse)\n",
    "\n",
    "    \n",
    "    U1_inverse_trans = scalers[input_term[0]].transform(U1_inverse.reshape(-1, 1)).flatten()\n",
    "    U2_inverse_trans = scalers[input_term[1]].transform(U2_inverse.reshape(-1, 1)).flatten()\n",
    "    U3_inverse_trans = scalers[input_term[2]].transform(U3_inverse.reshape(-1, 1)).flatten()\n",
    "    U4_inverse_trans = scalers[input_term[3]].transform(U4_inverse.reshape(-1, 1)).flatten()\n",
    "\n",
    "    \n",
    "    if(isprint):print(\"U1_inverse_trans:\", U1_inverse_trans)\n",
    "    if(isprint):print(\"U2_inverse_trans:\", U2_inverse_trans)\n",
    "    if(isprint):print(\"U3_inverse_trans:\", U3_inverse_trans)\n",
    "    if(isprint):print(\"U4_inverse_trans:\", U4_inverse_trans)\n",
    "\n",
    "    # 将 U1, U2, U3, U4 连接成一个序列\n",
    "    sequence = np.concatenate((U1_inverse_trans, U2_inverse_trans, U3_inverse_trans, U4_inverse_trans))\n",
    "    if(isprint):print(\"Connected sequence:\", sequence)\n",
    "\n",
    "    return sequence\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试series2U_numpy_in_control生成\n",
    "z = np.random.randn(z_dim * test_size_generated_data)\n",
    "generated_data = series2U_numpy_in_control(z,test_size_generated_data,  generated_numpy,scalers_X_GAN, scalers, input_term, isprint=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iscontrol = True\n",
    "# iscontrol = False\n",
    "Times = 200\n",
    "# 过度系数  0.1 越小过度越快\n",
    "rou = 0.1\n",
    "if_add_noise = 0\n",
    "if_gaolu_is_predict = 0\n",
    "if_update_model = True\n",
    "# if_update_model = False\n",
    "maxlen = 300\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成参考轨迹\n",
    "def get_yr(aim_value,current_value,alpha,P):\n",
    "    # 生成设定信号\n",
    "    setpoint_signal = np.full(10, aim_value)\n",
    "    # 初始化参数\n",
    "    alpha = alpha\n",
    "    y_r = np.zeros(P)\n",
    "    y_r[0] = current_value\n",
    "    # 模拟一阶模型\n",
    "    for k in range(1,P):\n",
    "        y_r[k] = alpha * y_r[k-1] + (1 - alpha) * aim_value\n",
    "\n",
    "    # # 绘制结果\n",
    "    # plt.plot(setpoint_signal, label='Setpoint Signal')\n",
    "    # plt.plot(y_r,'o-', label='Output Signal (Tracked)')\n",
    "    # plt.legend()\n",
    "    # plt.xlabel('Time')\n",
    "    # plt.ylabel('Amplitude')\n",
    "    # plt.title('Tracking Setpoint Signal with One-Order Model')\n",
    "    # plt.show()\n",
    "    return y_r\n",
    "# 测试\n",
    "y_r = get_yr(1,-0.5,rou,20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 生成期望数据\n",
    "\n",
    "def generate_y_aim_data(Times):\n",
    "    if Times == 400:\n",
    "        set_y1 = np.repeat(np.arange(1455, 1560, 5), 20)[10:410]\n",
    "        set_y2 = np.repeat(np.arange(0.34, 0.76, 0.02), 20)[0:400]\n",
    "        \n",
    "    elif Times == 200:\n",
    "        set_y1 = np.repeat(np.arange(1455, 1560, 5), 20)[10+75:410-125]\n",
    "        set_y2 = np.repeat(np.arange(0.34, 0.76, 0.02), 20)[0+75:400-125]\n",
    "\n",
    "    elif Times == 1000:\n",
    "        set_y1 = np.repeat(np.arange(1455, 1560, 5), 20)[10:410]\n",
    "        set_y2 = np.repeat(np.arange(0.34, 0.76, 0.02), 20)[0:400]\n",
    "        set_y1 = np.repeat(np.arange(1457.5, 1562.5, 5), 20)[10:410]\n",
    "        set_y2 = np.repeat(np.arange(0.35, 0.77, 0.02), 20)[0:400]\n",
    "\n",
    "    else:\n",
    "        set_y1 = np.full(Times,1500)\n",
    "        set_y2 = np.full(Times,0.45)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    set_y1_trans = scalers[output_term[0]].transform(set_y1.reshape(-1,1)).flatten()\n",
    "    set_y2_trans = scalers[output_term[1]].transform(set_y2.reshape(-1,1)).flatten()\n",
    "\n",
    "    return set_y1, set_y2, set_y1_trans, set_y2_trans\n",
    "set_y1, set_y2, set_y1_trans, set_y2_trans = generate_y_aim_data(Times)\n",
    "print(set_y1.shape)\n",
    "print(set_y1.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#生成控制时域的数据格式\n",
    "def generate_k_data(u1_data, u2_data, u3_data, u4_data, y1_data,y2_data, num_samples, P):\n",
    "    nearest_index = np.abs(y1_data - (-0.5)).argmin()\n",
    "    # 生成随机索引值\n",
    "    #从原有数据的randint时刻开始往下进行控制\n",
    "    randint = np.random.randint(1, num_samples - 2 - P - 1)\n",
    "    randint = nearest_index  # 如果你希望使用固定的值而不是随机生成\n",
    "    # randint = 250  # 如果你希望使用固定的值而不是随机生成\n",
    "    print(randint)\n",
    "    # 提取数据并构成 k_data\n",
    "    # 第一次得到下面五个变量，固定好格式构成k_data\n",
    "    u1   = u1_data[randint  :randint+3  ]\n",
    "    u2   = u2_data[randint  :randint+3  ]\n",
    "    u3   = u3_data[randint  :randint+3  ]\n",
    "    u4   = u4_data[randint  :randint+3  ]\n",
    "\n",
    "    y1   = y1_data[randint  :randint+3  ]\n",
    "    y2   = y2_data[randint  :randint+3  ]\n",
    "    k_data = np.concatenate((u1, u2, u3, u4, y1, y2), axis=0)\n",
    "    print(k_data.shape)\n",
    "\n",
    "    k_data = np.zeros_like(k_data)\n",
    "    return k_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义单时刻的MPC问题优化\n",
    "def my_MPC(k_data,params,M,P,y1_aim,y2_aim,isprint):\n",
    "\n",
    "    h1 = 1.0\n",
    "    h2 = 1.0\n",
    "    # lamda1 = 0.08\n",
    "    lamda1 = 0.001\n",
    "    lamda2 = lamda1\n",
    "    lamda3 = lamda1\n",
    "    lamda4 = lamda1\n",
    "    y1_percent = 1.0\n",
    "    y2_percent = 1.0\n",
    "\n",
    "    # 从固定格式k_data里面读取信息\n",
    "    u1   = k_data[0:3]\n",
    "    u2   = k_data[3:6]\n",
    "    u3   = k_data[6:9]\n",
    "    u4   = k_data[9:12]\n",
    "\n",
    "    y1   = k_data[12:15]\n",
    "    y2   = k_data[15:18]\n",
    "\n",
    "    \n",
    "    # 获取猜测值[h U1 U2]\n",
    "    # h, U1, U2  =params[0], params[1:M+1],params[M+1:]\n",
    "    params = series2U_numpy_in_control(params,M, generated_numpy,\n",
    "                                        scalers_X_GAN, scalers,\n",
    "                                        input_term, isprint=False)\n",
    "    U1, U2, U3, U4  =params[0:M], params[M:2*M],params[2*M:3*M], params[3*M:4*M]\n",
    "    \n",
    "    # 整理数据见   MPC推到.escel\n",
    "    u1   = np.concatenate((u1,U1,U1[-1]*np.ones(P-M)))\n",
    "    u2   = np.concatenate((u2,U2,U2[-1]*np.ones(P-M)))\n",
    "    u3   = np.concatenate((u3,U3,U3[-1]*np.ones(P-M)))\n",
    "    u4   = np.concatenate((u4,U4,U4[-1]*np.ones(P-M)))\n",
    "    y1   = np.concatenate((y1,np.zeros(P)))\n",
    "    y2   = np.concatenate((y2,np.zeros(P)))\n",
    "    if isprint:\n",
    "        print(u1.round(4))\n",
    "        print(u2.round(4))\n",
    "        print(u3.round(4))\n",
    "        print(u4.round(4))\n",
    "        print(y1.round(4))    \n",
    "        print(y2.round(4))\n",
    "        print('开始预测')\n",
    "\n",
    "    y1_k = y1[2]\n",
    "    y2_k = y2[2]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # 总共预测 P+1 次\n",
    "    # 对k时刻进行预测-----1次\n",
    "    for j in range(1):   # j = 0\n",
    "        x = np.column_stack((   u1[j+2],u2[j+2],u3[j+2],u4[j+2],\n",
    "                                u1[j+1],u2[j+1],u3[j+1],u4[j+1],\n",
    "                                y1[j+1],y2[j+1]))\n",
    "        # x = x.reshape((x.shape[0], 1, x.shape[1]))\n",
    "        y1_m_k, y2_m_k = model_numpy.my_predict(x)\n",
    "        E1_k = y1_k - y1_m_k\n",
    "        E2_k = y2_k - y2_m_k\n",
    "        if isprint:\n",
    "            print(j,'mode = 0')\n",
    "            print(x.round(4))\n",
    "            print(y1_k.round(4),y2_k.round(4))\n",
    "            print(y1_m_k.round(4),y2_m_k.round(4))\n",
    "\n",
    "    # 对控制时刻进行预测-----M次\n",
    "    for j in range(1,M+1):  # j = 1,2\n",
    "        x = np.column_stack((   u1[j+2],u2[j+2],u3[j+2],u4[j+2],\n",
    "                                u1[j+1],u2[j+1],u3[j+1],u4[j+1],\n",
    "                                y1[j+1],y2[j+1]))\n",
    "        # x = x.reshape((x.shape[0], 1, x.shape[1]))\n",
    "        y1_k_j, y2_k_j = model_numpy.my_predict(x)\n",
    "        y1[j+2] = y1_k_j.item()\n",
    "        y2[j+2] = y2_k_j.item()\n",
    "        if isprint:\n",
    "            print(j,'mode = 1')\n",
    "            print(x.round(4))\n",
    "            print(y1_k_j.round(4),y2_k_j.round(4))\n",
    "            print('更新后:')\n",
    "            print(u1.round(4))\n",
    "            print(u2.round(4))\n",
    "            print(u3.round(4))\n",
    "            print(u4.round(4))\n",
    "            print(u5.round(4))\n",
    "            print(u6.round(4))\n",
    "            print(u7.round(4))\n",
    "            print(y1.round(4))    \n",
    "            print(y2.round(4))\n",
    "\n",
    "    # 对控制时域外的部分进行预测-----P-M次\n",
    "    # 注意：这部分的信号是保持控制不变下进行\n",
    "    for j in range(M+1,P+1):  #j = 3,4\n",
    "        x = np.column_stack((   u1[j+2],u2[j+2],u3[j+2],u4[j+2],\n",
    "                                u1[j+1],u2[j+1],u3[j+1],u4[j+1],\n",
    "                                y1[j+1],y2[j+1]))\n",
    "        # x = x.reshape((x.shape[0], 1, x.shape[1]))\n",
    "        y1_k_j, y2_k_j = model_numpy.my_predict(x)\n",
    "        y1[j+2] = y1_k_j.item()#将预测值作为下一步的输出值\n",
    "        y2[j+2] = y2_k_j.item()\n",
    "        if isprint:\n",
    "            print(j,'mode = 2')\n",
    "            print(x.round(4))\n",
    "            print(y1_k_j.round(4),y2_k_j.round(4))\n",
    "            print('更新后:')\n",
    "            print(u1.round(4))\n",
    "            print(u2.round(4))\n",
    "            print(u3.round(4))\n",
    "            print(u4.round(4))\n",
    "            print(y1.round(4))    \n",
    "            print(y2.round(4))\n",
    "\n",
    "\n",
    "\n",
    "    k_data2 = np.concatenate((u1[1:4],u2[1:4],u3[1:4],u4[1:4],y1[1:4],y2[1:4]),axis=0)\n",
    "    if isprint:\n",
    "        print('更新k_data')\n",
    "        print(k_data2.round(4))\n",
    "\n",
    "\n",
    "    #获取参考轨迹\n",
    "    # 一定要对照好做差的序列\n",
    "    y1_r_aim  = get_yr(y1_aim,y1_k,rou,P+1)\n",
    "    y1_r = y1_r_aim[1:] \n",
    "\n",
    "\n",
    "    y2_r_aim  = get_yr(y2_aim,y2_k,rou,P+1)\n",
    "    y2_r = y2_r_aim[1:] \n",
    "\n",
    "    y1_M_k = y1[3:]\n",
    "    y2_M_k = y2[3:]\n",
    "    if isprint==1:\n",
    "        print('反馈补偿:')\n",
    "        print('y1_k',y1_k.round(4))  \n",
    "        print('y1_m_k',y1_m_k.round(4))    \n",
    "        print('h*E1_k',(h1*E1_k).round(4)) \n",
    "        print('y2_k',y2_k.round(4))  \n",
    "        print('y2_m_k',y2_m_k.round(4))   \n",
    "        print('h*E2_k',(h2*E2_k).round(4))\n",
    "\n",
    "        print('temp:')\n",
    "        print('y1_aim',y1_aim.round(4))\n",
    "        print('y1_r_aim',y1_r_aim.round(4))\n",
    "        print('y1_r',y1_r.round(4))\n",
    "        print('y1_M_k',y1_M_k.round(4))\n",
    "        print('y1_M_k+h1*E1_k',(y1_M_k+h1*E1_k).round(4))\n",
    "\n",
    "        print('Si_percent:')\n",
    "        print('y2_aim',y2_aim.round(4))\n",
    "        print('y2_r_aim',y2_r_aim.round(4))\n",
    "        print('y2_r',y2_r.round(4))\n",
    "        print('y2_M_k',y2_M_k.round(4))\n",
    "        print('y2_M_k+h2*E2_k',(y2_M_k+h2*E2_k).round(4))\n",
    "\n",
    "        print('u:')\n",
    "        print(u1[2:].round(4))\n",
    "        print(u2[2:].round(4))\n",
    "        print(u3[2:].round(4))\n",
    "        print(u4[2:].round(4))\n",
    "        \n",
    "    # 计算mse\n",
    "    # lamda1太大的话会导致y1_r和y1_M_k的误差加大*****************导致超调的原因\\与目标值之间存在间隙\n",
    "\n",
    "\n",
    "    y1_err = y1_percent*np.sum((y1_r-(y1_M_k+h1*E1_k))**2) \n",
    "    y2_err = y2_percent*np.sum((y2_r-(y2_M_k+h2*E2_k))**2) \n",
    "    u1_power = lamda1*np.sum((np.diff(u1[2:]))**2)\n",
    "    u2_power = lamda2*np.sum((np.diff(u2[2:]))**2)\n",
    "    u3_power = lamda3*np.sum((np.diff(u3[2:]))**2)\n",
    "    u4_power = lamda4*np.sum((np.diff(u4[2:]))**2)\n",
    "\n",
    "    # y1_err = y1_percent*np.sum(np.fabs(y1_r-(y1_M_k+h1*E1_k))) \n",
    "    # y2_err = y2_percent*np.sum(np.fabs(y2_r-(y2_M_k+h2*E2_k))) \n",
    "    # u1_power = lamda1*np.sum((np.fabs(np.diff(u1))))\n",
    "    # u2_power = lamda2*np.sum((np.fabs(np.diff(u2))))\n",
    "    # u3_power = lamda3*np.sum((np.fabs(np.diff(u3))))\n",
    "    # u4_power = lamda4*np.sum((np.fabs(np.diff(u4))))\n",
    "    # u5_power = lamda2*np.sum((np.fabs(np.diff(u5))))\n",
    "    # u6_power = lamda3*np.sum((np.fabs(np.diff(u6))))\n",
    "    # u7_power = lamda4*np.sum((np.fabs(np.diff(u7))))\n",
    "\n",
    "    mse = (0\n",
    "            +y1_err\n",
    "            +y2_err\n",
    "            +u1_power\n",
    "            +u2_power\n",
    "            +u3_power\n",
    "            +u4_power\n",
    "            )\n",
    "    \n",
    "    # print('mse {:.7f}'.format(mse))\n",
    "    if isprint==1:\n",
    "        print('mse {:.7f}'.format(mse))\n",
    "        print('1111 {:.7f}'.format(y1_err))\n",
    "        print('2222 {:.7f}'.format(y2_err))\n",
    "        print('1111 {:.7f}'.format(u1_power))\n",
    "        print('2222 {:.7f}'.format(u2_power))\n",
    "        print('3333 {:.7f}'.format(u3_power))\n",
    "        print('4444 {:.7f}'.format(u4_power))\n",
    "\n",
    "\n",
    "\n",
    "    return mse , k_data2, E1_k*h1,  E2_k*h2\n",
    "    # return mse , k_data2, E1_k*h1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成高斯噪声,设置随机种子，以便结果可重现\n",
    "np.random.seed(42)\n",
    "gaussian_noise_SI = np.random.normal(0,d_yuansu*0.001,Times)\n",
    "gaussian_noise_TEMP = np.random.normal(0,d_temp*0.1,Times)\n",
    "# plt.subplot(2, 1, 1)\n",
    "# plt.plot(gaussian_noise_SI)\n",
    "# plt.subplot(2, 1, 2)\n",
    "# plt.plot(gaussian_noise_TEMP)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_model(model_predict,model_gaolu,x,gaolu_data_past_x,gaolu_data_past_y,ifprint = False):\n",
    "    y1_pred, y2_pred = model_gaolu.my_predict(x)\n",
    "    y_label = np.column_stack((y1_pred, y2_pred))\n",
    "    gaolu_data_past_x.append(x)\n",
    "    gaolu_data_past_y.append(y_label)\n",
    "\n",
    "    # print(f\"After adding th element:\")\n",
    "    # print(\"gaolu_data_past_x:\", list(gaolu_data_past_x))\n",
    "    # print(\"gaolu_data_past_y:\", list(gaolu_data_past_y))\n",
    "    # print()\n",
    "    \n",
    "    \n",
    "    X_modified = np.array(gaolu_data_past_x)\n",
    "    y_modified = np.array(gaolu_data_past_y)\n",
    "    # if(ifprint):print(X_modified)\n",
    "    # if(ifprint):print(y_modified)\n",
    "    X = X_modified.reshape(X_modified.shape[0],X_modified.shape[2])\n",
    "    Y = y_modified.reshape(y_modified.shape[0],y_modified.shape[2])\n",
    "    # print(X_modified.shape)\n",
    "    # print(y_modified.shape)\n",
    "    if y_modified.shape[0]% 1 == 0:\n",
    "        model_predict.model_update(X, Y,\n",
    "                                epochs=10, \n",
    "                                batch_size=64,\n",
    "                                lr = 0.002,\n",
    "                                ifprint = ifprint)\n",
    "        # gaolu_data_past_x = []\n",
    "        # gaolu_data_past_y = []\n",
    "\n",
    "    return model_predict,model_gaolu,gaolu_data_past_x,gaolu_data_past_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对未来Times周期预测控制\n",
    "max_control = 1.0\n",
    "# 期望设定值\n",
    "set_y1, set_y2, set_y1_trans, set_y2_trans = generate_y_aim_data(Times)\n",
    "\n",
    "# MPC参数\n",
    "P = 3  # 预测时域长度  3\n",
    "M = 3  # 4\n",
    "#生成控制时域的数据格式\n",
    "k_data = generate_k_data(u1_data, u2_data, u3_data, u4_data,\n",
    "                        y1_data, y2_data, num_samples, P)\n",
    "\n",
    "\n",
    "# MPC控制循环   迭代的只有：k_data\n",
    "all_pred_y1 = []\n",
    "all_pred_y2 = []\n",
    "all_pred_u1 = []\n",
    "all_pred_u2 = []\n",
    "all_pred_u3 = []\n",
    "all_pred_u4 = []\n",
    "\n",
    "# 初始化一个最大长度为10的deque\n",
    "gaolu_data_past_x = deque(maxlen=maxlen)\n",
    "gaolu_data_past_y = deque(maxlen=maxlen)\n",
    "\n",
    "\n",
    "# MPC控制循环40\n",
    "for k in range(Times):\n",
    "    if iscontrol == False:\n",
    "        break\n",
    "\n",
    "    print(f\"这是对第{k}时刻的最优U1、U2输入求解\")\n",
    "\n",
    "    # 定义优化目标函数\n",
    "    def objective_function(params, *k_data):\n",
    "        mse, k_data2, E1_k_0, E2_k_0 = my_MPC(k_data=k_data[0], params=params, \n",
    "                                M=M, P=P, \n",
    "                                y1_aim = set_y1_trans[k], y2_aim = set_y2_trans[k],\n",
    "                                isprint = 0) \n",
    "        return mse\n",
    "    \n",
    "    # 初始猜测值[h U1 U2]   定义参数的上下限    设置退出条件\n",
    "    params = np.random.randn(z_dim * M)\n",
    "    bounds = [(-max_control, max_control) for _ in range(z_dim * M)]\n",
    "    \n",
    "    # params = np.concatenate([np.ones(M), np.ones(M),np.ones(M), np.ones(M)])\n",
    "    # bounds = [(-max_control, max_control) for _ in range(4 * M)]\n",
    "\n",
    "    exit_conditions = {'maxiter': 1000} \n",
    "    \n",
    "    options = {\n",
    "    'maxiter': 1000,      # 最大迭代次数\n",
    "    'disp': True,         # 显示详细的优化过程信息\n",
    "    'factr': 1e-20,       # 调整收敛精度（降低收敛阈值）\n",
    "    }\n",
    "    # 进行优化\n",
    "    result = minimize(objective_function, params, method='L-BFGS-B', \n",
    "                    bounds=bounds, args=k_data,\n",
    "                    options=options)\n",
    "\n",
    "    result_u = series2U_numpy_in_control(result.x,M, generated_numpy,\n",
    "                                        scalers_X_GAN, scalers,\n",
    "                                        input_term, isprint=False)\n",
    "    U1, U2, U3, U4 =    result_u[0:M], result_u[M:2*M], \\\n",
    "                        result_u[2*M:3*M], result_u[3*M:4*M]\n",
    "    \n",
    "    \n",
    "    # U1, U2, U3, U4 =    result.x[0:M], result.x[M:2*M], \\\n",
    "    #                     result.x[2*M:3*M], result.x[3*M:4*M]\n",
    "    \n",
    "\n",
    "    u1   = k_data[0:3]\n",
    "    u2   = k_data[3:6]\n",
    "    u3   = k_data[6:9]\n",
    "    u4   = k_data[9:12]\n",
    "\n",
    "    y1   = k_data[12:15]\n",
    "    y2   = k_data[15:18]\n",
    "    u1   = np.concatenate((u1,U1,U1[-1]*np.ones(P-M)))\n",
    "    u2   = np.concatenate((u2,U2,U2[-1]*np.ones(P-M)))\n",
    "    u3   = np.concatenate((u3,U3,U3[-1]*np.ones(P-M)))\n",
    "    u4   = np.concatenate((u4,U4,U4[-1]*np.ones(P-M)))\n",
    "    y1   = np.concatenate((y1,np.zeros(P)))\n",
    "    y2   = np.concatenate((y2,np.zeros(P)))\n",
    "\n",
    "\n",
    "\n",
    "    # 将控制序列第一个数作用于高炉\n",
    "    j = 1\n",
    "    x = np.column_stack((   u1[j+2],u2[j+2],u3[j+2],u4[j+2],\n",
    "                            u1[j+1],u2[j+1],u3[j+1],u4[j+1],\n",
    "                            y1[j+1],y2[j+1]))\n",
    "    # x = x.reshape((x.shape[0], 1, x.shape[1]))\n",
    "    y1_pred0, y2_pred0 = model_predict.my_predict(x)\n",
    "    if if_gaolu_is_predict:\n",
    "        y1_pred, y2_pred = model_predict.my_predict(x)\n",
    "        if if_add_noise:\n",
    "            y1_pred = y1_pred+gaussian_noise_TEMP[k].item()\n",
    "            y2_pred = y2_pred+gaussian_noise_SI[k].item()\n",
    "    else:\n",
    "        y1_pred, y2_pred = model_gaolu.my_predict(x)\n",
    "        if if_update_model:\n",
    "            model_predict,model_gaolu,gaolu_data_past_x,gaolu_data_past_y = update_model(model_predict,model_gaolu,\n",
    "                                            x,gaolu_data_past_x,gaolu_data_past_y,\n",
    "                                            ifprint = False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # 更新k_data\n",
    "    params = result.x\n",
    "    # params = np.concatenate((U1, U2, U3, U4),axis=0)\n",
    "    mse, k_data2, E1_k_0, E2_k_0 =my_MPC(k_data=k_data,params=params,\n",
    "                            M=M,P=P, \n",
    "                            y1_aim = set_y1_trans[k], y2_aim = set_y2_trans[k],\n",
    "                            isprint = 0) \n",
    "\n",
    "\n",
    "    print(  '1设定',set_y1_trans[k].round(4),\\\n",
    "            '预测',y1_pred0.round(4),\\\n",
    "            '高炉', y1_pred.round(4),\\\n",
    "            '高炉与设定误差',(set_y1_trans[k]-y1_pred).round(4),(set_y1_trans[k]-y1_pred).round(4)/d_temp,\\\n",
    "            '模型误差',(y1_pred0 - y1_pred).round(4),\\\n",
    "            '校正值',E1_k_0.round(4))\n",
    "    print(  '2设定',set_y2_trans[k].round(4),\\\n",
    "            '预测',y2_pred0.round(4),\\\n",
    "            '高炉', y2_pred.round(4),\\\n",
    "            '高炉与设定误差',(set_y2_trans[k]-y2_pred).round(4),(set_y2_trans[k]-y2_pred).round(4)/d_yuansu*100,\\\n",
    "            '模型误差',(y2_pred0 - y2_pred).round(4),\\\n",
    "            '校正值',E2_k_0.round(4))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    all_pred_y1.append(y1_pred)\n",
    "    all_pred_y2.append(y2_pred)\n",
    "    all_pred_u1.append(U1[0])\n",
    "    all_pred_u2.append(U2[0])\n",
    "    all_pred_u3.append(U3[0])\n",
    "    all_pred_u4.append(U4[0])\n",
    "    k_data2[14] = y1_pred.item()\n",
    "    k_data2[17] = y2_pred.item()\n",
    "    k_data = k_data2\n",
    "    # 进入下一时刻，更新预测时域、控制时域，即k_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if iscontrol:\n",
    "    y1_pred_inverse_transform = scalers[output_term[0]].inverse_transform(np.array(all_pred_y1).reshape(-1, 1)).flatten()\n",
    "    y2_pred_inverse_transform = scalers[output_term[1]].inverse_transform(np.array(all_pred_y2).reshape(-1, 1)).flatten()\n",
    "    \n",
    "    startt = 0\n",
    "    endd = 400\n",
    "    \n",
    "    plt.figure(figsize=(7, 6))\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(set_y1[startt:endd],'r', label='设定值')\n",
    "    plt.plot(y1_pred_inverse_transform[startt:endd],'b', label='实际值')\n",
    "    plt.axvline(x=0, color='gray', linestyle='--', linewidth=1.5)\n",
    "    plt.grid(linestyle='--', alpha=0.7, color='gray')\n",
    "    plt.ylabel(output_term[0], fontproperties=font)  # 使用中文标签\n",
    "    plt.legend(prop=font)\n",
    "    plt.title(f\"模型:MLP 训练次数:{epoch_sum_gaolu} 隐含层数:{3} 改进:{ischuangxin} 动态更新:{if_update_model} P:{P} M:{M} \", fontproperties=font)\n",
    "\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(set_y2[startt:endd],'r')\n",
    "    plt.plot(y2_pred_inverse_transform[startt:endd],'b')\n",
    "    plt.ylabel(output_term[1], fontproperties=font)  # 使用中文标签\n",
    "    plt.axvline(x=0, color='gray', linestyle='--', linewidth=1.5)\n",
    "    plt.grid(linestyle='--', alpha=0.7, color='gray')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "startt = 50\n",
    "endd = 150\n",
    "\n",
    "\n",
    "font222 = FontProperties(fname=r\"c:\\windows\\fonts\\simsun.ttc\", size=10)  # 替换为你的中文字体文件路径\n",
    "\n",
    "y1_pred_inverse_transform = scalers[output_term[0]].inverse_transform(np.array(all_pred_y1[startt:endd]).reshape(-1, 1)).flatten()\n",
    "y2_pred_inverse_transform = scalers[output_term[1]].inverse_transform(np.array(all_pred_y2[startt:endd]).reshape(-1, 1)).flatten()\n",
    "all_pred_u1_inverse_transform = scalers[input_term[0]].inverse_transform(np.array(all_pred_u1[startt:endd]).reshape(-1, 1)).flatten()\n",
    "all_pred_u2_inverse_transform = scalers[input_term[1]].inverse_transform(np.array(all_pred_u2[startt:endd]).reshape(-1, 1)).flatten()\n",
    "all_pred_u3_inverse_transform = scalers[input_term[2]].inverse_transform(np.array(all_pred_u3[startt:endd]).reshape(-1, 1)).flatten()\n",
    "all_pred_u4_inverse_transform = scalers[input_term[3]].inverse_transform(np.array(all_pred_u4[startt:endd]).reshape(-1, 1)).flatten()\n",
    "\n",
    "\n",
    "a1 = scalers[input_term[0]].inverse_transform(np.array([1,-1]).reshape(-1, 1)).flatten()\n",
    "a2 = scalers[input_term[1]].inverse_transform(np.array([1,-1]).reshape(-1, 1)).flatten()\n",
    "a3 = scalers[input_term[2]].inverse_transform(np.array([1,-1]).reshape(-1, 1)).flatten()\n",
    "a4 = scalers[input_term[3]].inverse_transform(np.array([1,-1]).reshape(-1, 1)).flatten()\n",
    "print(f'上线分别是：{a1}、{a2}、{a3}、{a4}')\n",
    "\n",
    "\n",
    "rmse_1 = np.mean(np.fabs(set_y1[startt:endd]-y1_pred_inverse_transform))\n",
    "rmse_2 = np.mean(np.fabs(set_y2[startt:endd]-y2_pred_inverse_transform))\n",
    "print('平均误差',rmse_1.round(4))\n",
    "print('平均误差',rmse_2.round(4))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 模型预测控制结果可视化\n",
    "# 创建两个子图，分别绘制每个维度\n",
    "plt.figure(figsize=(8, 13))\n",
    "\n",
    "# 第一个维度的曲线\n",
    "ax = plt.subplot(6, 1, 1)\n",
    "plt.plot(set_y1[startt:endd], 'k.-', label='设定值')\n",
    "plt.plot(y1_pred_inverse_transform, 'r', label='SC-MLP-U')\n",
    "plt.axvline(x=0, color='gray', linestyle='--', linewidth=1.5)\n",
    "plt.xlim((0,endd-startt))\n",
    "plt.ylabel('铁水温度MIT/℃', fontproperties=font)  # 使用中文标签\n",
    "plt.legend(prop=font222)\n",
    "plt.grid(linestyle='--', alpha=0.7, color='gray')\n",
    "ax.yaxis.set_label_coords(-0.07, 0.5)  # 固定纵坐标标签在最左边\n",
    "\n",
    "input_term222 =        ['富氧流量/(m\\u00b3/h)', '设定喷煤量/(t/h)', '热风压力/kPa', '热风温度/℃']\n",
    "output_term222 = ['铁水温度MIT/℃', '铁水硅含量[Si]/%']\n",
    "time_term= '时间戳h'\n",
    "input_term333 =        ['富氧流量', '设定喷煤量', '热风压力', '热风温度']\n",
    "output_term333 = ['铁水温度MIT', '铁水硅含量[Si]']\n",
    "time_term= '时间戳h'\n",
    "# 用于子图编号的字母序列\n",
    "subplot_labels = ['(a)', '(b)', '(c)', '(d)', '(e)', '(f)', '(g)', '(h)']\n",
    "dtiem = -0.5\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ax.text(0.5, dtiem, f'{subplot_labels[0]} {output_term333[0]}数据', \n",
    "            transform=ax.transAxes, ha='center', fontproperties=font)  # 添加每个子图的标题在下方\n",
    "    # 添加横坐标标题\n",
    "ax.set_xlabel('控制周期', fontproperties=font)  # 只给最下面的子图添加横坐标标签\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 第二个维度的曲线\n",
    "ax = plt.subplot(6, 1, 2)\n",
    "plt.plot(set_y2[startt:endd], 'k.-')\n",
    "plt.plot(y2_pred_inverse_transform, 'r')\n",
    "plt.axvline(x=0, color='gray', linestyle='--', linewidth=1.5)\n",
    "plt.xlim((0,endd-startt))\n",
    "plt.ylabel('铁水硅含量[Si]/%', fontproperties=font)  # 使用中文标签\n",
    "plt.grid(linestyle='--', alpha=0.7, color='gray')\n",
    "ax.text(0.5, dtiem, f'{subplot_labels[1]} {output_term333[1]}数据', \n",
    "            transform=ax.transAxes, ha='center', fontproperties=font)  # 添加每个子图的标题在下方\n",
    "    # 添加横坐标标题\n",
    "ax.set_xlabel('控制周期', fontproperties=font)  # 只给最下面的子图添加横坐标标签\n",
    "# 第一个维度的u1曲线\n",
    "ax = plt.subplot(6, 1, 3)\n",
    "plt.plot(all_pred_u1_inverse_transform, 'r')\n",
    "plt.axvline(x=0, color='gray', linestyle='--', linewidth=1.5)\n",
    "plt.xlim((0,endd-startt))\n",
    "plt.ylabel(input_term222[0], fontproperties=font)  # 使用中文标签\n",
    "plt.grid(linestyle='--', alpha=0.7, color='gray')\n",
    "ax.yaxis.set_label_coords(-0.07, 0.5)  # 固定纵坐标标签在最左边\n",
    "ax.text(0.5, dtiem, f'{subplot_labels[2]} {input_term333[0]}数据', \n",
    "            transform=ax.transAxes, ha='center', fontproperties=font)  # 添加每个子图的标题在下方\n",
    "    # 添加横坐标标题\n",
    "ax.set_xlabel('控制周期', fontproperties=font)  # 只给最下面的子图添加横坐标标签\n",
    "\n",
    "# 第二个维度的u2曲线\n",
    "ax = plt.subplot(6, 1, 4)\n",
    "plt.plot(all_pred_u2_inverse_transform, 'r')\n",
    "plt.axvline(x=0, color='gray', linestyle='--', linewidth=1.5)\n",
    "plt.xlim((0,endd-startt))\n",
    "plt.ylabel(input_term222[1], fontproperties=font)  # 使用中文标签\n",
    "plt.grid(linestyle='--', alpha=0.7, color='gray')\n",
    "ax.yaxis.set_label_coords(-0.07, 0.5)  # 固定纵坐标标签在最左边\n",
    "ax.text(0.5, dtiem, f'{subplot_labels[3]} {input_term333[1]}数据', \n",
    "            transform=ax.transAxes, ha='center', fontproperties=font)  # 添加每个子图的标题在下方\n",
    "    # 添加横坐标标题\n",
    "ax.set_xlabel('控制周期', fontproperties=font)  # 只给最下面的子图添加横坐标标签\n",
    "\n",
    "# 第三个维度的u3曲线\n",
    "ax = plt.subplot(6, 1, 5)\n",
    "plt.plot(all_pred_u3_inverse_transform, 'r')  # 修改标签为 'u3'\n",
    "plt.axvline(x=0, color='gray', linestyle='--', linewidth=1.5)\n",
    "plt.xlim((0,endd-startt))\n",
    "plt.ylabel(input_term222[2], fontproperties=font)  # 使用中文标签\n",
    "plt.grid(linestyle='--', alpha=0.7, color='gray')\n",
    "ax.yaxis.set_label_coords(-0.07, 0.5)  # 固定纵坐标标签在最左边\n",
    "ax.text(0.5, dtiem, f'{subplot_labels[4]} {input_term333[2]}数据', \n",
    "            transform=ax.transAxes, ha='center', fontproperties=font)  # 添加每个子图的标题在下方\n",
    "    # 添加横坐标标题\n",
    "ax.set_xlabel('控制周期', fontproperties=font)  # 只给最下面的子图添加横坐标标签\n",
    "\n",
    "# 第四个维度的u4曲线\n",
    "ax = plt.subplot(6, 1, 6)\n",
    "plt.plot(all_pred_u4_inverse_transform, 'r')  # 修改标签为 'u4'\n",
    "plt.axvline(x=0, color='gray', linestyle='--', linewidth=1.5)\n",
    "plt.xlim((0,endd-startt))\n",
    "plt.ylabel(input_term222[3], fontproperties=font)  # 使用中文标签\n",
    "plt.grid(linestyle='--', alpha=0.7, color='gray')\n",
    "ax.yaxis.set_label_coords(-0.07, 0.5)  # 固定纵坐标标签在最左边\n",
    "ax.text(0.5, dtiem, f'{subplot_labels[5]} {input_term333[3]}数据', \n",
    "            transform=ax.transAxes, ha='center', fontproperties=font)  # 添加每个子图的标题在下方\n",
    "    # 添加横坐标标题\n",
    "ax.set_xlabel('控制周期', fontproperties=font)  # 只给最下面的子图添加横坐标标签\n",
    "\n",
    "\n",
    "\n",
    "# 调整子图布局\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "startt = 50\n",
    "endd = 150\n",
    "\n",
    "y1_pred_inverse_transform = scalers[output_term[0]].inverse_transform(np.array(all_pred_y1[startt:endd]).reshape(-1, 1)).flatten()\n",
    "y2_pred_inverse_transform = scalers[output_term[1]].inverse_transform(np.array(all_pred_y2[startt:endd]).reshape(-1, 1)).flatten()\n",
    "all_pred_u1_inverse_transform = scalers[input_term[0]].inverse_transform(np.array(all_pred_u1[startt:endd]).reshape(-1, 1)).flatten()\n",
    "all_pred_u2_inverse_transform = scalers[input_term[1]].inverse_transform(np.array(all_pred_u2[startt:endd]).reshape(-1, 1)).flatten()\n",
    "all_pred_u3_inverse_transform = scalers[input_term[2]].inverse_transform(np.array(all_pred_u3[startt:endd]).reshape(-1, 1)).flatten()\n",
    "all_pred_u4_inverse_transform = scalers[input_term[3]].inverse_transform(np.array(all_pred_u4[startt:endd]).reshape(-1, 1)).flatten()\n",
    "\n",
    "# # 将数据转换为DataFrame\n",
    "# data = {\n",
    "#     'set_y1': set_y1,\n",
    "#     'set_y2': set_y2,\n",
    "#     'RES_MLP_U_y1': y1_pred_inverse_transform,\n",
    "#     'MLP_U_y2': y2_pred_inverse_transform,\n",
    "#     'RES_MLP_U_u1': all_pred_u1_inverse_transform,\n",
    "#     'RES_MLP_U_u2': all_pred_u2_inverse_transform,\n",
    "#     'RES_MLP_U_u3': all_pred_u3_inverse_transform,\n",
    "#     'RES_MLP_U_u4': all_pred_u4_inverse_transform\n",
    "# }\n",
    "\n",
    "# df = pd.DataFrame(data)\n",
    "\n",
    "# # 保存DataFrame到Excel文件\n",
    "# df.to_excel('pred_data_RES_MLP_U_'+str(cengshu)+'.xlsx', index=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "a1 = scalers[input_term[0]].inverse_transform(np.array([1,-1]).reshape(-1, 1)).flatten()\n",
    "a2 = scalers[input_term[1]].inverse_transform(np.array([1,-1]).reshape(-1, 1)).flatten()\n",
    "a3 = scalers[input_term[2]].inverse_transform(np.array([1,-1]).reshape(-1, 1)).flatten()\n",
    "a4 = scalers[input_term[3]].inverse_transform(np.array([1,-1]).reshape(-1, 1)).flatten()\n",
    "print(f'上线分别是：{a1}、{a2}、{a3}、{a4}')\n",
    "\n",
    "\n",
    "rmse_1 = np.mean(np.fabs(set_y1[startt:endd]-y1_pred_inverse_transform))\n",
    "rmse_2 = np.mean(np.fabs(set_y2[startt:endd]-y2_pred_inverse_transform))\n",
    "print('平均误差',rmse_1.round(4))\n",
    "print('平均误差',rmse_2.round(4))\n",
    "\n",
    "# 模型预测控制结果可视化\n",
    "# 创建两个子图，分别绘制每个维度\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# 第一个维度的曲线\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(set_y1[startt:endd], 'ro-', label='设定值')\n",
    "plt.plot(y1_pred_inverse_transform, 'bo-', label='RES_MLP_U 实际值')\n",
    "plt.axvline(x=0, color='gray', linestyle='--', linewidth=1.5)\n",
    "plt.xlim((0,endd-startt))\n",
    "plt.ylabel(output_term[0], fontproperties=font)  # 使用中文标签\n",
    "plt.legend(prop=font)\n",
    "plt.grid(linestyle='--', alpha=0.7, color='gray')\n",
    "\n",
    "# 第二个维度的曲线\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(set_y2[startt:endd], 'ro-')\n",
    "plt.plot(y2_pred_inverse_transform, 'bo-')\n",
    "plt.axvline(x=0, color='gray', linestyle='--', linewidth=1.5)\n",
    "plt.xlim((0,endd-startt))\n",
    "plt.ylabel(output_term[1], fontproperties=font)  # 使用中文标签\n",
    "plt.legend()\n",
    "plt.grid(linestyle='--', alpha=0.7, color='gray')\n",
    "\n",
    "# 调整子图布局\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "# 第一个维度的u1曲线\n",
    "plt.subplot(4, 1, 1)\n",
    "plt.plot(all_pred_u1_inverse_transform, 'bo-')\n",
    "plt.axvline(x=0, color='gray', linestyle='--', linewidth=1.5)\n",
    "plt.xlim((0,endd-startt))\n",
    "plt.ylabel(input_term[0], fontproperties=font)  # 使用中文标签\n",
    "plt.legend()\n",
    "plt.grid(linestyle='--', alpha=0.7, color='gray')\n",
    "\n",
    "# 第二个维度的u2曲线\n",
    "plt.subplot(4, 1, 2)\n",
    "plt.plot(all_pred_u2_inverse_transform, 'bo-')\n",
    "plt.axvline(x=0, color='gray', linestyle='--', linewidth=1.5)\n",
    "plt.xlim((0,endd-startt))\n",
    "plt.ylabel(input_term[1], fontproperties=font)  # 使用中文标签\n",
    "plt.legend()\n",
    "plt.grid(linestyle='--', alpha=0.7, color='gray')\n",
    "\n",
    "# 第三个维度的u3曲线\n",
    "plt.subplot(4, 1, 3)\n",
    "plt.plot(all_pred_u3_inverse_transform, 'bo-')  # 修改标签为 'u3'\n",
    "plt.axvline(x=0, color='gray', linestyle='--', linewidth=1.5)\n",
    "plt.xlim((0,endd-startt))\n",
    "plt.ylabel(input_term[2], fontproperties=font)  # 使用中文标签\n",
    "plt.legend()\n",
    "plt.grid(linestyle='--', alpha=0.7, color='gray')\n",
    "\n",
    "# 第四个维度的u4曲线\n",
    "plt.subplot(4, 1, 4)\n",
    "plt.plot(all_pred_u4_inverse_transform, 'bo-')  # 修改标签为 'u4'\n",
    "plt.axvline(x=0, color='gray', linestyle='--', linewidth=1.5)\n",
    "plt.xlim((0,endd-startt))\n",
    "plt.ylabel(input_term[3], fontproperties=font)  # 使用中文标签\n",
    "plt.legend()\n",
    "plt.grid(linestyle='--', alpha=0.7, color='gray')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39_tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
