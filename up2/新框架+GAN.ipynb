{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基础库\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import sys\n",
    "import os\n",
    "import datetime\n",
    "import pickle\n",
    "# 机器学习库\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "# 数据归一化、逆归一化\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# 优化相关库\n",
    "from skopt import gp_minimize\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# 深度学习库\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# 忽略警告\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# 中文字体设置\n",
    "from matplotlib.font_manager import FontProperties\n",
    "font = FontProperties(fname=r\"c:\\windows\\fonts\\simsun.ttc\", size=12)  # 替换为你的中文字体文件路径\n",
    "\n",
    "# 其他路径设置\n",
    "sys.path.append(r\"C:\\Users\\haokw\\Documents\\GitHub\\gaolu\\MPC\\高炉\")\n",
    "\n",
    "# 自定义模块\n",
    "import base \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取Excel文件\n",
    "excel_path = f'C:\\\\Users\\\\haokw\\\\Documents\\\\GitHub\\\\gaolu\\\\up2\\\\data\\\\data.xlsx'\n",
    "df_sheet_X = pd.read_excel(excel_path, sheet_name='X') \n",
    "\n",
    "\n",
    "excel_path = f'C:\\\\Users\\\\haokw\\\\Documents\\\\GitHub\\\\gaolu\\\\up2\\\\data\\\\data.xlsx'\n",
    "df_sheet_Y = pd.read_excel(excel_path, sheet_name='Y') \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 检查 DataFrame 中是否包含 NaN 值\n",
    "def check_if_NaN(data):\n",
    "    print(data.shape)\n",
    "    contains_nan = data.isna().any().any()\n",
    "    if contains_nan:\n",
    "        print(\"数据包含 NaN 值\")\n",
    "    else:\n",
    "        print(\"数据不包含 NaN 值\")\n",
    "\n",
    "check_if_NaN(df_sheet_X)\n",
    "check_if_NaN(df_sheet_Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基本变量设置\n",
    "input_term =  ['富氧流量', '设定喷煤量', '热风压力', '热风温度']\n",
    "output_term = ['铁水温度[MIT]', '铁水硅含量[SI]']\n",
    "time_term=  '时间戳h'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 处理异常值\n",
    "\n",
    "# 创建数据框副本以避免修改原始数据\n",
    "df_sheet_X_process = df_sheet_X.copy()\n",
    "df_sheet_Y_process = df_sheet_Y.copy()\n",
    "\n",
    "\n",
    "def IQR_process(df_IQR, columns):\n",
    "    df_IQR = df_IQR\n",
    "    columns = columns\n",
    "\n",
    "    print(columns)      # 获取数据框的所有列名\n",
    "    outlier_indices = set()  # 用于存储异常值的行索引\n",
    "\n",
    "    # 1. 分别处理每个变量\n",
    "    for column in columns:\n",
    "        # 计算描述性统计\n",
    "        stats = df_IQR[column].describe()\n",
    "\n",
    "        # 计算IQR（四分位距）以及上下须的范围\n",
    "        Q1 = stats['25%']\n",
    "        Q3 = stats['75%']\n",
    "        IQR = Q3 - Q1\n",
    "        lower_whisker = Q1 - 1.5 * IQR\n",
    "        upper_whisker = Q3 + 1.5 * IQR\n",
    "\n",
    "        # # 绘制箱线图\n",
    "        # plt.figure(figsize=(8, 6))\n",
    "        # sns.boxplot(data=df_IQR[column])\n",
    "        # plt.title(f'Boxplot of {column}', fontproperties=font)\n",
    "        # plt.xlabel('Feature', fontproperties=font)\n",
    "        # plt.ylabel('Value', fontproperties=font)\n",
    "        # plt.show()\n",
    "\n",
    "        # 查找异常值的索引\n",
    "        outliers = df_IQR[(df_IQR[column] < lower_whisker) | \n",
    "                            (df_IQR[column] > upper_whisker)].index\n",
    "        outlier_indices.update(outliers)\n",
    "\n",
    "        # # 打印统计信息和异常值范围\n",
    "        # print(f\"列: {column}\")\n",
    "        # print(f\"第一四分位数 (Q1): {Q1}\")\n",
    "        # print(f\"第三四分位数 (Q3): {Q3}\")\n",
    "        # print(f\"下须 (lower whisker): {lower_whisker}\")\n",
    "        # print(f\"上须 (upper whisker): {upper_whisker}\")\n",
    "        # print(f\"找到的异常值索引: {list(outliers)}\")\n",
    "\n",
    "        \n",
    "        # print(f\"异常值数量: {len(outliers)}\")\n",
    "        # print(f\"总数: {len(df_IQR[column])}\")\n",
    "\n",
    "        # print(f\"异常值比例: {len(outliers)/len(df_IQR[column])}\\n\")\n",
    "\n",
    "    # 2. 删除所有异常值\n",
    "    df_cleaned = df_IQR.drop(index=outlier_indices)\n",
    "    # 重新设置索引，使索引从 0 开始，并丢弃旧索引\n",
    "    df_cleaned.reset_index(drop=True, inplace=True)\n",
    "    # 输出处理后的数据框信息\n",
    "    print(f\"原始数据行数: {df_IQR.shape[0]}\")\n",
    "    print(f\"删除异常值后的数据行数: {df_cleaned.shape[0]}\")\n",
    "\n",
    "    # 你可以继续对 df_cleaned 进行后续处理\n",
    "\n",
    "\n",
    "\n",
    "    return df_cleaned\n",
    "\n",
    "\n",
    "df_cleaned_X = IQR_process(df_sheet_X_process, input_term)\n",
    "df_cleaned_Y = IQR_process(df_sheet_Y_process, output_term)\n",
    "\n",
    "print(np.max(df_cleaned_Y['铁水温度[MIT]']))\n",
    "print(np.min(df_cleaned_Y['铁水温度[MIT]']))\n",
    "print(np.max(df_cleaned_Y['铁水硅含量[SI]']))\n",
    "print(np.min(df_cleaned_Y['铁水硅含量[SI]']))\n",
    "\n",
    "\n",
    "# 画出数据\n",
    "def plot_subplot(data_x_yuan,data_y_yuan,data_x,data_y,column):\n",
    "    plt.plot(data_x_yuan,data_y_yuan,'r.')\n",
    "    plt.plot(data_x,data_y,'m.')\n",
    "    # plt.xlabel(time_term, fontproperties=font)  # 使用中文标签\n",
    "    plt.ylabel(column, fontproperties=font)  # 使用中文标签\n",
    "    # 使用中文标签\n",
    "\n",
    "\n",
    "    \n",
    "plt.figure(figsize=(15, 4))\n",
    "for idx, column in enumerate(input_term):\n",
    "    plt.subplot(len(input_term), 1, idx+1)\n",
    "    plot_subplot(   df_sheet_X[time_term].values,   df_sheet_X[column].values, \n",
    "                    df_cleaned_X[time_term].values, df_cleaned_X[column].values,column\n",
    "                )\n",
    "\n",
    "plt.figure(figsize=(15, 2))\n",
    "for idx, column in enumerate(output_term):\n",
    "    plt.subplot(len(output_term), 1, idx+1)\n",
    "    plot_subplot(   df_sheet_Y[time_term].values,   df_sheet_Y[column].values, \n",
    "                    df_cleaned_Y[time_term].values, df_cleaned_Y[column].values,column\n",
    "                )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 画出选取的数据\n",
    "def plot_subplot(data_x,data_y,column,index_predict,index_gaolu):\n",
    "    plt.plot(data_x,data_y,'-', label='origin_data')\n",
    "    plt.plot(data_x[index_gaolu],data_y[index_gaolu],'r-', label='gaolu_data')\n",
    "    plt.plot(data_x[index_predict],data_y[index_predict],'g-', label='predict_data')\n",
    "    plt.legend()\n",
    "    # plt.xlabel(time_term, fontproperties=font)  # 使用中文标签\n",
    "    plt.ylabel(column, fontproperties=font)  # 使用中文标签\n",
    "\n",
    "# 6509\n",
    "\n",
    "length1 = 400\n",
    "start1 = 1000\n",
    "\n",
    "length2 = 400\n",
    "start2 = 1400\n",
    "\n",
    "\n",
    "index_gaolu   = range(start1, start1+length1+1, 1)\n",
    "index_predict     = range(start2, start2+length2+1, 1)\n",
    "# index = range(1, 7572, 1)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15, 6))\n",
    "for idx, column in enumerate(input_term):\n",
    "    plt.subplot(len(input_term+output_term), 1, idx+1)\n",
    "    plot_subplot(df_cleaned_X[time_term].values, df_cleaned_X[column].values, column, index_predict, index_gaolu)\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15, 6))\n",
    "for idx, column in enumerate(output_term):\n",
    "    plt.subplot(len(input_term+output_term), 1, idx+1)\n",
    "    plot_subplot(df_cleaned_Y[time_term].values,df_cleaned_Y[column].values,column,index_predict,index_gaolu)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将数据存储为字典，每个键对应一列数据\n",
    "\n",
    "# 全部数据\n",
    "X_dict_original = {\n",
    "    input_term[0]:   df_cleaned_X[input_term[0]].values,\n",
    "    input_term[1]:   df_cleaned_X[input_term[1]].values,\n",
    "    input_term[2]:   df_cleaned_X[input_term[2]].values,\n",
    "    input_term[3]:   df_cleaned_X[input_term[3]].values\n",
    "}\n",
    "Y_dict_original = {\n",
    "    output_term[0]:  df_cleaned_Y[output_term[0]].values,\n",
    "    output_term[1]:  df_cleaned_Y[output_term[1]].values\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 高炉数据\n",
    "X_dict_original_index_gaolu = {\n",
    "    input_term[0]:   df_cleaned_X[input_term[0]][index_gaolu].values,\n",
    "    input_term[1]:   df_cleaned_X[input_term[1]][index_gaolu].values,\n",
    "    input_term[2]:   df_cleaned_X[input_term[2]][index_gaolu].values,\n",
    "    input_term[3]:   df_cleaned_X[input_term[3]][index_gaolu].values\n",
    "}\n",
    "Y_dict_original_index_gaolu = {\n",
    "    output_term[0]:  df_cleaned_Y[output_term[0]][index_gaolu].values,\n",
    "    output_term[1]:  df_cleaned_Y[output_term[1]][index_gaolu].values\n",
    "}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化缩放器\n",
    "scalers_X = {}\n",
    "scalers_Y = {}\n",
    "\n",
    "# 进行拟合\n",
    "for column, data in X_dict_original.items():\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    scaler.fit(data.reshape(-1, 1))  # 保证数据是列向量\n",
    "    scalers_X[column] = scaler\n",
    "\n",
    "for column, data in Y_dict_original.items():\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    scaler.fit(data.reshape(-1, 1))  # 保证数据是列向量\n",
    "    scalers_Y[column] = scaler\n",
    "\n",
    "\n",
    "# 进行归一化\n",
    "X_dict_normal = {}\n",
    "Y_dict_normal = {}\n",
    "for column, scaler in scalers_X.items():\n",
    "    X_dict_normal[column] = scaler.transform(X_dict_original[column].reshape(-1, 1)).flatten()\n",
    "for column, scaler in scalers_Y.items():\n",
    "    Y_dict_normal[column] = scaler.transform(Y_dict_original[column].reshape(-1, 1)).flatten()\n",
    "\n",
    "\n",
    "\n",
    "# 高炉部分数据\n",
    "# 进行归一化\n",
    "X_dict_normal_index_gaolu = {}\n",
    "Y_dict_normal_index_gaolu = {}\n",
    "for column, scaler in scalers_X.items():\n",
    "    X_dict_normal_index_gaolu[column] = scaler.transform(X_dict_original_index_gaolu[column].reshape(-1, 1)).flatten()\n",
    "for column, scaler in scalers_Y.items():\n",
    "    Y_dict_normal_index_gaolu[column] = scaler.transform(Y_dict_original_index_gaolu[column].reshape(-1, 1)).flatten()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 标定归一化前后数据\n",
    "data_point = np.array([1500]).reshape(-1, 1)\n",
    "data1 = scalers_Y[output_term[0]].transform(data_point).flatten()\n",
    "\n",
    "data_point = np.array(data1).reshape(-1, 1)\n",
    "data2 = scalers_Y[output_term[0]].inverse_transform(data_point).flatten()\n",
    "\n",
    "data_point = np.array([1510]).reshape(-1, 1)\n",
    "data3 = scalers_Y[output_term[0]].transform(data_point).flatten()\n",
    "\n",
    "data_point = np.array(data3).reshape(-1, 1)\n",
    "data4 = scalers_Y[output_term[0]].inverse_transform(data_point).flatten()\n",
    "\n",
    "# print(data1)\n",
    "# print(data2)\n",
    "# print(data3)\n",
    "# print(data4)\n",
    "d_temp = (data3-data1)/(data4-data2)\n",
    "print('每摄氏度的输出差：',d_temp)\n",
    "\n",
    "\n",
    "\n",
    "data_point = np.array([0.50]).reshape(-1, 1)\n",
    "data1 = scalers_Y[output_term[1]].transform(data_point).flatten()\n",
    "\n",
    "data_point = np.array(data1).reshape(-1, 1)\n",
    "data2 = scalers_Y[output_term[1]].inverse_transform(data_point).flatten()\n",
    "\n",
    "data_point = np.array([0.60]).reshape(-1, 1)\n",
    "data3 = scalers_Y[output_term[1]].transform(data_point).flatten()\n",
    "\n",
    "data_point = np.array(data3).reshape(-1, 1)\n",
    "data4 = scalers_Y[output_term[1]].inverse_transform(data_point).flatten()\n",
    "\n",
    "# print(data1)\n",
    "# print(data2)\n",
    "# print(data3)\n",
    "# print(data4)\n",
    "d_yuansu = (data3-data1)/(data4-data2)\n",
    "print('每浓度的输出差：',(data3-data1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制叠加的散点图矩阵。\n",
    "def plot_scatter_matrix(X_df_normal, X_df_normal_index_gaolu, figsize=(10, 8),font=font, save_path=None):\n",
    "    \"\"\"\n",
    "    绘制叠加的散点图矩阵。\n",
    "\n",
    "    参数:\n",
    "    X_df_normal (DataFrame): 第一组数据。\n",
    "    X_df_normal_index_gaolu (DataFrame): 第二组数据。\n",
    "    font (FontProperties, optional): 字体属性，用于设置标签的字体。\n",
    "    \"\"\"\n",
    "    # 设置颜色和标记\n",
    "    color_left = 'blue'\n",
    "    color_right = 'red'\n",
    "    marker_left = '.'\n",
    "    marker_right = '.'\n",
    "\n",
    "    # 设置数据\n",
    "    df_left = X_df_normal  # 第一组数据\n",
    "    df_right = X_df_normal_index_gaolu  # 第二组数据\n",
    "\n",
    "    # 绘制叠加的散点图矩阵\n",
    "    plt.figure(figsize = figsize)\n",
    "    num_cols = len(df_left.columns)\n",
    "    \n",
    "    for i, col1 in enumerate(df_left.columns):\n",
    "        for j, col2 in enumerate(df_left.columns):\n",
    "            ax = plt.subplot(num_cols, num_cols, i * num_cols + j + 1)\n",
    "            \n",
    "            if i != j:\n",
    "                ax.scatter(df_left[col1], df_left[col2], color=color_left, alpha=0.5, marker=marker_left, label='Left Data' if i == 0 and j == 1 else \"\")\n",
    "                ax.scatter(df_right[col1], df_right[col2], color=color_right, alpha=0.5, marker=marker_right, label='Right Data' if i == 0 and j == 1 else \"\")\n",
    "                ax.set_xlim([-1, 1])\n",
    "                ax.set_ylim([-1, 1])\n",
    "            else:\n",
    "                ax.hist(df_left[col1], bins=50, alpha=0.5, color=color_left)\n",
    "                ax.hist(df_right[col1], bins=50, alpha=0.5, color=color_right)\n",
    "                ax.set_xlim([-1, 1])\n",
    "\n",
    "            if i == num_cols - 1:\n",
    "                ax.set_xlabel(col2, fontproperties=font)\n",
    "            if j == 0:\n",
    "                ax.set_ylabel(col1, fontproperties=font)\n",
    "\n",
    "    # # 添加图例\n",
    "    # plt.legend(loc='upper right', bbox_to_anchor=(1.5, 1))\n",
    "    # 手动添加图例\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    ax.legend(handles, labels, loc='upper right', bbox_to_anchor=(1.5, 1))\n",
    "\n",
    "    # 添加标题并调整布局\n",
    "    plt.suptitle('Overlaid Scatter Matrix of Features', y=1.02)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # 如果提供了保存路径，则保存图像\n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "        plt.close()  # 关闭当前的图像，以节省内存\n",
    "    else:\n",
    "        plt.show()  # 否则显示图像\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制散点图矩阵\n",
    "# 转换为DataFrame\n",
    "print('高炉部分数据')\n",
    "X_df_normal_index_gaolu = pd.DataFrame(X_dict_normal_index_gaolu)\n",
    "# check_if_NaN(X_df_normal_index_gaolu)\n",
    "Y_df_normal_index_gaolu = pd.DataFrame(Y_dict_normal_index_gaolu)\n",
    "# check_if_NaN(Y_df_normal_index_gaolu)\n",
    "print(X_df_normal_index_gaolu.shape)\n",
    "\n",
    "\n",
    "# 转换为DataFrame\n",
    "print('全部数据')\n",
    "X_df_normal = pd.DataFrame(X_dict_normal)\n",
    "# check_if_NaN(X_df_normal)\n",
    "Y_df_normal = pd.DataFrame(Y_dict_normal)\n",
    "# check_if_NaN(Y_df_normal)\n",
    "\n",
    "print(X_df_normal.shape)\n",
    "\n",
    "# 绘制散点图矩阵\n",
    "plot_scatter_matrix(X_df_normal, X_df_normal_index_gaolu, font=font, figsize=(10, 8))\n",
    "# 绘制散点图矩阵\n",
    "plot_scatter_matrix(Y_df_normal, Y_df_normal_index_gaolu, font=font, figsize=(5, 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将历史数据转换为 PyTorch 张量\n",
    "data_item = X_df_normal\n",
    "# data_item = data_test\n",
    "\n",
    "data = torch.tensor(data_item.values, dtype=torch.float32)\n",
    "df_GAN = data_item\n",
    "test_sample_num = data.shape[0]\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LS-GAN超参数\n",
    "z_dim = 4  # 随机噪声维度\n",
    "data_dim = data.shape[1]  # 数据维度，4\n",
    "learning_rate = 0.0002\n",
    "num_epochs = 200\n",
    "n_critic = 5  # 每次更新生成器前，更新 Critic 的次数\n",
    "\n",
    "print_piture_d = 10\n",
    "\n",
    "# 设置数据加载器\n",
    "batch_size = 512\n",
    "data_loader = torch.utils.data.DataLoader(data, batch_size=batch_size, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LS-GAN模型定义和训练\n",
    "class LSGANGenerator(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LSGANGenerator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(256, output_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# LSGAN的判别器\n",
    "class LSGANDiscriminator(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(LSGANDiscriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# LSGAN的损失函数\n",
    "def generator_loss(fake_output):\n",
    "    return torch.mean((fake_output - 1) ** 2)\n",
    "\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = torch.mean((real_output - 1) ** 2)\n",
    "    fake_loss = torch.mean(fake_output ** 2)\n",
    "    return real_loss + fake_loss\n",
    "\n",
    "# LSGAN训练\n",
    "def train_lsgan(generator, discriminator, data_loader, num_epochs, z_dim, optimizer_G, optimizer_D, output_dir):\n",
    "    output_dir = r\"data\\train_output_picture\\LS-GAN_training_output\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    for epoch in range(num_epochs):\n",
    "        for real_data in data_loader:\n",
    "            batch_size = real_data.size(0)\n",
    "\n",
    "            # 生成数据\n",
    "            z = torch.randn(batch_size, z_dim)\n",
    "            fake_data = generator(z)\n",
    "\n",
    "            # 判别器前向传播\n",
    "            real_output = discriminator(real_data)\n",
    "            fake_output = discriminator(fake_data.detach())\n",
    "\n",
    "            # 计算判别器损失\n",
    "            d_loss = discriminator_loss(real_output, fake_output)\n",
    "            optimizer_D.zero_grad()\n",
    "            d_loss.backward()\n",
    "            optimizer_D.step()\n",
    "\n",
    "            # 生成器前向传播\n",
    "            fake_output = discriminator(fake_data)\n",
    "\n",
    "            # 计算生成器损失\n",
    "            g_loss = generator_loss(fake_output)\n",
    "            optimizer_G.zero_grad()\n",
    "            g_loss.backward()\n",
    "            optimizer_G.step()\n",
    "\n",
    "        print(f\"LS-GAN_training Epoch [{epoch}/{num_epochs}], D_loss: {d_loss.item():.4f}, G_loss: {g_loss.item():.4f}\")\n",
    "\n",
    "        if epoch % print_piture_d == print_piture_d-1:\n",
    "            # 生成数据test_sample_num\n",
    "            z = torch.randn(test_sample_num, z_dim)\n",
    "            generated_data = generator(z).detach().numpy()\n",
    "\n",
    "            # 将生成的数据转换为 DataFrame\n",
    "            generated_df = pd.DataFrame(generated_data, columns=df_GAN.columns)\n",
    "\n",
    "            # 获取当前时间\n",
    "            current_time = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "            # 构建保存路径和文件名\n",
    "            filename = f\"{current_time}_LSGAN_Epoch_{epoch+1}_D_loss_{d_loss.item():.4f}_G_loss_{g_loss.item():.4f}.png\"\n",
    "            save_path = os.path.join(output_dir, filename)\n",
    "\n",
    "            # 可视化生成的数据分布（你可以实现自己的可视化函数）\n",
    "            plot_scatter_matrix(df_GAN, generated_df, figsize=(10, 8), font=font, save_path=save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化LSGAN优化器  训练\n",
    "# # 使用WGAN生成的数据初始化LSGAN\n",
    "lsgan_generator = LSGANGenerator(input_dim=z_dim, output_dim=data_dim)\n",
    "lsgan_discriminator = LSGANDiscriminator(input_dim=data_dim)\n",
    "\n",
    "optimizer_G = optim.Adam(lsgan_generator.parameters(), lr=learning_rate, betas=(0.2, 0.999))\n",
    "optimizer_D = optim.Adam(lsgan_discriminator.parameters(), lr=learning_rate, betas=(0.2, 0.999))\n",
    "\n",
    "# 使用WGAN生成的数据作为输入进行LSGAN训练\n",
    "train_lsgan(lsgan_generator, lsgan_discriminator, data_loader, \n",
    "                num_epochs=num_epochs, z_dim=z_dim, \n",
    "                optimizer_G=optimizer_G, optimizer_D=optimizer_D, \n",
    "                output_dir=\"lsgan_outputs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 各种参数、信息数据的整理保存\n",
    "\n",
    "# 缩放器\n",
    "with open(r'data\\scalers\\scalers_X.pkl', 'wb') as f:\n",
    "    pickle.dump(scalers_X, f)\n",
    "with open(r'data\\scalers\\scalers_Y.pkl', 'wb') as f:\n",
    "    pickle.dump(scalers_Y, f)\n",
    "\n",
    "\n",
    "# 保存模型参数\n",
    "def save_model(generator, discriminator, output_dir, epoch):\n",
    "    # 创建保存目录（如果不存在）\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # 定义文件名\n",
    "    generator_filename = os.path.join(output_dir, f\"lsgan_generator_epoch_{epoch}.pth\")\n",
    "    discriminator_filename = os.path.join(output_dir, f\"lsgan_discriminator_epoch_{epoch}.pth\")\n",
    "\n",
    "    # 保存生成器的模型参数\n",
    "    torch.save(generator.state_dict(), generator_filename)\n",
    "    print(f\"Generator model saved to {generator_filename}\")\n",
    "\n",
    "    # 保存判别器的模型参数\n",
    "    torch.save(discriminator.state_dict(), discriminator_filename)\n",
    "    print(f\"Discriminator model saved to {discriminator_filename}\")\n",
    "save_model(lsgan_generator, lsgan_discriminator, \n",
    "        output_dir=r\"data\\model_params\\lsgan_model\", \n",
    "        epoch=num_epochs)\n",
    "\n",
    "# save_model(lsgan_generator, lsgan_discriminator, \n",
    "#         output_dir=r\"data\\model_params\\wgan_model\", \n",
    "#         epoch=num_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成样本测试\n",
    "z = torch.randn(test_sample_num, z_dim)\n",
    "print(z.shape)\n",
    "\n",
    "generated_data = lsgan_generator(z).detach().numpy()\n",
    "print(generated_data.shape)\n",
    "\n",
    "# 将生成的数据转换为 DataFrame\n",
    "generated_df = pd.DataFrame(generated_data, columns=df_GAN.columns)\n",
    "\n",
    "# 可视化生成的数据分布（你可以实现自己的可视化函数）\n",
    "plot_scatter_matrix(df_GAN, generated_df, figsize=(10, 8), font=font)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSGAN的生成器   初始化   加载生成器参数  获取 PyTorch 模型参数  构建numpy版本  初始化numpy生成器\n",
    "\n",
    "# 初始化生成器和判别器\n",
    "lsgan_generator_item = LSGANGenerator(input_dim=z_dim, output_dim=data_dim)\n",
    "\n",
    "\n",
    "# 加载生成器参数\n",
    "generator_path = r\"data\\model_params\\lsgan_model\\lsgan_generator_epoch_200.pth\"\n",
    "lsgan_generator_item.load_state_dict(torch.load(generator_path))\n",
    "lsgan_generator_item.eval()  # 切换到评估模式\n",
    "print(f\"Generator model loaded from {generator_path}\")\n",
    "\n",
    "\n",
    "# 假设 lsgan_generator_item 已经是训练好的模型\n",
    "def extract_pytorch_params(model):\n",
    "    params = {}\n",
    "    for name, param in model.named_parameters():\n",
    "        params[name] = param.data.numpy()\n",
    "    return params\n",
    "# 获取 PyTorch 模型参数\n",
    "lsgan_generator_item_params = extract_pytorch_params(lsgan_generator_item)\n",
    "\n",
    "\n",
    "\n",
    "# 构建numpy版本\n",
    "class LSGANGeneratorNumpy:\n",
    "    def __init__(self, input_dim, output_dim, params):\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        # 使用从 PyTorch 模型中提取的参数初始化\n",
    "        self.W1 = params['model.0.weight'].T  # 转置以匹配 numpy 矩阵乘法\n",
    "        self.b1 = params['model.0.bias']\n",
    "        \n",
    "        self.W2 = params['model.2.weight'].T\n",
    "        self.b2 = params['model.2.bias']\n",
    "        \n",
    "        self.W3 = params['model.4.weight'].T\n",
    "        self.b3 = params['model.4.bias']\n",
    "    \n",
    "    def relu(self, x):\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    def tanh(self, x):\n",
    "        return np.tanh(x)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        z1 = np.dot(x, self.W1) + self.b1\n",
    "        a1 = self.relu(z1)\n",
    "        \n",
    "        z2 = np.dot(a1, self.W2) + self.b2\n",
    "        a2 = self.relu(z2)\n",
    "        \n",
    "        z3 = np.dot(a2, self.W3) + self.b3\n",
    "        output = self.tanh(z3)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "\n",
    "\n",
    "# 初始化numpy生成器\n",
    "lsgan_generator_numpy = LSGANGeneratorNumpy(z_dim, data_dim, lsgan_generator_item_params)\n",
    "\n",
    "\n",
    "\n",
    "print('参数已迁移')\n",
    "\n",
    "\n",
    "\n",
    "# 从文件中加载 scalers_X 和 scalers_Y\n",
    "with open(r'data\\scalers\\scalers_X.pkl', 'rb') as f:\n",
    "    scalers_X = pickle.load(f)\n",
    "\n",
    "with open(r'data\\scalers\\scalers_Y.pkl', 'rb') as f:\n",
    "    scalers_Y = pickle.load(f)\n",
    "\n",
    "print('scalers参数已迁移')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 验证环节\n",
    "def series2U(z, M, scalers_X, input_term, isprint = True):\n",
    "    if(isprint):print(z.shape)\n",
    "\n",
    "    # 将 z 重新整理成 (M, z_dim) 的二维数组\n",
    "    z_reshaped = z.reshape(M, z_dim)\n",
    "\n",
    "    # 将 numpy 数组转换为 tensor，指定 dtype 为 float32\n",
    "    z_tensor = torch.from_numpy(z_reshaped).float()\n",
    "    if(isprint):print(z_tensor.shape)\n",
    "\n",
    "    # z_tensor = torch.randn(2, z_dim)\n",
    "    generated_data = lsgan_generator(z_tensor).detach().numpy()\n",
    "\n",
    "    if(isprint):print(generated_data.shape)\n",
    "    if(isprint):print(generated_data)\n",
    "\n",
    "    # 分别提取 U1, U2, U3, U4\n",
    "    U1 = generated_data[:, 0]\n",
    "    U2 = generated_data[:, 1]\n",
    "    U3 = generated_data[:, 2]\n",
    "    U4 = generated_data[:, 3]\n",
    "\n",
    "    # 将 U1, U2, U3, U4 连接成一个序列\n",
    "    sequence = np.concatenate((U1, U2, U3, U4))\n",
    "\n",
    "    if(isprint):print(\"U1:\", U1)\n",
    "    if(isprint):print(\"U2:\", U2)\n",
    "    if(isprint):print(\"U3:\", U3)\n",
    "    if(isprint):print(\"U4:\", U4)\n",
    "    if(isprint):print(\"Connected sequence:\", sequence)\n",
    "\n",
    "\n",
    "    U1_inverse = scalers_X[input_term[0]].inverse_transform(U1.reshape(-1, 1)).flatten()\n",
    "    U2_inverse = scalers_X[input_term[1]].inverse_transform(U2.reshape(-1, 1)).flatten()\n",
    "    U3_inverse = scalers_X[input_term[2]].inverse_transform(U3.reshape(-1, 1)).flatten()\n",
    "    U4_inverse = scalers_X[input_term[3]].inverse_transform(U4.reshape(-1, 1)).flatten()\n",
    "\n",
    "    if(isprint):print(\"U1_inverse:\", U1_inverse)\n",
    "    if(isprint):print(\"U2_inverse:\", U2_inverse)\n",
    "    if(isprint):print(\"U3_inverse:\", U3_inverse)\n",
    "    if(isprint):print(\"U4_inverse:\", U4_inverse)\n",
    "\n",
    "    return sequence\n",
    "\n",
    "\n",
    "def series2U_numpy(z, M, scalers_X, input_term, isprint = True):\n",
    "\n",
    "    if(isprint):print(z.shape)\n",
    "\n",
    "    # 生成一些随机数据进行测试\n",
    "    x =  z.reshape(M, z_dim)\n",
    "\n",
    "    # 前向传播\n",
    "    generated_data = lsgan_generator_numpy.forward(x)\n",
    "\n",
    "    if(isprint):print(\"Generated data shape:\", generated_data.shape)\n",
    "    if(isprint):print(\"Generated data:\\n\", generated_data)\n",
    "\n",
    "\n",
    "    # 分别提取 U1, U2, U3, U4\n",
    "    U1 = generated_data[:, 0]\n",
    "    U2 = generated_data[:, 1]\n",
    "    U3 = generated_data[:, 2]\n",
    "    U4 = generated_data[:, 3]\n",
    "\n",
    "    # 将 U1, U2, U3, U4 连接成一个序列\n",
    "    sequence = np.concatenate((U1, U2, U3, U4))\n",
    "\n",
    "    if(isprint):print(\"U1:\", U1)\n",
    "    if(isprint):print(\"U2:\", U2)\n",
    "    if(isprint):print(\"U3:\", U3)\n",
    "    if(isprint):print(\"U4:\", U4)\n",
    "    if(isprint):print(\"Connected sequence:\", sequence)\n",
    "\n",
    "    U1_inverse = scalers_X[input_term[0]].inverse_transform(U1.reshape(-1, 1)).flatten()\n",
    "    U2_inverse = scalers_X[input_term[1]].inverse_transform(U2.reshape(-1, 1)).flatten()\n",
    "    U3_inverse = scalers_X[input_term[2]].inverse_transform(U3.reshape(-1, 1)).flatten()\n",
    "    U4_inverse = scalers_X[input_term[3]].inverse_transform(U4.reshape(-1, 1)).flatten()\n",
    "\n",
    "    if(isprint):print(\"U1_inverse:\", U1_inverse)\n",
    "    if(isprint):print(\"U2_inverse:\", U2_inverse)\n",
    "    if(isprint):print(\"U3_inverse:\", U3_inverse)\n",
    "    if(isprint):print(\"U4_inverse:\", U4_inverse)\n",
    "\n",
    "    return sequence\n",
    "\n",
    "\n",
    "# 定义 M 和 z_dim\n",
    "test_size_generated_data = 3      # 样本数量\n",
    "\n",
    "# 使用 numpy 生成标准正态分布的随机数，形状为 (z_dim * M,)\n",
    "z = np.random.randn(z_dim * test_size_generated_data)\n",
    "generated_data          = series2U      (z,test_size_generated_data, scalers_X, input_term, isprint=False)\n",
    "generated_data_numpy    = series2U_numpy(z,test_size_generated_data, scalers_X, input_term, isprint=False)\n",
    "\n",
    "print(\"验证原模型与numpy模型的输出是否一致：\")\n",
    "result_d_state = np.fabs(generated_data-generated_data_numpy)<1e-6\n",
    "# print(result_d_state)\n",
    "print('总数量：',test_size_generated_data*4,',错误数量：',np.sum(result_d_state==False),'，正确数量：',np.sum(result_d_state==True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据集划分参数\n",
    "isShuffle = True\n",
    "isShuffle = False\n",
    "time_steps = 2\n",
    "test_size = 0.15\n",
    "val_size = 0.15\n",
    "train_size = 1-val_size-test_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 组合训练数据--拆分训练、测试集\n",
    "\n",
    "# 定义时间步数和特征数\n",
    "\n",
    "# 构成    \n",
    "# X = [X(t),X(t-1),Y(t-1)]\n",
    "# Y = [Y(t)]\n",
    "def make_data(X_df_normal,Y_df_normal,index_fanwei,ifprint = True):\n",
    "    X_modified = []\n",
    "    y_modified = []\n",
    "\n",
    "\n",
    "    for i in range(0, min(X_df_normal.shape[0],Y_df_normal.shape[0])):\n",
    "        # print(i)\n",
    "        if i in index_fanwei:\n",
    "            # print(i)\n",
    "\n",
    "            Y_time = df_cleaned_Y[time_term][i]\n",
    "            # print('输出时间：',df_cleaned_Y[time_term][i])\n",
    "\n",
    "            closest_10 = df_cleaned_X[df_cleaned_X[time_term] <= Y_time].nlargest(time_steps, time_term)\n",
    "            # print(closest_10)\n",
    "            # 检查 closest_10 是否为空\n",
    "            if closest_10.empty:\n",
    "                print(\"No closest values found. closest_10 is empty.\")\n",
    "                continue\n",
    "                \n",
    "            index = closest_10.index\n",
    "            # print(list(index))\n",
    "            # print(closest_10.iloc[-1][time_term])\n",
    "            # print(Y_time - time_steps + 1 )\n",
    "\n",
    "            if closest_10.iloc[-1][time_term] != Y_time - time_steps + 1 :\n",
    "                # print(i,Y_time)\n",
    "                print(i,',t',Y_time,',||||  t',closest_10.iloc[0][time_term],',t-time_steps',closest_10.iloc[-1][time_term],'index',index[0],index[-1],'errloss')\n",
    "            else:\n",
    "                # print(X_df_normal.loc[index])\n",
    "                # 拼接行数据 (axis=0 表示纵向拼接)\n",
    "                new_x_sample = np.concatenate([X_df_normal.loc[i, :].values for i in index], axis=0)\n",
    "                # print(new_x_sample)\n",
    "                y_last = Y_df_normal.loc[i-1]\n",
    "                \n",
    "                # print(y_last, 'y_last time : ',df_cleaned_Y[time_term][i-1])\n",
    "\n",
    "                new_x_sample = np.concatenate([new_x_sample,y_last],axis=0)\n",
    "                # print(new_x_sample)\n",
    "                y_sample = Y_df_normal.loc[i]\n",
    "                # print(y_sample)\n",
    "                X_modified.append(new_x_sample)\n",
    "                y_modified.append(y_sample)\n",
    "                print(i,',t',Y_time,',t',closest_10.iloc[0][time_term],',t-time_steps',closest_10.iloc[-1][time_term],'index',index[0],index[-1])\n",
    "\n",
    "\n",
    "            # break\n",
    "    \n",
    "    # 将列表转换为 NumPy 数组\n",
    "    \n",
    "    # 查看二维列表的形状\n",
    "    rows = len(X_modified)\n",
    "    columns = len(X_modified[0]) if rows > 0 else 0\n",
    "    print(f\"二维列表的形状: ({rows}, {columns})\")\n",
    "\n",
    "\n",
    "\n",
    "    X_modified = np.array(X_modified)\n",
    "    y_modified = np.array(y_modified)\n",
    "    X_reshaped = X_modified.reshape((X_modified.shape[0], X_modified.shape[1]))\n",
    "\n",
    "    # 打印新数据的形状\n",
    "    print(\"Modified Input Shape:\", X_reshaped.shape)\n",
    "    print(\"Modified Output Shape:\", y_modified.shape)\n",
    "\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_reshaped, y_modified, \n",
    "                                                        test_size=test_size, \n",
    "                                                        random_state=42, \n",
    "                                                        shuffle=isShuffle)\n",
    "\n",
    "    # 将剩余的70%训练数据再次拆分成训练数据和验证数据（20%验证数据，50%训练数据）\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, \n",
    "                                                        test_size=val_size/(train_size+val_size), \n",
    "                                                        random_state=42, \n",
    "                                                        shuffle=isShuffle)\n",
    "\n",
    "    print('训练数量：',X_train.shape,y_train.shape)\n",
    "    print('验证数量：',X_val.shape,y_val.shape)\n",
    "    print('测试数量：',X_test.shape,y_test.shape)\n",
    "\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('高炉模型数据')\n",
    "X_gaolu_train, X_gaolu_val, X_gaolu_test,\\\n",
    "y_gaolu_train, y_gaolu_val, y_gaolu_test = make_data(X_df_normal,Y_df_normal,\n",
    "                                                    index_gaolu,ifprint = False)\n",
    "\n",
    "\n",
    "\n",
    "print('预测模型数据')\n",
    "X_predict_train, X_predict_val, X_predict_test,\\\n",
    "y_predict_train, y_predict_val, y_predict_test = make_data(X_df_normal,Y_df_normal,\n",
    "                                                    index_predict,ifprint = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练模型参数\n",
    "epoch_once_time = 50\n",
    "ischuangxin = True\n",
    "# ischuangxin = False\n",
    "cengshu = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义模型\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class MyNeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, if_chuangxin = False,gamma = 0.1):\n",
    "        self.if_chuangxin = if_chuangxin\n",
    "        super(MyNeuralNetwork, self).__init__()\n",
    "        if cengshu == 3:    \n",
    "            if self.if_chuangxin:            \n",
    "                self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "                self.relu = nn.ReLU()\n",
    "                self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "                self.fc3 = nn.Linear(hidden_size, hidden_size)\n",
    "                self.fc4 = nn.Linear(hidden_size, output_size)\n",
    "            else:\n",
    "                self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "                self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "                self.fc3 = nn.Linear(hidden_size, hidden_size)\n",
    "                self.fc4 = nn.Linear(hidden_size, output_size)\n",
    "                self.relu = nn.ReLU()\n",
    "        elif cengshu == 2:  \n",
    "            if self.if_chuangxin:            \n",
    "                self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "                self.relu = nn.ReLU()\n",
    "                self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "                self.fc3 = nn.Linear(hidden_size, output_size)\n",
    "            else:\n",
    "                self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "                self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "                self.fc3 = nn.Linear(hidden_size, output_size)\n",
    "                self.relu = nn.ReLU()\n",
    "\n",
    "\n",
    "    def forward(self, x0):\n",
    "        if cengshu == 3:    \n",
    "            if self.if_chuangxin:\n",
    "\n",
    "                x = self.fc1(x0)\n",
    "                x = self.relu(x)\n",
    "\n",
    "                x2 = self.fc2(x)\n",
    "                x2 = self.relu(x2)\n",
    "\n",
    "                x3 = self.fc3(x2)\n",
    "                x3 = self.relu(x3)\n",
    "\n",
    "                x4 = x + x2 + x3\n",
    "                output = self.fc4(x4)\n",
    "            else:\n",
    "                x = self.fc1(x0)\n",
    "                x = self.relu(x)\n",
    "\n",
    "                x2 = self.fc2(x)\n",
    "                x2 = self.relu(x2)\n",
    "\n",
    "                x3 = self.fc3(x2)\n",
    "                x3 = self.relu(x3)\n",
    "\n",
    "                output = self.fc4(x3)\n",
    "        elif cengshu == 2:  \n",
    "            if self.if_chuangxin:\n",
    "\n",
    "                x = self.fc1(x0)\n",
    "                x = self.relu(x)\n",
    "\n",
    "                x2 = self.fc2(x)\n",
    "                x2 = self.relu(x2)\n",
    "\n",
    "                x3 = x + x2\n",
    "                output = self.fc3(x3)\n",
    "            else:\n",
    "                x = self.fc1(x0)\n",
    "                x = self.relu(x)\n",
    "\n",
    "                x2 = self.fc2(x)\n",
    "                x2 = self.relu(x2)\n",
    "\n",
    "                output = self.fc3(x2)\n",
    "        return output\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    def custom_loss(self, y_true, y_pred):\n",
    "\n",
    "        squared_diff = torch.pow(y_true - y_pred, 2)\n",
    "        sum_squared_diff = torch.sum(squared_diff)\n",
    "        mse = sum_squared_diff / len(y_true)\n",
    "        return mse\n",
    "    \n",
    "\n",
    "    def my_fit(self, \n",
    "                X_train, y_train, \n",
    "                X_val, y_val, \n",
    "                train_loss_list,val_loss_list,\n",
    "                epochs=1, batch_size=32, lr=0.001):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            epoch_loss = 0\n",
    "            for i in range(0, len(X_train), batch_size):\n",
    "                x_batch = torch.tensor(X_train[i:i+batch_size], dtype=torch.float32)\n",
    "                y_batch = torch.tensor(y_train[i:i+batch_size], dtype=torch.float32)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                y_pred = self(x_batch)\n",
    "                loss = self.custom_loss(y_batch, y_pred)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "            average_epoch_train_loss = epoch_loss / (len(X_train) / batch_size)\n",
    "            # 验证集评估\n",
    "            self.eval()\n",
    "            with torch.no_grad():\n",
    "                val_loss = 0\n",
    "                for i in range(0, len(X_val), batch_size):\n",
    "                    x_batch_val = torch.tensor(X_val[i:i+batch_size], dtype=torch.float32)\n",
    "                    y_batch_val = torch.tensor(y_val[i:i+batch_size], dtype=torch.float32)\n",
    "\n",
    "                    y_pred_val = self(x_batch_val)\n",
    "                    val_loss += self.custom_loss(y_batch_val, y_pred_val).item()\n",
    "\n",
    "                average_epoch_val_loss = val_loss / (len(X_val) / batch_size)\n",
    "\n",
    "            print(f'第 {epoch + 1}/{epochs} 轮, 训练误差: {average_epoch_train_loss:.4f}, 验证误差: {average_epoch_val_loss:.4f}', end='\\r')\n",
    "            train_loss_list.append(average_epoch_train_loss)\n",
    "            val_loss_list.append(average_epoch_val_loss)\n",
    "\n",
    "        return train_loss_list,val_loss_list\n",
    "    \n",
    "    def model_update(self, \n",
    "                X_train, y_train, \n",
    "                epochs=1, batch_size=32, lr=0.001,isprint = True):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "        for epoch in range(epochs):\n",
    "            epoch_loss = 0\n",
    "            for i in range(0, len(X_train), batch_size):\n",
    "                x_batch = torch.tensor(X_train[i:i+batch_size], dtype=torch.float32)\n",
    "                y_batch = torch.tensor(y_train[i:i+batch_size], dtype=torch.float32)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                y_pred = self(x_batch)\n",
    "                loss = self.custom_loss(y_batch, y_pred)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "            average_epoch_train_loss = epoch_loss / (len(X_train) / batch_size)\n",
    "            if(isprint):print(f'第 {epoch + 1}/{epochs} 轮, 训练误差: {average_epoch_train_loss:.4f}')\n",
    "            \n",
    "            \n",
    "        return 0\n",
    "    \n",
    "    \n",
    "\n",
    "    def my_predict(self, X_test):\n",
    "        # 设置模型为评估模式，这会关闭 dropout 等层\n",
    "        self.eval()\n",
    "        # 将输入数据转换为张量，并设置 requires_grad=True\n",
    "        x_tensor = torch.tensor(X_test, dtype=torch.float32, requires_grad=True)\n",
    "        \n",
    "        # 获取模型的预测输出\n",
    "        y_pred = self(x_tensor)\n",
    "        # 保留预测值的梯度信息\n",
    "        y_pred.retain_grad()\n",
    "        # 返回预测结果和包含梯度信息的张量\n",
    "        return y_pred[:,0].detach().numpy(),y_pred[:,1].detach().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立高炉模型实例\n",
    "input_size = 10  # 输入特征大小\n",
    "hidden_size = 16  # 32\n",
    "output_size = 2  # 输出大小\n",
    "# 设置随机种子\n",
    "torch.manual_seed(0)\n",
    "model_gaolu = MyNeuralNetwork(input_size, \n",
    "                            hidden_size,\n",
    "                            output_size,\n",
    "                            ischuangxin,\n",
    "                            gamma = 0.1)\n",
    "epoch_sum_gaolu = 0\n",
    "gaolu_train_loss_list = []\n",
    "gaolu_val_loss_list = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 高炉模型训练\n",
    "epoch_once = epoch_once_time\n",
    "epoch_sum_gaolu = epoch_sum_gaolu + epoch_once\n",
    "gaolu_train_loss_list,gaolu_val_loss_list = model_gaolu.my_fit(X_gaolu_train, y_gaolu_train,\n",
    "                                    X_gaolu_val, y_gaolu_val, \n",
    "                                    gaolu_train_loss_list, gaolu_val_loss_list,\n",
    "                                    epochs=epoch_once, \n",
    "                                    batch_size=32,\n",
    "                                    lr = 0.002)\n",
    "\n",
    "print('\\nepoch_sum:',epoch_sum_gaolu)\n",
    "\n",
    "# 绘制训练和验证损失曲线\n",
    "plt.plot(gaolu_train_loss_list, label='Train Loss')\n",
    "plt.plot(gaolu_val_loss_list, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 结果输出图\n",
    "ditem = -0.65\n",
    "\n",
    "\n",
    "# 用于子图编号的字母序列\n",
    "subplot_labels = ['(a)', '(b)', '(c)', '(d)', '(e)', '(f)', '(g)', '(h)']\n",
    "input_term333 =        ['富氧流量', '设定喷煤量', '热风压力', '热风温度']\n",
    "output_term333 = ['铁水温度MIT', '铁水硅含量[Si]']\n",
    "time_term= '时间戳h'\n",
    "# print(input_term333)\n",
    "# print(output_term333)\n",
    "input_term222 =        ['富氧流量/(m\\u00b3/h)', '设定喷煤量/(t/h)', '热风压力/kPa', '热风温度/℃']\n",
    "output_term222 = ['铁水温度MIT/℃', '铁水硅含量[Si]/%']\n",
    "time_term= '时间戳h'\n",
    "# print(input_term222)\n",
    "# print(output_term222)\n",
    "\n",
    "def double_control_train_test_result(scalers, output_term, y_test, y_pred_0, y_pred_1, y_test_2, y_pred_0_2, y_pred_1_2,ditem,a,b):\n",
    "    y_test_0 = scalers[output_term[0]].inverse_transform((y_test[:, 0]).reshape(-1, 1)).flatten()\n",
    "    y_test_1 = scalers[output_term[1]].inverse_transform((y_test[:, 1]).reshape(-1, 1)).flatten()\n",
    "    y_pred_0_inverse_transform = scalers[output_term[0]].inverse_transform((y_pred_0).reshape(-1, 1)).flatten()\n",
    "    y_pred_1_inverse_transform = scalers[output_term[1]].inverse_transform((y_pred_1).reshape(-1, 1)).flatten()\n",
    "\n",
    "    output0 = y_test_0 - y_pred_0_inverse_transform\n",
    "    output1 = y_test_1 - y_pred_1_inverse_transform\n",
    "\n",
    "    plt.figure(figsize=(9, 8))\n",
    "    \n",
    "    \n",
    "    ax = plt.subplot(4, 1, 1)\n",
    "    plt.plot(y_test_0,'k', label=\"真实值\")\n",
    "    plt.plot(y_pred_0_inverse_transform,'r--', label=\"预测值\")\n",
    "    ax.legend(prop=font, loc='upper right', bbox_to_anchor=(a, b), ncol=4) \n",
    "    \n",
    "    plt.ylabel(output_term222[0], fontproperties=font)  \n",
    "    ax.yaxis.set_label_coords(-0.07, 0.5)  \n",
    "    ax.text(0.5, ditem, f'{subplot_labels[0]} {output_term333[0]}建模效果', \n",
    "            transform=ax.transAxes, ha='center', fontproperties=font)  \n",
    "    ax.set_xlabel('训练样本', fontproperties=font)  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ax = plt.subplot(4, 1, 2)\n",
    "    plt.plot(y_test_1,'k')\n",
    "    plt.plot(y_pred_1_inverse_transform,'r--')\n",
    "    plt.ylabel(output_term222[1], fontproperties=font) \n",
    "\n",
    "    ax.yaxis.set_label_coords(-0.07, 0.5)  \n",
    "    ax.text(0.5, ditem, f'{subplot_labels[1]} {output_term333[1]}建模效果', \n",
    "            transform=ax.transAxes, ha='center', fontproperties=font)  \n",
    "    \n",
    "    ax.set_xlabel('训练样本', fontproperties=font)  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    y_test_0 = scalers[output_term[0]].inverse_transform((y_test_2[:, 0]).reshape(-1, 1)).flatten()\n",
    "    y_test_1 = scalers[output_term[1]].inverse_transform((y_test_2[:, 1]).reshape(-1, 1)).flatten()\n",
    "    y_pred_0_inverse_transform = scalers[output_term[0]].inverse_transform((y_pred_0_2).reshape(-1, 1)).flatten()\n",
    "    y_pred_1_inverse_transform = scalers[output_term[1]].inverse_transform((y_pred_1_2).reshape(-1, 1)).flatten()\n",
    "\n",
    "    output0 = y_test_0 - y_pred_0_inverse_transform\n",
    "    output1 = y_test_1 - y_pred_1_inverse_transform\n",
    "\n",
    "    ax = plt.subplot(4, 1, 3)\n",
    "    plt.plot(y_test_0,'k', label=\"真实值\")\n",
    "    plt.plot(y_pred_0_inverse_transform,'r--', label=\"预测值\")\n",
    "    plt.ylabel(output_term222[0], fontproperties=font)  \n",
    "    ax.yaxis.set_label_coords(-0.07, 0.5)  \n",
    "    ax.text(0.5, ditem, f'{subplot_labels[2]} {output_term333[0]}预测效果', \n",
    "            transform=ax.transAxes, ha='center', fontproperties=font)  \n",
    "    ax.set_xlabel('测试样本', fontproperties=font)  \n",
    "\n",
    "\n",
    "\n",
    "    ax = plt.subplot(4, 1, 4)\n",
    "    plt.plot(y_test_1,'k')\n",
    "    plt.plot(y_pred_1_inverse_transform,'r--')\n",
    "    plt.ylabel(output_term222[1], fontproperties=font)  \n",
    "    ax.yaxis.set_label_coords(-0.07, 0.5)  \n",
    "    ax.text(0.5, ditem, f'{subplot_labels[3]} {output_term333[1]}预测效果', \n",
    "            transform=ax.transAxes, ha='center', fontproperties=font)  \n",
    "    ax.set_xlabel('测试样本', fontproperties=font)  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    plt.subplots_adjust(hspace=0.6)  # 调整子图之间的垂直间距\n",
    "    plt.tight_layout()  # 自动调整子图布局\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 高炉模型建模效果\n",
    "y_train_pred_0,y_train_pred_1 = model_gaolu.my_predict(X_gaolu_train)\n",
    "y_test_pred_0,y_test_pred_1 = model_gaolu.my_predict(X_gaolu_test)\n",
    "\n",
    "double_control_train_test_result(scalers_Y,  output_term,\n",
    "                                        y_gaolu_train,  y_train_pred_0, y_train_pred_1,\n",
    "                                        y_gaolu_test ,   y_test_pred_0,  y_test_pred_1,\n",
    "                                        ditem = -0.65   ,a = 0.63, b = 0.38 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建预测模型实例\n",
    "# 设置随机种子\n",
    "torch.manual_seed(0)\n",
    "model_predict = MyNeuralNetwork(input_size, hidden_size, output_size,ischuangxin)\n",
    "epoch_sum_predict = 0\n",
    "predict_train_loss_list = []\n",
    "predict_val_loss_list = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预测模型训练\n",
    "epoch_once = epoch_once_time\n",
    "epoch_sum = epoch_sum_predict + epoch_once\n",
    "predict_train_loss_list, predict_val_loss_list = model_predict.my_fit(X_predict_train, y_predict_train,\n",
    "                                    X_predict_val, y_predict_val, \n",
    "                                    predict_train_loss_list, predict_val_loss_list,\n",
    "                                    epochs=epoch_once, \n",
    "                                    batch_size=64,\n",
    "                                    lr = 0.002)\n",
    "\n",
    "print('\\nepoch_sum:',epoch_sum_predict)\n",
    "\n",
    "\n",
    "# 绘制训练和验证损失曲线\n",
    "plt.plot(predict_train_loss_list, label='Train Loss')\n",
    "plt.plot(predict_val_loss_list, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预测模型建模效果\n",
    "y_train_pred_0,y_train_pred_1 = model_predict.my_predict(X_predict_train)\n",
    "y_test_pred_0,y_test_pred_1 = model_predict.my_predict(X_predict_test)\n",
    "\n",
    "double_control_train_test_result(scalers_Y,  output_term,\n",
    "                                        y_predict_train,  y_train_pred_0, y_train_pred_1,\n",
    "                                        y_predict_test,   y_test_pred_0,  y_test_pred_1,\n",
    "                                        ditem = -0.65   ,a = 0.31, b = 0.38 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base.gaolu_predict_raw(scalers_Y,output_term,model_predict,model_gaolu,X_predict_test,y_predict_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用NumPy重新构建神经网络架构\n",
    "class MyNeuralNetworkNumpy:\n",
    "    def __init__(self, model, input_size, hidden_size, output_size,ifchuangxin):\n",
    "        self.ifchuangxin = ifchuangxin\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        params = {name: param.detach().numpy() for name, param in model.state_dict().items()}\n",
    "        if cengshu == 3:    \n",
    "            self.weights_fc1 = params['fc1.weight']\n",
    "            self.bias_fc1 = params['fc1.bias']\n",
    "            self.weights_fc2 = params['fc2.weight']\n",
    "            self.bias_fc2 = params['fc2.bias']\n",
    "            self.weights_fc3 = params['fc3.weight']\n",
    "            self.bias_fc3 = params['fc3.bias']\n",
    "            self.weights_fc4 = params['fc4.weight']\n",
    "            self.bias_fc4 = params['fc4.bias']\n",
    "        elif cengshu == 2:  \n",
    "            self.weights_fc1 = params['fc1.weight']\n",
    "            self.bias_fc1 = params['fc1.bias']\n",
    "            self.weights_fc2 = params['fc2.weight']\n",
    "            self.bias_fc2 = params['fc2.bias']\n",
    "            self.weights_fc3 = params['fc3.weight']\n",
    "            self.bias_fc3 = params['fc3.bias']\n",
    "        \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    def relu(self, x):\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if cengshu == 3:    \n",
    "            if self.ifchuangxin:\n",
    "                hidden1 = np.dot(x, self.weights_fc1.T) + self.bias_fc1\n",
    "                hidden1 = self.relu(hidden1)\n",
    "\n",
    "                hidden2 = np.dot(hidden1, self.weights_fc2.T) + self.bias_fc2\n",
    "                hidden2 = self.relu(hidden2)\n",
    "\n",
    "                hidden3 = np.dot(hidden2, self.weights_fc3.T) + self.bias_fc3\n",
    "                hidden3 = self.relu(hidden3)\n",
    "\n",
    "                hidden4 = hidden1 + hidden2 + hidden3\n",
    "                output = np.dot(hidden4, self.weights_fc4.T) + self.bias_fc4\n",
    "\n",
    "            else:\n",
    "                hidden1 = np.dot(x, self.weights_fc1.T) + self.bias_fc1\n",
    "                hidden1 = self.relu(hidden1)\n",
    "\n",
    "                hidden2 = np.dot(hidden1, self.weights_fc2.T) + self.bias_fc2\n",
    "                hidden2 = self.relu(hidden2)\n",
    "\n",
    "                hidden3 = np.dot(hidden2, self.weights_fc3.T) + self.bias_fc3\n",
    "                hidden3 = self.relu(hidden3)\n",
    "\n",
    "                output = np.dot(hidden3, self.weights_fc4.T) + self.bias_fc4\n",
    "        elif cengshu == 2:  \n",
    "            if self.ifchuangxin:\n",
    "                hidden1 = np.dot(x, self.weights_fc1.T) + self.bias_fc1\n",
    "                hidden1 = self.relu(hidden1)\n",
    "\n",
    "                hidden2 = np.dot(hidden1, self.weights_fc2.T) + self.bias_fc2\n",
    "                hidden2 = self.relu(hidden2)\n",
    "\n",
    "                hidden3 = hidden1 + hidden2\n",
    "                output = np.dot(hidden3, self.weights_fc3.T) + self.bias_fc3\n",
    "                # hidden2 = self.relu(hidden2)\n",
    "\n",
    "                # hidden3 = np.concatenate([x, hidden1, hidden2], axis=1)  # 按列连接\n",
    "                # output = np.dot(hidden3, self.weights_fc3.T) + self.bias_fc3\n",
    "\n",
    "            else:\n",
    "                hidden1 = np.dot(x, self.weights_fc1.T) + self.bias_fc1\n",
    "                hidden1 = self.relu(hidden1)\n",
    "\n",
    "                hidden2 = np.dot(hidden1, self.weights_fc2.T) + self.bias_fc2\n",
    "                hidden2 = self.relu(hidden2)\n",
    "\n",
    "                output = np.dot(hidden2, self.weights_fc3.T) + self.bias_fc3\n",
    "\n",
    "\n",
    "            \n",
    "        return output\n",
    "    \n",
    "    \n",
    "    def my_predict(self, data_input):\n",
    "        input = data_input  # 随机初始化一个输入序列\n",
    "        output_prediction = model_numpy.forward(input)\n",
    "        return output_prediction[:,0], output_prediction[:,1]\n",
    "\n",
    "# 使用NumPy模型进行预测\n",
    "model_numpy = MyNeuralNetworkNumpy(model_predict, input_size, hidden_size, output_size,ischuangxin)\n",
    "\n",
    "y_pred_0_numpy, y_pred_1_numpy = model_numpy.my_predict(X_predict_test)\n",
    "y_pred_0,       y_pred_1       = model_predict.my_predict(X_predict_test)\n",
    "\n",
    "print(\"验证原模型与numpy模型的输出是否一致：\")\n",
    "result_d_state = np.fabs(y_pred_0_numpy-y_pred_0)<1e-5\n",
    "# print(result_d_state)\n",
    "print('总数量：',y_pred_0_numpy.shape[0],',错误：',np.sum(result_d_state==False),'，正确：',np.sum(result_d_state==True))\n",
    "result_d_state = np.fabs(y_pred_1_numpy-y_pred_1)<1e-6\n",
    "# print(result_d_state)\n",
    "print('总数量：',y_pred_0_numpy.shape[0],',错误：',np.sum(result_d_state==False),'，正确：',np.sum(result_d_state==True))\n",
    "\n",
    "\n",
    "\n",
    "# # 计算 RMSE、MRE\n",
    "# y_test = y_predict_test\n",
    "\n",
    "\n",
    "# base.double_control_predict_result(scalers_Y,output_term,y_test,y_pred_0,y_pred_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 控制参数设置\n",
    "iscontrol = True\n",
    "# iscontrol = False\n",
    "Times = 50\n",
    "# 过度系数  0.1 越小过度越快\n",
    "rou = 0.1\n",
    "if_add_noise = False\n",
    "if_gaolu_is_predict = False\n",
    "\n",
    "if_update_model = True\n",
    "# if_update_model = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成参考轨迹\n",
    "def get_yr(aim_value,current_value,alpha,P):\n",
    "    # 生成设定信号\n",
    "    setpoint_signal = np.full(10, aim_value)\n",
    "    # 初始化参数\n",
    "    alpha = alpha\n",
    "    y_r = np.zeros(P)\n",
    "    y_r[0] = current_value\n",
    "    # 模拟一阶模型\n",
    "    for k in range(1,P):\n",
    "        y_r[k] = alpha * y_r[k-1] + (1 - alpha) * aim_value\n",
    "\n",
    "    # # 绘制结果\n",
    "    # plt.plot(setpoint_signal, label='Setpoint Signal')\n",
    "    # plt.plot(y_r,'o-', label='Output Signal (Tracked)')\n",
    "    # plt.legend()\n",
    "    # plt.xlabel('Time')\n",
    "    # plt.ylabel('Amplitude')\n",
    "    # plt.title('Tracking Setpoint Signal with One-Order Model')\n",
    "    # plt.show()\n",
    "    return y_r\n",
    "# 测试\n",
    "y_r = get_yr(1,-0.5,rou,20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成期望数据\n",
    "\n",
    "def generate_y_aim_data(Times):\n",
    "    if Times == 50:\n",
    "        set_y1 = np.repeat(np.arange(1480, 1520, 5), 10)[10+5:80-15]\n",
    "        set_y2 = np.repeat(np.arange(0.30, 0.70, 0.05), 10)[0+5:80-25]\n",
    "        \n",
    "    elif Times == 200:\n",
    "        set_y1 = np.repeat(np.arange(1455, 1560, 5), 20)[10+75:410-125]\n",
    "        set_y2 = np.repeat(np.arange(0.34, 0.76, 0.02), 20)[0+75:400-125]\n",
    "\n",
    "    elif Times == 400:\n",
    "        set_y1 = np.repeat(np.arange(1455, 1560, 5), 20)[10:410]\n",
    "        set_y2 = np.repeat(np.arange(0.34, 0.76, 0.02), 20)[0:400]\n",
    "        \n",
    "    elif Times == 1000:\n",
    "        set_y1 = np.repeat(np.arange(1455, 1560, 5), 20)[10:410]\n",
    "        set_y2 = np.repeat(np.arange(0.34, 0.76, 0.02), 20)[0:400]\n",
    "        set_y1 = np.repeat(np.arange(1457.5, 1562.5, 5), 20)[10:410]\n",
    "        set_y2 = np.repeat(np.arange(0.35, 0.77, 0.02), 20)[0:400]\n",
    "\n",
    "    else:\n",
    "        set_y1 = np.full(Times,1500)\n",
    "        set_y2 = np.full(Times,0.45)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    set_y1_trans = scalers_Y[output_term[0]].transform(set_y1.reshape(-1,1)).flatten()\n",
    "    set_y2_trans = scalers_Y[output_term[1]].transform(set_y2.reshape(-1,1)).flatten()\n",
    "\n",
    "    return set_y1, set_y2, set_y1_trans, set_y2_trans\n",
    "\n",
    "set_y1, set_y2, set_y1_trans, set_y2_trans = generate_y_aim_data(Times)\n",
    "print(set_y1.shape)\n",
    "print(set_y2.shape)\n",
    "print(set_y1_trans.shape)\n",
    "print(set_y2_trans.shape)\n",
    "print(set_y1)\n",
    "print(set_y2)\n",
    "print(set_y1_trans)\n",
    "print(set_y2_trans)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成高斯噪声,设置随机种子，以便结果可重现\n",
    "np.random.seed(42)\n",
    "gaussian_noise_SI = np.random.normal(0,d_yuansu*0.001,Times)\n",
    "gaussian_noise_TEMP = np.random.normal(0,d_temp*0.1,Times)\n",
    "# plt.subplot(2, 1, 1)\n",
    "# plt.plot(gaussian_noise_SI)\n",
    "# plt.subplot(2, 1, 2)\n",
    "# plt.plot(gaussian_noise_TEMP)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_model(model_predict,model_gaolu,x,gaolu_data_past_x,gaolu_data_past_y):\n",
    "    y1_pred, y2_pred = model_gaolu.my_predict(x)\n",
    "    y_label = np.column_stack((y1_pred, y2_pred))\n",
    "    gaolu_data_past_x.append(x)\n",
    "    gaolu_data_past_y.append(y_label)\n",
    "\n",
    "    \n",
    "    X_modified = np.array(gaolu_data_past_x)\n",
    "    y_modified = np.array(gaolu_data_past_y)\n",
    "    # print(X_modified)\n",
    "    # print(y_modified)\n",
    "    X = X_modified.reshape(X_modified.shape[0],X_modified.shape[2])\n",
    "    Y = y_modified.reshape(y_modified.shape[0],y_modified.shape[2])\n",
    "    # print(X_modified.shape)\n",
    "    # print(y_modified.shape)\n",
    "    if y_modified.shape[0]% 1 == 0:\n",
    "        model_predict.model_update(X, Y,\n",
    "                                epochs=100, \n",
    "                                batch_size=64,\n",
    "                                lr = 0.002,\n",
    "                                isprint = False\n",
    "                                )\n",
    "        # gaolu_data_past_x = []\n",
    "        # gaolu_data_past_y = []\n",
    "\n",
    "    return model_predict,model_gaolu,gaolu_data_past_x,gaolu_data_past_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def series2U_numpy_in_control(z, M, scalers_X, scalers, input_term, isprint = True):\n",
    "\n",
    "    if(isprint):print(z.shape)\n",
    "\n",
    "    # 生成一些随机数据进行测试\n",
    "    x =  z.reshape(M, z_dim)\n",
    "\n",
    "    # 前向传播\n",
    "    generated_data = lsgan_generator_numpy.forward(x)\n",
    "\n",
    "    if(isprint):print(\"Generated data shape:\", generated_data.shape)\n",
    "    if(isprint):print(\"Generated data:\\n\", generated_data)\n",
    "\n",
    "\n",
    "    # 分别提取 U1, U2, U3, U4\n",
    "    U1 = generated_data[:, 0]\n",
    "    U2 = generated_data[:, 1]\n",
    "    U3 = generated_data[:, 2]\n",
    "    U4 = generated_data[:, 3]\n",
    "\n",
    "    if(isprint):print(\"U1:\", U1)\n",
    "    if(isprint):print(\"U2:\", U2)\n",
    "    if(isprint):print(\"U3:\", U3)\n",
    "    if(isprint):print(\"U4:\", U4)\n",
    "\n",
    "    U1_inverse = scalers_X[input_term[0]].inverse_transform(U1.reshape(-1, 1)).flatten()\n",
    "    U2_inverse = scalers_X[input_term[1]].inverse_transform(U2.reshape(-1, 1)).flatten()\n",
    "    U3_inverse = scalers_X[input_term[2]].inverse_transform(U3.reshape(-1, 1)).flatten()\n",
    "    U4_inverse = scalers_X[input_term[3]].inverse_transform(U4.reshape(-1, 1)).flatten()\n",
    "\n",
    "    if(isprint):print(\"U1_inverse:\", U1_inverse)\n",
    "    if(isprint):print(\"U2_inverse:\", U2_inverse)\n",
    "    if(isprint):print(\"U3_inverse:\", U3_inverse)\n",
    "    if(isprint):print(\"U4_inverse:\", U4_inverse)\n",
    "\n",
    "    \n",
    "    U1_inverse_trans = scalers[input_term[0]].transform(U1_inverse.reshape(-1, 1)).flatten()\n",
    "    U2_inverse_trans = scalers[input_term[1]].transform(U2_inverse.reshape(-1, 1)).flatten()\n",
    "    U3_inverse_trans = scalers[input_term[2]].transform(U3_inverse.reshape(-1, 1)).flatten()\n",
    "    U4_inverse_trans = scalers[input_term[3]].transform(U4_inverse.reshape(-1, 1)).flatten()\n",
    "\n",
    "    \n",
    "    if(isprint):print(\"U1_inverse_trans:\", U1_inverse_trans)\n",
    "    if(isprint):print(\"U2_inverse_trans:\", U2_inverse_trans)\n",
    "    if(isprint):print(\"U3_inverse_trans:\", U3_inverse_trans)\n",
    "    if(isprint):print(\"U4_inverse_trans:\", U4_inverse_trans)\n",
    "\n",
    "    # 将 U1, U2, U3, U4 连接成一个序列\n",
    "    sequence = np.concatenate((U1, U2, U3, U4))\n",
    "    if(isprint):print(\"Connected sequence:\", sequence)\n",
    "\n",
    "    return sequence\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试series2U_numpy_in_control生成\n",
    "\n",
    "z = np.random.randn(z_dim* test_size_generated_data)\n",
    "generated_data = series2U_numpy_in_control(z,test_size_generated_data, scalers_X, scalers_X, input_term, isprint=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义单时刻的MPC问题优化\n",
    "def my_MPC(k_data,params,M,P,y1_aim,y2_aim,isprint):\n",
    "\n",
    "    h1 = 1.0\n",
    "    h2 = 1.0\n",
    "    lamda1 = 0.001\n",
    "    lamda2 = 0.001\n",
    "    lamda3 = 0.001\n",
    "    lamda4 = 0.001\n",
    "    y1_percent = 1.0\n",
    "    y2_percent = 1.0\n",
    "\n",
    "    # 从固定格式k_data里面读取信息\n",
    "    u1   = k_data[0:3]\n",
    "    u2   = k_data[3:6]\n",
    "    u3   = k_data[6:9]\n",
    "    u4   = k_data[9:12]\n",
    "\n",
    "    y1   = k_data[12:15]\n",
    "    y2   = k_data[15:18]\n",
    "\n",
    "    \n",
    "    # 获取猜测值[h U1 U2]\n",
    "    # h, U1, U2  =params[0], params[1:M+1],params[M+1:]    \n",
    "    # params = series2U_numpy_in_control(params,M, scalers_X, scalers_X, input_term, isprint=False)\n",
    "    U1, U2, U3, U4  =params[0:M], params[M:2*M],params[2*M:3*M], params[3*M:4*M]\n",
    "    # 整理数据见   MPC推到.escel\n",
    "    u1   = np.concatenate((u1,U1,U1[-1]*np.ones(P-M)))\n",
    "    u2   = np.concatenate((u2,U2,U2[-1]*np.ones(P-M)))\n",
    "    u3   = np.concatenate((u3,U3,U3[-1]*np.ones(P-M)))\n",
    "    u4   = np.concatenate((u4,U4,U4[-1]*np.ones(P-M)))\n",
    "    y1   = np.concatenate((y1,np.zeros(P)))\n",
    "    y2   = np.concatenate((y2,np.zeros(P)))\n",
    "    if isprint:\n",
    "        print(u1.round(4))\n",
    "        print(u2.round(4))\n",
    "        print(u3.round(4))\n",
    "        print(u4.round(4))\n",
    "        print(y1.round(4))    \n",
    "        print(y2.round(4))\n",
    "        print('开始预测')\n",
    "\n",
    "    y1_k = y1[2]\n",
    "    y2_k = y2[2]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # 总共预测 P+1 次\n",
    "    # 对k时刻进行预测-----1次\n",
    "    for j in range(1):   # j = 0\n",
    "        x = np.column_stack((   u1[j+2],u2[j+2],u3[j+2],u4[j+2],\n",
    "                                u1[j+1],u2[j+1],u3[j+1],u4[j+1],\n",
    "                                y1[j+1],y2[j+1]))\n",
    "        # x = x.reshape((x.shape[0], 1, x.shape[1]))\n",
    "        y1_m_k, y2_m_k = model_numpy.my_predict(x)\n",
    "        E1_k = y1_k - y1_m_k\n",
    "        E2_k = y2_k - y2_m_k\n",
    "        if isprint:\n",
    "            print(j,'mode = 0')\n",
    "            print(x.round(4))\n",
    "            print(y1_k.round(4),y2_k.round(4))\n",
    "            print(y1_m_k.round(4),y2_m_k.round(4))\n",
    "\n",
    "    # 对控制时刻进行预测-----M次\n",
    "    for j in range(1,M+1):  # j = 1,2\n",
    "        x = np.column_stack((   u1[j+2],u2[j+2],u3[j+2],u4[j+2],\n",
    "                                u1[j+1],u2[j+1],u3[j+1],u4[j+1],\n",
    "                                y1[j+1],y2[j+1]))\n",
    "        # x = x.reshape((x.shape[0], 1, x.shape[1]))\n",
    "        y1_k_j, y2_k_j = model_numpy.my_predict(x)\n",
    "        y1[j+2] = y1_k_j.item()\n",
    "        y2[j+2] = y2_k_j.item()\n",
    "        if isprint:\n",
    "            print(j,'mode = 1')\n",
    "            print(x.round(4))\n",
    "            print(y1_k_j.round(4),y2_k_j.round(4))\n",
    "            print('更新后:')\n",
    "            print(u1.round(4))\n",
    "            print(u2.round(4))\n",
    "            print(u3.round(4))\n",
    "            print(u4.round(4))\n",
    "            print(y1.round(4))    \n",
    "            print(y2.round(4))\n",
    "\n",
    "    # 对控制时域外的部分进行预测-----P-M次\n",
    "    # 注意：这部分的信号是保持控制不变下进行\n",
    "    for j in range(M+1,P+1):  #j = 3,4\n",
    "        x = np.column_stack((   u1[j+2],u2[j+2],u3[j+2],u4[j+2],\n",
    "                                u1[j+1],u2[j+1],u3[j+1],u4[j+1],\n",
    "                                y1[j+1],y2[j+1]))\n",
    "        # x = x.reshape((x.shape[0], 1, x.shape[1]))\n",
    "        y1_k_j, y2_k_j = model_numpy.my_predict(x)\n",
    "        y1[j+2] = y1_k_j.item()#将预测值作为下一步的输出值\n",
    "        y2[j+2] = y2_k_j.item()\n",
    "        if isprint:\n",
    "            print(j,'mode = 2')\n",
    "            print(x.round(4))\n",
    "            print(y1_k_j.round(4),y2_k_j.round(4))\n",
    "            print('更新后:')\n",
    "            print(u1.round(4))\n",
    "            print(u2.round(4))\n",
    "            print(u3.round(4))\n",
    "            print(u4.round(4))\n",
    "            print(y1.round(4))    \n",
    "            print(y2.round(4))\n",
    "\n",
    "\n",
    "\n",
    "    k_data2 = np.concatenate((u1[1:4],u2[1:4],u3[1:4],u4[1:4],y1[1:4],y2[1:4]),axis=0)\n",
    "    if isprint:\n",
    "        print('更新k_data')\n",
    "        print(k_data2.round(4))\n",
    "\n",
    "\n",
    "    #获取参考轨迹\n",
    "    # 一定要对照好做差的序列\n",
    "    y1_r_aim  = get_yr(y1_aim,y1_k,rou,P+1)\n",
    "    y1_r = y1_r_aim[1:] \n",
    "\n",
    "\n",
    "    y2_r_aim  = get_yr(y2_aim,y2_k,rou,P+1)\n",
    "    y2_r = y2_r_aim[1:] \n",
    "\n",
    "    y1_M_k = y1[3:]\n",
    "    y2_M_k = y2[3:]\n",
    "    if isprint==1:\n",
    "        print('反馈补偿:')\n",
    "        print('y1_k',y1_k.round(4))  \n",
    "        print('y1_m_k',y1_m_k.round(4))    \n",
    "        print('h*E1_k',(h1*E1_k).round(4)) \n",
    "        print('y2_k',y2_k.round(4))  \n",
    "        print('y2_m_k',y2_m_k.round(4))   \n",
    "        print('h*E2_k',(h2*E2_k).round(4))\n",
    "\n",
    "        print('temp:')\n",
    "        print('y1_aim',y1_aim.round(4))\n",
    "        print('y1_r_aim',y1_r_aim.round(4))\n",
    "        print('y1_r',y1_r.round(4))\n",
    "        print('y1_M_k',y1_M_k.round(4))\n",
    "        print('y1_M_k+h1*E1_k',(y1_M_k+h1*E1_k).round(4))\n",
    "\n",
    "        print('Si_percent:')\n",
    "        print('y2_aim',y2_aim.round(4))\n",
    "        print('y2_r_aim',y2_r_aim.round(4))\n",
    "        print('y2_r',y2_r.round(4))\n",
    "        print('y2_M_k',y2_M_k.round(4))\n",
    "        print('y2_M_k+h2*E2_k',(y2_M_k+h2*E2_k).round(4))\n",
    "\n",
    "        print('u:')\n",
    "        print(u1[2:].round(4))\n",
    "        print(u2[2:].round(4))\n",
    "        print(u3[2:].round(4))\n",
    "        print(u4[2:].round(4))\n",
    "        \n",
    "    # 计算mse\n",
    "    # lamda1太大的话会导致y1_r和y1_M_k的误差加大*****************导致超调的原因\\与目标值之间存在间隙\n",
    "\n",
    "\n",
    "    y1_err = y1_percent*np.sum((y1_r-(y1_M_k+h1*E1_k))**2) \n",
    "    y2_err = y2_percent*np.sum((y2_r-(y2_M_k+h2*E2_k))**2) \n",
    "    u1_power = lamda1*np.sum((np.diff(u1[2:]))**2)\n",
    "    u2_power = lamda2*np.sum((np.diff(u2[2:]))**2)\n",
    "    u3_power = lamda3*np.sum((np.diff(u3[2:]))**2)\n",
    "    u4_power = lamda4*np.sum((np.diff(u4[2:]))**2)\n",
    "\n",
    "    # y1_err = y1_percent*np.sum(np.fabs(y1_r-(y1_M_k+h1*E1_k))) \n",
    "    # y2_err = y2_percent*np.sum(np.fabs(y2_r-(y2_M_k+h2*E2_k))) \n",
    "    # u1_power = lamda1*np.sum((np.fabs(np.diff(u1))))\n",
    "    # u2_power = lamda2*np.sum((np.fabs(np.diff(u2))))\n",
    "    # u3_power = lamda3*np.sum((np.fabs(np.diff(u3))))\n",
    "    # u4_power = lamda4*np.sum((np.fabs(np.diff(u4))))\n",
    "    # u5_power = lamda2*np.sum((np.fabs(np.diff(u5))))\n",
    "    # u6_power = lamda3*np.sum((np.fabs(np.diff(u6))))\n",
    "    # u7_power = lamda4*np.sum((np.fabs(np.diff(u7))))\n",
    "\n",
    "    mse = (0\n",
    "            +y1_err\n",
    "            +y2_err\n",
    "            +u1_power\n",
    "            +u2_power\n",
    "            +u3_power\n",
    "            +u4_power\n",
    "            )\n",
    "    \n",
    "    # print('mse {:.7f}'.format(mse))\n",
    "    # if isprint==1:\n",
    "    print('mse {:.7f}'.format(mse))\n",
    "    # print('1111 {:.7f}'.format(y1_err))\n",
    "    # print('2222 {:.7f}'.format(y2_err))\n",
    "    # print('1111 {:.7f}'.format(u1_power))\n",
    "    # print('2222 {:.7f}'.format(u2_power))\n",
    "    # print('3333 {:.7f}'.format(u3_power))\n",
    "    # print('4444 {:.7f}'.format(u4_power))\n",
    "\n",
    "\n",
    "\n",
    "    return mse , k_data2, E1_k*h1,  E2_k*h2\n",
    "    # return mse , k_data2, E1_k*h1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对未来Times周期预测控制\n",
    "max_control = 1.0\n",
    "# 期望设定值\n",
    "set_y1, set_y2, set_y1_trans, set_y2_trans = generate_y_aim_data(Times)\n",
    "\n",
    "# MPC参数\n",
    "P = 3  # 预测时域长度  3\n",
    "M = 3  # 4\n",
    "#生成控制时域的数据格式\n",
    "# k_data = np.zeros(6*M)\n",
    "k_data = [ 0.2751 ,-0.7662 ,-0.8419 , 0.2653 ,-0.5934 ,-0.5297, -0.0021, -0.5839, -0.1657,\n",
    "  0.7492 ,-0.0294,  0.041 , -0.0727,-0.0187 ,-0.2888, -0.5091, -0.1361, -0.2065]\n",
    "# print(k_data.shape)\n",
    "print(k_data)\n",
    "\n",
    "\n",
    "# MPC控制循环   迭代的只有：k_data\n",
    "all_pred_y1 = []\n",
    "all_pred_y2 = []\n",
    "all_pred_u1 = []\n",
    "all_pred_u2 = []\n",
    "all_pred_u3 = []\n",
    "all_pred_u4 = []\n",
    "gaolu_data_past_x = []\n",
    "gaolu_data_past_y = []\n",
    "\n",
    "\n",
    "# MPC控制循环40\n",
    "for k in range(50):\n",
    "    if iscontrol == False:\n",
    "        break\n",
    "\n",
    "    print(f\"这是对第{k}时刻的最优U1、U2输入求解\")\n",
    "\n",
    "    # 定义优化目标函数\n",
    "    def objective_function(params, *k_data):\n",
    "        mse, k_data2, E1_k_0, E2_k_0 = my_MPC(k_data=k_data[0], params=params, \n",
    "                                M=M, P=P, \n",
    "                                y1_aim = set_y1_trans[k], y2_aim = set_y2_trans[k],\n",
    "                                isprint = 0) \n",
    "        return mse\n",
    "    \n",
    "    # 初始猜测值[h U1 U2]   定义参数的上下限    设置退出条件\n",
    "\n",
    "    \n",
    "    # z = np.ones(z_dim * M)\n",
    "    # bounds = [(-max_control, max_control) for _ in range(z_dim * M)]\n",
    "\n",
    "    z = np.concatenate([np.ones(M), np.ones(M),np.ones(M), np.ones(M)])\n",
    "    bounds = [(-max_control, max_control) for _ in range(4 * M)]\n",
    "\n",
    "    # 进行优化\n",
    "    options = {\n",
    "    'maxiter': 1000,      # 最大迭代次数\n",
    "    'disp': True,         # 显示详细的优化过程信息\n",
    "    'factr': 1e-20,       # 调整收敛精度（降低收敛阈值）\n",
    "    }\n",
    "\n",
    "    def callback_func(xk):\n",
    "        print(f\"Current params: {xk}\")\n",
    "        \n",
    "    result = minimize(objective_function, z, method='L-BFGS-B', \n",
    "                    bounds=bounds, args=k_data,  # 传递 k_data 元组\n",
    "                    callback=lambda xk: callback_func(xk),\n",
    "                    options=options)\n",
    "    print(result.message)\n",
    "    # result_u = series2U_numpy_in_control(result.x,M, scalers_X, scalers_X, input_term, isprint=False)\n",
    "    result_u = result.x\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    U1, U2, U3, U4 =    result_u[0:M], result_u[M:2*M], \\\n",
    "                        result_u[2*M:3*M], result_u[3*M:4*M]\n",
    "\n",
    "    # 从固定格式k_data里面读取信息\n",
    "    u1   = k_data[0:3]\n",
    "    u2   = k_data[3:6]\n",
    "    u3   = k_data[6:9]\n",
    "    u4   = k_data[9:12]\n",
    "\n",
    "    y1   = k_data[12:15]\n",
    "    y2   = k_data[15:18]\n",
    "\n",
    "    u1   = np.concatenate((u1,U1,U1[-1]*np.ones(P-M)))\n",
    "    u2   = np.concatenate((u2,U2,U2[-1]*np.ones(P-M)))\n",
    "    u3   = np.concatenate((u3,U3,U3[-1]*np.ones(P-M)))\n",
    "    u4   = np.concatenate((u4,U4,U4[-1]*np.ones(P-M)))\n",
    "    y1   = np.concatenate((y1,np.zeros(P)))\n",
    "    y2   = np.concatenate((y2,np.zeros(P)))\n",
    "\n",
    "\n",
    "\n",
    "    # 将控制序列第一个数作用于高炉\n",
    "    j = 1\n",
    "    x = np.column_stack((   u1[j+2],u2[j+2],u3[j+2],u4[j+2],\n",
    "                            u1[j+1],u2[j+1],u3[j+1],u4[j+1],\n",
    "                            y1[j+1],y2[j+1]))\n",
    "    # x = x.reshape((x.shape[0], 1, x.shape[1]))\n",
    "    y1_pred0, y2_pred0 = model_predict.my_predict(x)\n",
    "    y1_pred0_numpy, y2_pred0_numpy = model_numpy.my_predict(x)\n",
    "    if if_gaolu_is_predict:\n",
    "        y1_pred, y2_pred = model_predict.my_predict(x)\n",
    "        if if_add_noise:\n",
    "            y1_pred = y1_pred+gaussian_noise_TEMP[k].item()\n",
    "            y2_pred = y2_pred+gaussian_noise_SI[k].item()\n",
    "    else:\n",
    "        y1_pred, y2_pred = model_gaolu.my_predict(x)\n",
    "        if if_update_model:\n",
    "            model_predict,model_gaolu,gaolu_data_past_x,gaolu_data_past_y = update_model(model_predict,model_gaolu,x,gaolu_data_past_x,gaolu_data_past_y)\n",
    "            # 使用NumPy模型进行预测\n",
    "            model_numpy = MyNeuralNetworkNumpy(model_predict, input_size, hidden_size, output_size,ischuangxin)\n",
    "\n",
    "\n",
    "\n",
    "    # 更新k_data\n",
    "    params = result.x\n",
    "    mse, k_data2, E1_k_0, E2_k_0 =my_MPC(k_data=k_data,params=params,\n",
    "                            M=M,P=P, \n",
    "                            y1_aim = set_y1_trans[k], y2_aim = set_y2_trans[k],\n",
    "                            isprint = 1) \n",
    "\n",
    "\n",
    "    print(  '1设定',set_y1_trans[k].round(4),\\\n",
    "            '预测',y1_pred0.round(4),\\\n",
    "            '高炉', y1_pred.round(4),\\\n",
    "            '高炉与设定误差',(set_y1_trans[k]-y1_pred).round(4),(set_y1_trans[k]-y1_pred).round(4)/d_temp,'°C',\\\n",
    "            '模型误差',(y1_pred0 - y1_pred).round(4),\\\n",
    "            '校正值',E1_k_0.round(4))\n",
    "\n",
    "    print(  '2设定',set_y2_trans[k].round(4),\\\n",
    "            '预测',y2_pred0.round(4),\\\n",
    "            '高炉', y2_pred.round(4),\\\n",
    "            '高炉与设定误差',(set_y2_trans[k]-y2_pred).round(4),(set_y2_trans[k]-y2_pred).round(4)/d_yuansu,'%',\\\n",
    "            '模型误差',(y2_pred0 - y2_pred).round(4),\\\n",
    "            '校正值',E2_k_0.round(4))\n",
    "\n",
    "\n",
    "    all_pred_y1.append(y1_pred)\n",
    "    all_pred_y2.append(y2_pred)\n",
    "    all_pred_u1.append(U1[0])\n",
    "    all_pred_u2.append(U2[0])\n",
    "    all_pred_u3.append(U3[0])\n",
    "    all_pred_u4.append(U4[0])\n",
    "    k_data2[14] = y1_pred.item()\n",
    "    k_data2[17] = y2_pred.item()\n",
    "    k_data = k_data2\n",
    "\n",
    "    print(k_data)\n",
    "    # 进入下一时刻，更新预测时域、控制时域，即k_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if iscontrol:\n",
    "    y1_pred_inverse_transform = scalers_Y[output_term[0]].inverse_transform(np.array(all_pred_y1).reshape(-1, 1)).flatten()\n",
    "    y2_pred_inverse_transform = scalers_Y[output_term[1]].inverse_transform(np.array(all_pred_y2).reshape(-1, 1)).flatten()\n",
    "    \n",
    "    startt = 0\n",
    "    endd = 400\n",
    "    \n",
    "    plt.figure(figsize=(7, 6))\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(set_y1[startt:endd],'r', label='设定值')\n",
    "    plt.plot(y1_pred_inverse_transform[startt:endd],'bo-', label='实际值')\n",
    "    plt.axvline(x=0, color='gray', linestyle='--', linewidth=1.5)\n",
    "    plt.grid(linestyle='--', alpha=0.7, color='gray')\n",
    "    plt.ylabel(output_term[0], fontproperties=font)  # 使用中文标签\n",
    "    plt.legend(prop=font)\n",
    "    plt.title(f\"模型:MLP 训练次数:{epoch_sum_gaolu} 隐含层数:{3} 改进:{ischuangxin} 动态更新:{if_update_model} P:{P} M:{M} \", fontproperties=font)\n",
    "\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(set_y2[startt:endd],'r')\n",
    "    plt.plot(y2_pred_inverse_transform[startt:endd],'bo-')\n",
    "    plt.ylabel(output_term[1], fontproperties=font)  # 使用中文标签\n",
    "    plt.axvline(x=0, color='gray', linestyle='--', linewidth=1.5)\n",
    "    plt.grid(linestyle='--', alpha=0.7, color='gray')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "startt = 0\n",
    "endd = 50\n",
    "# 合并 scaler\n",
    "scalers = scalers_X | scalers_Y\n",
    "y1_pred_inverse_transform = scalers[output_term[0]].inverse_transform(np.array(all_pred_y1[startt:endd]).reshape(-1, 1)).flatten()\n",
    "y2_pred_inverse_transform = scalers[output_term[1]].inverse_transform(np.array(all_pred_y2[startt:endd]).reshape(-1, 1)).flatten()\n",
    "all_pred_u1_inverse_transform = scalers[input_term[0]].inverse_transform(np.array(all_pred_u1[startt:endd]).reshape(-1, 1)).flatten()\n",
    "all_pred_u2_inverse_transform = scalers[input_term[1]].inverse_transform(np.array(all_pred_u2[startt:endd]).reshape(-1, 1)).flatten()\n",
    "all_pred_u3_inverse_transform = scalers[input_term[2]].inverse_transform(np.array(all_pred_u3[startt:endd]).reshape(-1, 1)).flatten()\n",
    "all_pred_u4_inverse_transform = scalers[input_term[3]].inverse_transform(np.array(all_pred_u4[startt:endd]).reshape(-1, 1)).flatten()\n",
    "\n",
    "# # 将数据转换为DataFrame\n",
    "# data = {\n",
    "#     'set_y1': set_y1,\n",
    "#     'set_y2': set_y2,\n",
    "#     'RES_MLP_U_y1': y1_pred_inverse_transform,\n",
    "#     'MLP_U_y2': y2_pred_inverse_transform,\n",
    "#     'RES_MLP_U_u1': all_pred_u1_inverse_transform,\n",
    "#     'RES_MLP_U_u2': all_pred_u2_inverse_transform,\n",
    "#     'RES_MLP_U_u3': all_pred_u3_inverse_transform,\n",
    "#     'RES_MLP_U_u4': all_pred_u4_inverse_transform\n",
    "# }\n",
    "\n",
    "# df = pd.DataFrame(data)\n",
    "\n",
    "# # 保存DataFrame到Excel文件\n",
    "# df.to_excel('pred_data_RES_MLP_U_'+str(cengshu)+'.xlsx', index=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "a1 = scalers[input_term[0]].inverse_transform(np.array([1,-1]).reshape(-1, 1)).flatten()\n",
    "a2 = scalers[input_term[1]].inverse_transform(np.array([1,-1]).reshape(-1, 1)).flatten()\n",
    "a3 = scalers[input_term[2]].inverse_transform(np.array([1,-1]).reshape(-1, 1)).flatten()\n",
    "a4 = scalers[input_term[3]].inverse_transform(np.array([1,-1]).reshape(-1, 1)).flatten()\n",
    "print(f'上线分别是：{a1}、{a2}、{a3}、{a4}')\n",
    "\n",
    "\n",
    "rmse_1 = np.mean(np.fabs(set_y1[startt:endd]-y1_pred_inverse_transform))\n",
    "rmse_2 = np.mean(np.fabs(set_y2[startt:endd]-y2_pred_inverse_transform))\n",
    "print('平均误差',rmse_1.round(4))\n",
    "print('平均误差',rmse_2.round(4))\n",
    "\n",
    "# 模型预测控制结果可视化\n",
    "# 创建两个子图，分别绘制每个维度\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# 第一个维度的曲线\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(set_y1[startt:endd], 'ro-', label='设定值')\n",
    "plt.plot(y1_pred_inverse_transform, 'bo-', label='RES_MLP_U 实际值')\n",
    "plt.axvline(x=0, color='gray', linestyle='--', linewidth=1.5)\n",
    "plt.xlim((0,endd-startt))\n",
    "plt.ylabel(output_term[0], fontproperties=font)  # 使用中文标签\n",
    "plt.legend(prop=font)\n",
    "plt.grid(linestyle='--', alpha=0.7, color='gray')\n",
    "\n",
    "# 第二个维度的曲线\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(set_y2[startt:endd], 'ro-')\n",
    "plt.plot(y2_pred_inverse_transform, 'bo-')\n",
    "plt.axvline(x=0, color='gray', linestyle='--', linewidth=1.5)\n",
    "plt.xlim((0,endd-startt))\n",
    "plt.ylabel(output_term[1], fontproperties=font)  # 使用中文标签\n",
    "plt.legend()\n",
    "plt.grid(linestyle='--', alpha=0.7, color='gray')\n",
    "\n",
    "# 调整子图布局\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "# 第一个维度的u1曲线\n",
    "plt.subplot(4, 1, 1)\n",
    "plt.plot(all_pred_u1_inverse_transform, 'bo-')\n",
    "plt.axvline(x=0, color='gray', linestyle='--', linewidth=1.5)\n",
    "plt.xlim((0,endd-startt))\n",
    "plt.ylabel(input_term[0], fontproperties=font)  # 使用中文标签\n",
    "plt.legend()\n",
    "plt.grid(linestyle='--', alpha=0.7, color='gray')\n",
    "\n",
    "# 第二个维度的u2曲线\n",
    "plt.subplot(4, 1, 2)\n",
    "plt.plot(all_pred_u2_inverse_transform, 'bo-')\n",
    "plt.axvline(x=0, color='gray', linestyle='--', linewidth=1.5)\n",
    "plt.xlim((0,endd-startt))\n",
    "plt.ylabel(input_term[1], fontproperties=font)  # 使用中文标签\n",
    "plt.legend()\n",
    "plt.grid(linestyle='--', alpha=0.7, color='gray')\n",
    "\n",
    "# 第三个维度的u3曲线\n",
    "plt.subplot(4, 1, 3)\n",
    "plt.plot(all_pred_u3_inverse_transform, 'bo-')  # 修改标签为 'u3'\n",
    "plt.axvline(x=0, color='gray', linestyle='--', linewidth=1.5)\n",
    "plt.xlim((0,endd-startt))\n",
    "plt.ylabel(input_term[2], fontproperties=font)  # 使用中文标签\n",
    "plt.legend()\n",
    "plt.grid(linestyle='--', alpha=0.7, color='gray')\n",
    "\n",
    "# 第四个维度的u4曲线\n",
    "plt.subplot(4, 1, 4)\n",
    "plt.plot(all_pred_u4_inverse_transform, 'bo-')  # 修改标签为 'u4'\n",
    "plt.axvline(x=0, color='gray', linestyle='--', linewidth=1.5)\n",
    "plt.xlim((0,endd-startt))\n",
    "plt.ylabel(input_term[3], fontproperties=font)  # 使用中文标签\n",
    "plt.legend()\n",
    "plt.grid(linestyle='--', alpha=0.7, color='gray')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39_tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
