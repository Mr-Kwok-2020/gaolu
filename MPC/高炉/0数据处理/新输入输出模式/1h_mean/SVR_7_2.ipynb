{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 库文件\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "from skopt import gp_minimize\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "# 库文件\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "from skopt import gp_minimize\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "import optuna\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.svm import SVR\n",
    "from scipy.optimize import minimize\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.svm import SVR\n",
    "from scipy.optimize import minimize\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "# 库文件\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from skopt import gp_minimize\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.font_manager import FontProperties\n",
    "# 设置中文字体\n",
    "font = FontProperties(fname=r\"c:\\windows\\fonts\\simsun.ttc\", size=12)  # 替换为你的中文字体文件路径\n",
    "\n",
    "from itertools import chain\n",
    "import sys\n",
    "sys.path.append(r\"C:\\Users\\haokw\\Documents\\GitHub\\gaolu\\MPC\\高炉\")\n",
    "import base\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取Excel文件\n",
    "excel_path = f'C:\\\\Users\\\\haokw\\\\Documents\\\\GitHub\\\\gaolu\\\\MPC\\\\高炉\\\\0数据处理\\\\新输入输出模式\\\\1h_mean.xlsx'\n",
    "df_sheet_yuansu = pd.read_excel(excel_path, sheet_name='原始输出') \n",
    "# df_sheet_yuansu = pd.read_excel(excel_path, sheet_name='剔除直线输出') \n",
    "# df_sheet_yuansu = pd.read_excel(excel_path, sheet_name='单SI_0.2_0.8') \n",
    "# print(df_sheet_yuansu.info())\n",
    "# print(df_sheet_yuansu.columns)\n",
    "\n",
    "excel_path = f'C:\\\\Users\\\\haokw\\\\Documents\\\\GitHub\\\\gaolu\\\\MPC\\\\高炉\\\\0数据处理\\\\新输入输出模式\\\\1h_mean.xlsx'\n",
    "df_sheet_params = pd.read_excel(excel_path, sheet_name='1h_mean') \n",
    "\n",
    "# print(df_sheet_params.info())\n",
    "# print(df_sheet_params.columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义输入输出参数\n",
    "input_term = ['富氧流量', '冷风流量', '热风压力', '冷风温度', '热风温度', '鼓风湿度', '设定喷煤量']\n",
    "output_term = ['铁口1温度', 'SI']\n",
    "time_term= '时间戳h'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 异常数据处理-处理前后对比\n",
    "# 创建数据框副本以避免修改原始数据\n",
    "df_sheet_yuansu_process = df_sheet_yuansu.copy()\n",
    "df_sheet_params_process = df_sheet_params.copy()\n",
    "\n",
    "\n",
    "\n",
    "# 对指定列应用替代异常值的函数\n",
    "# 对指定列应用替代异常值的函数\n",
    "base.replace_outliers_with_weighted_diff(df_sheet_params_process[time_term], df_sheet_params_process[input_term[0]])\n",
    "base.replace_outliers_with_weighted_diff(df_sheet_params_process[time_term], df_sheet_params_process[input_term[1]])\n",
    "base.replace_outliers_with_weighted_diff(df_sheet_params_process[time_term], df_sheet_params_process[input_term[2]])\n",
    "base.replace_outliers_with_weighted_diff(df_sheet_params_process[time_term], df_sheet_params_process[input_term[3]])\n",
    "base.replace_outliers_with_weighted_diff(df_sheet_params_process[time_term], df_sheet_params_process[input_term[4]])\n",
    "base.replace_outliers_with_weighted_diff(df_sheet_params_process[time_term], df_sheet_params_process[input_term[5]])\n",
    "base.replace_outliers_with_weighted_diff(df_sheet_params_process[time_term], df_sheet_params_process[input_term[6]])\n",
    "\n",
    "base.replace_outliers_with_weighted_diff(df_sheet_yuansu_process[time_term], df_sheet_yuansu_process[output_term[0]])\n",
    "base.replace_outliers_with_weighted_diff(df_sheet_yuansu_process[time_term], df_sheet_yuansu_process[output_term[1]])\n",
    "\n",
    "# plt.figure(figsize=(15, 10))\n",
    "# for idx, column in enumerate(input_term):\n",
    "#     plt.subplot(len(input_term), 1, idx+1)\n",
    "#     base.plot_subplot(df_sheet_params_process[time_term].values,df_sheet_params[column].values,df_sheet_params_process[column].values,column)\n",
    "\n",
    "# plt.figure(figsize=(15, 10))\n",
    "# for idx, column in enumerate(output_term):\n",
    "#     plt.subplot(len(output_term), 1, idx+1)\n",
    "#     base.plot_subplot(df_sheet_yuansu_process[time_term].values,df_sheet_yuansu[column].values,df_sheet_yuansu_process[column].values,column)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 画出选取的数据\n",
    "def plot_subplot(data_x,data_y,column,index_predict,index_gaolu):\n",
    "    plt.plot(data_x,data_y,'-', label='origin_data')\n",
    "    plt.plot(data_x[index_gaolu],data_y[index_gaolu],'r.', label='gaolu_data')\n",
    "    plt.plot(data_x[index_predict],data_y[index_predict],'g-', label='predict_data')\n",
    "    plt.legend()\n",
    "    # plt.xlabel(time_term, fontproperties=font)  # 使用中文标签\n",
    "    plt.ylabel(column, fontproperties=font)  # 使用中文标签\n",
    "\n",
    "\n",
    "\n",
    "length1 = 400\n",
    "start1 = 0\n",
    "length2 = 400\n",
    "start2 = 4800\n",
    "\n",
    "\n",
    "index_gaolu   = range(start1, start1+length1+1, 1)\n",
    "index_predict     = range(start2, start2+length2+1, 1)\n",
    "# index = range(1, 7572, 1)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "for idx, column in enumerate(output_term):\n",
    "    plt.subplot(len(input_term+output_term), 1, idx+1)\n",
    "    plot_subplot(df_sheet_yuansu_process[time_term].values,df_sheet_yuansu_process[column].values,column,index_predict,index_gaolu)\n",
    "\n",
    "plt.figure(figsize=(15, 6))\n",
    "for idx, column in enumerate(input_term):\n",
    "    plt.subplot(len(input_term+output_term), 1, idx+1)\n",
    "    plot_subplot(df_sheet_params_process[time_term].values,df_sheet_params_process[column].values,column,index_predict,index_gaolu)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据归一化、逆归一化\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# 将数据存储为字典，每个键对应一列数据\n",
    "original_data_dict = {\n",
    "    input_term[0]:   df_sheet_params_process[input_term[0]].values,\n",
    "    input_term[1]:   df_sheet_params_process[input_term[1]].values,\n",
    "    input_term[2]:   df_sheet_params_process[input_term[2]].values,\n",
    "    input_term[3]:   df_sheet_params_process[input_term[3]].values,\n",
    "    input_term[4]:   df_sheet_params_process[input_term[4]].values,\n",
    "    input_term[5]:   df_sheet_params_process[input_term[5]].values,\n",
    "    input_term[6]:   df_sheet_params_process[input_term[6]].values,\n",
    "    output_term[0]:  df_sheet_yuansu_process[output_term[0]].values,\n",
    "    output_term[1]:  df_sheet_yuansu_process[output_term[1]].values\n",
    "}\n",
    "\n",
    "# 初始化缩放器\n",
    "scalers = {}\n",
    "\n",
    "# 进行拟合\n",
    "for column, data in original_data_dict.items():\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    scaler.fit(data.reshape(-1, 1))  # 保证数据是列向量\n",
    "    scalers[column] = scaler\n",
    "\n",
    "# 进行归一化\n",
    "normalized_data_dict = {}\n",
    "for column, scaler in scalers.items():\n",
    "    normalized_data_dict[column] = scaler.transform(original_data_dict[column].reshape(-1, 1)).flatten()\n",
    "\n",
    "# 进行反归一化\n",
    "original_data_dict = {}\n",
    "for column, scaler in scalers.items():\n",
    "    original_data_dict[column] = scaler.inverse_transform(normalized_data_dict[column].reshape(-1, 1)).flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 标定归一化前后数据\n",
    "data_point = np.array([1500]).reshape(-1, 1)\n",
    "data1 = scalers[output_term[0]].transform(data_point).flatten()\n",
    "\n",
    "data_point = np.array(data1).reshape(-1, 1)\n",
    "data2 = scalers[output_term[0]].inverse_transform(data_point).flatten()\n",
    "\n",
    "data_point = np.array([1510]).reshape(-1, 1)\n",
    "data3 = scalers[output_term[0]].transform(data_point).flatten()\n",
    "\n",
    "data_point = np.array(data3).reshape(-1, 1)\n",
    "data4 = scalers[output_term[0]].inverse_transform(data_point).flatten()\n",
    "\n",
    "print(data1)\n",
    "print(data2)\n",
    "print(data3)\n",
    "print(data4)\n",
    "d_temp = (data3-data1)/(data4-data2)\n",
    "print('每摄氏度的输出差：',d_temp)\n",
    "\n",
    "\n",
    "\n",
    "data_point = np.array([0.50]).reshape(-1, 1)\n",
    "data1 = scalers[output_term[1]].transform(data_point).flatten()\n",
    "\n",
    "data_point = np.array(data1).reshape(-1, 1)\n",
    "data2 = scalers[output_term[1]].inverse_transform(data_point).flatten()\n",
    "\n",
    "data_point = np.array([0.60]).reshape(-1, 1)\n",
    "data3 = scalers[output_term[1]].transform(data_point).flatten()\n",
    "\n",
    "data_point = np.array(data3).reshape(-1, 1)\n",
    "data4 = scalers[output_term[1]].inverse_transform(data_point).flatten()\n",
    "\n",
    "print(data1)\n",
    "print(data2)\n",
    "print(data3)\n",
    "print(data4)\n",
    "d_yuansu = (data3-data1)/(data4-data2)\n",
    "print('每0.01浓度的输出差：',(data3-data1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isShuffle = True\n",
    "isShuffle = False\n",
    "time_steps = 2\n",
    "\n",
    "\n",
    "test_size = 0.30\n",
    "val_size = 0.001\n",
    "train_size = 1-val_size-test_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 组合训练数据--拆分训练、测试集\n",
    "# 定义时间步数和特征数\n",
    "\n",
    "# 构成    \n",
    "# X = [X(t),X(t-1),Y(t-1)]\n",
    "# Y = [Y(t)]\n",
    "def make_data(u1_data,u2_data,u3_data,u4_data,u5_data,u6_data,u7_data,y1_data,y2_data,index_fanwei):\n",
    "    X = np.column_stack((u1_data,u2_data,u3_data,u4_data,u5_data,u6_data,u7_data))\n",
    "    y = np.column_stack((y1_data, y2_data))\n",
    "\n",
    "    X_modified = []\n",
    "    y_modified = []\n",
    "    \n",
    "    for i in range(8,len(y1_data)):\n",
    "        if i in index_fanwei:\n",
    "            # print(i)\n",
    "            # print(df_sheet_yuansu[time_term][i])\n",
    "            yuansu_time = df_sheet_yuansu[time_term][i]\n",
    "            closest_10 = df_sheet_params[df_sheet_params[time_term] <= yuansu_time].nlargest(time_steps, time_term)\n",
    "            # print(closest_10)\n",
    "            \n",
    "            index = closest_10.index\n",
    "            # print(index)\n",
    "            # print(closest_10.iloc[-1][time_term])\n",
    "            if closest_10.iloc[-1][time_term] < yuansu_time - time_steps + 1:\n",
    "                print(i,yuansu_time,'errloss')\n",
    "            else:\n",
    "\n",
    "                # print(X[index, :])\n",
    "                new_x_sample = np.concatenate([X[i, :] for i in index],axis=0)\n",
    "                # print(new_x_sample)\n",
    "\n",
    "                \n",
    "                y_last = y[i-1, :]\n",
    "                # print(y_last, 'y_last time : ',df_sheet_yuansu[time_term][i-1])\n",
    "                new_x_sample = np.concatenate([new_x_sample,y_last],axis=0)                \n",
    "                # print(new_x_sample)\n",
    "                \n",
    "                # y_last = y[i-2, :]\n",
    "                # # print(y_last, 'y_last time : ',df_sheet_yuansu[time_term][i-1])\n",
    "                # new_x_sample = np.concatenate([new_x_sample,y_last],axis=0)                \n",
    "                # # print(new_x_sample)\n",
    "                \n",
    "                # y_last = y[i-3, :]\n",
    "                # # print(y_last, 'y_last time : ',df_sheet_yuansu[time_term][i-1])\n",
    "                # new_x_sample = np.concatenate([new_x_sample,y_last],axis=0)                \n",
    "                # # print(new_x_sample)\n",
    "                \n",
    "                # y_last = y[i-4, :]\n",
    "                # # print(y_last, 'y_last time : ',df_sheet_yuansu[time_term][i-1])\n",
    "                # new_x_sample = np.concatenate([new_x_sample,y_last],axis=0)                \n",
    "                # # print(new_x_sample)\n",
    "\n",
    "\n",
    "                y_sample = y[i, :]  \n",
    "                X_modified.append(new_x_sample)\n",
    "                y_modified.append(y_sample)\n",
    "                print(i,yuansu_time,index[0],index[-1], end='\\r')\n",
    "                # break\n",
    "\n",
    "    # 将列表转换为 NumPy 数组\n",
    "    X_modified = np.array(X_modified)\n",
    "    y_modified = np.array(y_modified)\n",
    "    X_reshaped = X_modified.reshape((X_modified.shape[0], X_modified.shape[1]))\n",
    "\n",
    "    # 打印新数据的形状\n",
    "    print(\"Modified Input Shape:\", X_reshaped.shape)\n",
    "    print(\"Modified Output Shape:\", y_modified.shape)\n",
    "\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_reshaped, y_modified, \n",
    "                                                        test_size=test_size, \n",
    "                                                        random_state=42, \n",
    "                                                        shuffle=isShuffle)\n",
    "\n",
    "    # 将剩余的70%训练数据再次拆分成训练数据和验证数据（20%验证数据，50%训练数据）\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, \n",
    "                                                        test_size=val_size/(train_size+val_size), \n",
    "                                                        random_state=42, \n",
    "                                                        shuffle=isShuffle)\n",
    "\n",
    "    print('训练数量：',X_train.shape,y_train.shape)\n",
    "    print('验证数量：',X_val.shape,y_val.shape)\n",
    "    print('测试数量：',X_test.shape,y_test.shape)\n",
    "\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 高炉模型列数据\n",
    "u1_data = normalized_data_dict[input_term[0]]\n",
    "u2_data = normalized_data_dict[input_term[1]]\n",
    "u3_data = normalized_data_dict[input_term[2]]\n",
    "u4_data = normalized_data_dict[input_term[3]]\n",
    "u5_data = normalized_data_dict[input_term[4]]\n",
    "u6_data = normalized_data_dict[input_term[5]]\n",
    "u7_data = normalized_data_dict[input_term[6]]\n",
    "y1_data = normalized_data_dict[output_term[0]]\n",
    "y2_data = normalized_data_dict[output_term[1]]\n",
    "num_samples = y2_data.shape[0]\n",
    "print('高炉模型数据')\n",
    "X_gaolu_train, X_gaolu_val, X_gaolu_test,\\\n",
    "y_gaolu_train, y_gaolu_val, y_gaolu_test = make_data(u1_data,u2_data,u3_data,u4_data,u5_data,u6_data,u7_data,\n",
    "                                                            y1_data,y2_data,\n",
    "                                                            index_fanwei=index_gaolu)\n",
    "\n",
    "\n",
    "\n",
    "# 预测模型列数据\n",
    "u1_data = normalized_data_dict[input_term[0]]\n",
    "u2_data = normalized_data_dict[input_term[1]]\n",
    "u3_data = normalized_data_dict[input_term[2]]\n",
    "u4_data = normalized_data_dict[input_term[3]]\n",
    "u5_data = normalized_data_dict[input_term[4]]\n",
    "u6_data = normalized_data_dict[input_term[5]]\n",
    "u7_data = normalized_data_dict[input_term[6]]\n",
    "y1_data = normalized_data_dict[output_term[0]]\n",
    "y2_data = normalized_data_dict[output_term[1]]\n",
    "num_samples = y2_data.shape[0]\n",
    "print('预测模型数据')\n",
    "X_predict_train, X_predict_val, X_predict_test,\\\n",
    "y_predict_train, y_predict_val, y_predict_test = make_data(u1_data,u2_data,u3_data,u4_data,u5_data,u6_data,u7_data,\n",
    "                                                            y1_data,y2_data,\n",
    "                                                            index_fanwei=index_predict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取模型参数\n",
    "def get_params(W_b):\n",
    "\n",
    "    mid_indix = W_b.shape[0]//2-1\n",
    "    pred_0 = W_b[-2:-1]\n",
    "    W0 = W_b[:mid_indix]\n",
    "    b1 = W_b[-1:]\n",
    "    W1 = W_b[mid_indix:mid_indix*2]\n",
    "    W_b_0 = np.concatenate((W0, pred_0))\n",
    "    W_b_1 = np.concatenate((W1, b1))\n",
    "    # print(mid_indix)\n",
    "\n",
    "    # print('pred_0:',pred_0.shape)\n",
    "    # print('W0:',W0.shape)\n",
    "    # print('b1:',b1.shape)\n",
    "    # print('W1:',W1.shape)\n",
    "    return pred_0,W0,b1,W1,W_b_0,W_b_1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义My_LS_SVRModel\n",
    "class My_M_LS_SVRModel:\n",
    "    def __init__(self, params):\n",
    "        C0, C1, C00, gamma= params\n",
    "        self.C0 = C0\n",
    "        self.C1 = C1\n",
    "        self.C00 = C00\n",
    "        self.gamma = gamma\n",
    "        self.W_b = None\n",
    "        self.X_train = None\n",
    "\n",
    "    def model_train(self, X_train, y_train, K_train):\n",
    "        def objective(W_b, X, y):\n",
    "            b0,W0,b1,W1,W_b_0,W_b_1 = get_params(W_b)\n",
    "\n",
    "            y_pred_0 = np.dot(K_train, W0) + b0\n",
    "            y_pred_1 = np.dot(K_train, W1) + b1\n",
    "            errors0 = y[:,0] - y_pred_0\n",
    "            errors1 = y[:,1] - y_pred_1\n",
    "\n",
    "            # 损失函数\n",
    "            loss = ( 0.5 * (np.dot(W0, W0)+np.dot(W1, W1)) \n",
    "                    + self.C0 * np.sum(errors0**2) + self.C1 * np.sum(errors1**2)\n",
    "                    + self.C00 * np.sum(errors0**2 + errors1**2) # L2 范数的平方\n",
    "                    )\n",
    "            return loss\n",
    "\n",
    "        # 初始化权重向量+偏移项b\n",
    "        initial_W_b = np.zeros((X_train.shape[0])*2+(1)*2)\n",
    "\n",
    "        # 使用minimize 函数最小化目标函数\n",
    "        result = minimize(objective, initial_W_b, args=(X_train, y_train),\n",
    "                            method='L-BFGS-B')\n",
    "        # 输出最优的权重向量\n",
    "        best_W_b = result.x\n",
    "        return best_W_b\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        self.X_train = X_train\n",
    "        K_train = rbf_kernel(X_train, X_train, gamma=self.gamma)\n",
    "        self.W_b = self.model_train(X_train, y_train, K_train)\n",
    "\n",
    "    def my_predict(self, X_test):\n",
    "        b0,W0,b1,W1,W_b_0,W_b_1 = get_params(self.W_b)\n",
    "        K_test = rbf_kernel(X_test, self.X_train, gamma=self.gamma)\n",
    "        y_pred_0 = np.dot(K_test, W0) + b0\n",
    "        y_pred_1 = np.dot(K_test, W1) + b1\n",
    "        return y_pred_0, y_pred_1\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "# # 示例用法\n",
    "# params = [8.78525340e+01, 2.01347249e-03]\n",
    "# # 创建模型\n",
    "# my_svr_model = My_LS_SVRModel(params=params)\n",
    "# # 训练模型\n",
    "# my_svr_model.fit(X_train, y_train)\n",
    "# # 模型预测\n",
    "# y_pred = my_svr_model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建模\n",
    "params =   [1 , 1, 1 ,0.1] \n",
    "svr_model_gaolu = My_M_LS_SVRModel(params=params)\n",
    "# 训练模型\n",
    "svr_model_gaolu.fit(X_gaolu_train, y_gaolu_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 高炉模型预测\n",
    "y_train_pred_0,y_train_pred_1 = svr_model_gaolu.my_predict(X_gaolu_train)\n",
    "y_test_pred_0,y_test_pred_1 = svr_model_gaolu.my_predict(X_gaolu_test)\n",
    "\n",
    "base.double_control_train_test_result(scalers,  output_term,\n",
    "                                        y_gaolu_train,  y_train_pred_0, y_train_pred_1,\n",
    "                                        y_gaolu_test,   y_test_pred_0,  y_test_pred_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建模\n",
    "params =   [1 , 1, 1 ,0.1] \n",
    "svr_model_predict = My_M_LS_SVRModel(params=params)\n",
    "# 训练模型\n",
    "svr_model_predict.fit(X_predict_train, y_predict_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预测建模效果\n",
    "y_train_pred_0,y_train_pred_1 = svr_model_predict.my_predict(X_predict_train)\n",
    "y_test_pred_0,y_test_pred_1 = svr_model_predict.my_predict(X_predict_test)\n",
    "\n",
    "base.double_control_train_test_result(scalers,  output_term,\n",
    "                                        y_predict_train,  y_train_pred_0, y_train_pred_1,\n",
    "                                        y_predict_test,   y_test_pred_0,  y_test_pred_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base.gaolu_predict_raw(scalers,output_term,svr_model_predict,svr_model_gaolu,X_predict_test,y_predict_test)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39_tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
