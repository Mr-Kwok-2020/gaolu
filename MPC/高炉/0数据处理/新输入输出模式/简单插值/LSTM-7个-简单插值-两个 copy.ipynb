{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 库文件\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from skopt import gp_minimize\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.font_manager import FontProperties\n",
    "# 设置中文字体\n",
    "font = FontProperties(fname=r\"c:\\windows\\fonts\\simsun.ttc\", size=12)  # 替换为你的中文字体文件路径\n",
    "import sys\n",
    "sys.path.append(r\"C:\\Users\\haokw\\Documents\\GitHub\\gaolu\\MPC\\高炉\")\n",
    "from base import    double_control_predict_result,\\\n",
    "                    gaolu_predict_raw,\\\n",
    "                    data_tranform_plot,\\\n",
    "                    get_y_aim_data,\\\n",
    "                    generate_yr,\\\n",
    "                    replace_outliers_with_weighted_diff\n",
    "\n",
    "from base import MyRNNModel,CustomPredictor_double\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取Excel文件\n",
    "excel_path = f'C:\\\\Users\\\\haokw\\\\Documents\\\\GitHub\\\\gaolu\\\\MPC\\\\高炉\\\\0数据处理\\\\新输入输出模式\\\\简单插值数据-时间戳.xlsx'\n",
    "df_sheet = pd.read_excel(excel_path, sheet_name='Sheet2') \n",
    "print(df_sheet.info())\n",
    "print(df_sheet.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义输入输出参数\n",
    "input_term = ['富氧流量', '冷风流量', '热风压力', '热风温度', '鼓风湿度', '设定喷煤量', '上小时喷煤量']\n",
    "output_term = ['铁口1温度', 'SI']\n",
    "time_term= '时间戳'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 异常数据处理-处理前后对比\n",
    "# 创建数据框副本以避免修改原始数据\n",
    "df_sheet_process = df_sheet.copy()\n",
    "# 定义一个函数，用前后两个值的差值按照距离进行加权替换异常值\n",
    "def replace_outliers_with_weighted_diff(x, y):\n",
    "    # 计算列的中位数\n",
    "    median_value = y.median()\n",
    "    # 检测异常值的索引\n",
    "    outliers_index = (y - median_value).abs() > 1.5 * y.std()  # 使用标准差作为阈值\n",
    "    \n",
    "    # 遍历异常值的索引\n",
    "    for idx in outliers_index[outliers_index].index:\n",
    "        # 获取异常值前一个和后一个值的索引\n",
    "        prev_idx = idx - 1 if idx - 1 >= 0 else idx\n",
    "        next_idx = idx + 1 if idx + 1 < len(y) else idx\n",
    "        # 计算当前 x 与前后两个 x 的距离\n",
    "        dist_prev = abs(x[idx] - x[prev_idx])\n",
    "        dist_next = abs(x[next_idx] - x[idx])\n",
    "        total_dist = dist_prev + dist_next\n",
    "        # 计算权重\n",
    "        weight_prev = dist_next / total_dist\n",
    "        weight_next = dist_prev / total_dist\n",
    "        # 计算前后两个值的差值\n",
    "        diff = y[next_idx] - y[prev_idx]\n",
    "        # 根据权重进行插值\n",
    "        interpolated_value = y[prev_idx] + weight_prev * diff\n",
    "        # 用插值结果替代异常值\n",
    "        y[idx] = interpolated_value\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 画出数据\n",
    "def plot_subplot(data_x,data_y_yuan,data_y,column):\n",
    "    plt.plot(data_x,data_y_yuan,'r-')\n",
    "    plt.plot(data_x,data_y,'m-')\n",
    "    # plt.xlabel(time_term, fontproperties=font)  # 使用中文标签\n",
    "    plt.ylabel(column, fontproperties=font)  # 使用中文标签\n",
    "    # 使用中文标签\n",
    "\n",
    "\n",
    "# 对指定列应用替代异常值的函数\n",
    "# 对指定列应用替代异常值的函数\n",
    "replace_outliers_with_weighted_diff(df_sheet_process['时间戳'], df_sheet_process[input_term[0]])\n",
    "replace_outliers_with_weighted_diff(df_sheet_process['时间戳'], df_sheet_process[input_term[1]])\n",
    "replace_outliers_with_weighted_diff(df_sheet_process['时间戳'], df_sheet_process[input_term[2]])\n",
    "replace_outliers_with_weighted_diff(df_sheet_process['时间戳'], df_sheet_process[input_term[4]])\n",
    "replace_outliers_with_weighted_diff(df_sheet_process['时间戳'], df_sheet_process[input_term[5]])\n",
    "replace_outliers_with_weighted_diff(df_sheet_process['时间戳'], df_sheet_process[input_term[6]])\n",
    "replace_outliers_with_weighted_diff(df_sheet_process['时间戳'], df_sheet_process[output_term[0]])\n",
    "replace_outliers_with_weighted_diff(df_sheet_process['时间戳'], df_sheet_process[output_term[1]])\n",
    "\n",
    "plt.figure(figsize=(15, 6))\n",
    "for idx, column in enumerate(input_term+output_term):\n",
    "    \n",
    "    plt.subplot(len(input_term+output_term), 1, idx+1)\n",
    "    plot_subplot(df_sheet_process[time_term].values,df_sheet[column].values,df_sheet_process[column].values,column)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 画出选取的数据\n",
    "def plot_subplot(data_x,data_y,column,index_predict,index_gaolu):\n",
    "    plt.plot(data_x,data_y,'-', label='origin_data')\n",
    "    plt.plot(data_x[index_gaolu],data_y[index_gaolu],'r*-', label='gaolu_data')\n",
    "    plt.plot(data_x[index_predict],data_y[index_predict],'g-', label='predict_data')\n",
    "    plt.legend()\n",
    "    # plt.xlabel(time_term, fontproperties=font)  # 使用中文标签\n",
    "    plt.ylabel(column, fontproperties=font)  # 使用中文标签\n",
    "\n",
    "\n",
    "\n",
    "length1 = 400\n",
    "start1 = 0\n",
    "length2 = 400\n",
    "start2 = 400\n",
    "\n",
    "\n",
    "index_gaolu   = range(start1, start1+length1+1, 1)\n",
    "index_predict     = range(start2, start2+length2+1, 1)\n",
    "# index = range(1, 7572, 1)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15, 4))\n",
    "for idx, column in enumerate(input_term+output_term):\n",
    "    plt.subplot(len(input_term+output_term), 1, idx+1)\n",
    "    plot_subplot(df_sheet_process[time_term].values,df_sheet_process[column].values,column,index_predict,index_gaolu)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据归一化、逆归一化\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# 将数据存储为字典，每个键对应一列数据\n",
    "original_data_dict = {\n",
    "    input_term[0]:   df_sheet_process[input_term[0]].values,\n",
    "    input_term[1]:   df_sheet_process[input_term[1]].values,\n",
    "    input_term[2]:   df_sheet_process[input_term[2]].values,\n",
    "    input_term[3]:   df_sheet_process[input_term[3]].values,\n",
    "    input_term[4]:   df_sheet_process[input_term[4]].values,\n",
    "    input_term[5]:   df_sheet_process[input_term[5]].values,\n",
    "    input_term[6]:   df_sheet_process[input_term[6]].values,\n",
    "    output_term[0]:  df_sheet_process[output_term[0]].values,\n",
    "    output_term[1]:  df_sheet_process[output_term[1]].values\n",
    "}\n",
    "\n",
    "# 初始化缩放器\n",
    "scalers = {}\n",
    "\n",
    "# 进行拟合\n",
    "for column, data in original_data_dict.items():\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    scaler.fit(data.reshape(-1, 1))  # 保证数据是列向量\n",
    "    scalers[column] = scaler\n",
    "\n",
    "# 进行归一化\n",
    "normalized_data_dict = {}\n",
    "for column, scaler in scalers.items():\n",
    "    normalized_data_dict[column] = scaler.transform(original_data_dict[column].reshape(-1, 1)).flatten()\n",
    "\n",
    "# 进行反归一化\n",
    "original_data_dict = {}\n",
    "for column, scaler in scalers.items():\n",
    "    original_data_dict[column] = scaler.inverse_transform(normalized_data_dict[column].reshape(-1, 1)).flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 标定归一化前后数据\n",
    "data_point = np.array([1500]).reshape(-1, 1)\n",
    "data1 = scalers[output_term[0]].transform(data_point).flatten()\n",
    "\n",
    "data_point = np.array(data1).reshape(-1, 1)\n",
    "data2 = scalers[output_term[0]].inverse_transform(data_point).flatten()\n",
    "\n",
    "data_point = np.array([1510]).reshape(-1, 1)\n",
    "data3 = scalers[output_term[0]].transform(data_point).flatten()\n",
    "\n",
    "data_point = np.array(data3).reshape(-1, 1)\n",
    "data4 = scalers[output_term[0]].inverse_transform(data_point).flatten()\n",
    "\n",
    "print(data1)\n",
    "print(data2)\n",
    "print(data3)\n",
    "print(data4)\n",
    "d_temp = (data3-data1)/(data4-data2)\n",
    "print('每摄氏度的输出差：',d_temp)\n",
    "\n",
    "\n",
    "\n",
    "data_point = np.array([0.50]).reshape(-1, 1)\n",
    "data1 = scalers[output_term[1]].transform(data_point).flatten()\n",
    "\n",
    "data_point = np.array(data1).reshape(-1, 1)\n",
    "data2 = scalers[output_term[1]].inverse_transform(data_point).flatten()\n",
    "\n",
    "data_point = np.array([0.60]).reshape(-1, 1)\n",
    "data3 = scalers[output_term[1]].transform(data_point).flatten()\n",
    "\n",
    "data_point = np.array(data3).reshape(-1, 1)\n",
    "data4 = scalers[output_term[1]].inverse_transform(data_point).flatten()\n",
    "\n",
    "print(data1)\n",
    "print(data2)\n",
    "print(data3)\n",
    "print(data4)\n",
    "d_yuansu = (data3-data1)/(data4-data2)\n",
    "print('每0.01浓度的输出差：',(data3-data1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isShuffle = True\n",
    "# isShuffle = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 组合训练数据--拆分训练、测试集\n",
    "train_size = 0.7\n",
    "val_size = 0.15\n",
    "test_size = 0.15\n",
    "# 构成    \n",
    "# X = [X(t),X(t-1),Y(t-1)]\n",
    "# Y = [Y(t)]\n",
    "def make_data(u1_data,u2_data,u3_data,u4_data,u5_data,u6_data,u7_data,y1_data,y2_data):\n",
    "    u1_data = u1_data\n",
    "    u2_data = u2_data\n",
    "    u3_data = u3_data\n",
    "    u4_data = u4_data\n",
    "    u5_data = u5_data\n",
    "    u6_data = u6_data\n",
    "    u7_data = u7_data\n",
    "\n",
    "    y1_data = y1_data\n",
    "    y2_data = y2_data\n",
    "\n",
    "    X = np.column_stack((   u1_data  [1:-1], u2_data  [1:-1], u3_data  [1:-1], u4_data  [1:-1], u5_data  [1:-1], u6_data  [1:-1], u7_data  [1:-1],\n",
    "                            u1_data  [0:-2], u2_data  [0:-2], u3_data  [0:-2], u4_data  [0:-2], u5_data  [0:-2], u6_data  [0:-2], u7_data  [0:-2],\n",
    "                            y1_data  [0:-2], y2_data  [0:-2])\n",
    "                        )\n",
    "    y = np.column_stack((y1_data[1:-1],y2_data[1:-1]))\n",
    "\n",
    "    # 使用 reshape 转换形状(457, 10) (457, 2)--->(457, 1, 10) (457, 1, 2)\n",
    "    X_reshaped = X.reshape((X.shape[0], 1, X.shape[1]))\n",
    "    y_reshaped = y.reshape((y.shape[0], 1, y.shape[1]))\n",
    "    print('总数据数量：',y.shape)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_reshaped, y_reshaped, \n",
    "                                                        test_size=test_size, \n",
    "                                                        random_state=42, \n",
    "                                                        shuffle=isShuffle)\n",
    "\n",
    "    # 将剩余的70%训练数据再次拆分成训练数据和验证数据（20%验证数据，50%训练数据）\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, \n",
    "                                                        test_size=val_size/(train_size+val_size), \n",
    "                                                        random_state=42, \n",
    "                                                        shuffle=isShuffle)\n",
    "\n",
    "    y_train = y_train.reshape((y_train.shape[0],y_train.shape[2]))\n",
    "    y_val = y_val.reshape((y_val.shape[0],y_val.shape[2]))\n",
    "    y_test = y_test.reshape((y_test.shape[0],y_test.shape[2]))\n",
    "    print('训练数量：',X_train.shape,y_train.shape)\n",
    "    print('验证数量：',X_val.shape,y_val.shape)\n",
    "    print('测试数量：',X_test.shape,y_test.shape)\n",
    "\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 高炉模型列数据\n",
    "u1_data = normalized_data_dict[input_term[0]][index_gaolu]\n",
    "u2_data = normalized_data_dict[input_term[1]][index_gaolu]\n",
    "u3_data = normalized_data_dict[input_term[2]][index_gaolu]\n",
    "u4_data = normalized_data_dict[input_term[3]][index_gaolu]\n",
    "u5_data = normalized_data_dict[input_term[4]][index_gaolu]\n",
    "u6_data = normalized_data_dict[input_term[5]][index_gaolu]\n",
    "u7_data = normalized_data_dict[input_term[6]][index_gaolu]\n",
    "y1_data = normalized_data_dict[output_term[0]][index_gaolu]\n",
    "y2_data = normalized_data_dict[output_term[1]][index_gaolu]\n",
    "num_samples = y1_data.shape[0]\n",
    "print('高炉模型数据')\n",
    "X_gaolu_train, X_gaolu_val, X_gaolu_test,\\\n",
    "y_gaolu_train, y_gaolu_val, y_gaolu_test = make_data(u1_data,u2_data,u3_data,u4_data,u5_data,u6_data,u7_data,y1_data,y2_data)\n",
    "\n",
    "# 预测模型列数据\n",
    "u1_data = normalized_data_dict[input_term[0]][index_predict]\n",
    "u2_data = normalized_data_dict[input_term[1]][index_predict]\n",
    "u3_data = normalized_data_dict[input_term[2]][index_predict]\n",
    "u4_data = normalized_data_dict[input_term[3]][index_predict]\n",
    "u5_data = normalized_data_dict[input_term[4]][index_predict]\n",
    "u6_data = normalized_data_dict[input_term[5]][index_predict]\n",
    "u7_data = normalized_data_dict[input_term[6]][index_predict]\n",
    "y1_data = normalized_data_dict[output_term[0]][index_predict]\n",
    "y2_data = normalized_data_dict[output_term[1]][index_predict]\n",
    "num_samples = y1_data.shape[0]\n",
    "print('预测模型数据')\n",
    "X_predict_train, X_predict_val, X_predict_test,\\\n",
    "y_predict_train, y_predict_val, y_predict_test = make_data(u1_data,u2_data,u3_data,u4_data,u5_data,u6_data,u7_data,y1_data,y2_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义LSTM模型\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class MyRNNModel(torch.nn.Module):\n",
    "    def __init__(self,features_size,hidden_size,isbidirectional):\n",
    "        super(MyRNNModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=features_size,\n",
    "            hidden_size=hidden_size,\n",
    "            batch_first=True,\n",
    "            bidirectional=isbidirectional\n",
    "        )\n",
    "        if isbidirectional:\n",
    "            self.fc = nn.Linear(2 * hidden_size, 2)\n",
    "        else:\n",
    "            self.fc = nn.Linear(hidden_size, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        \n",
    "        last_lstm_output = lstm_out[:, -1, :]\n",
    "        # print(last_lstm_output)\n",
    "        output = self.fc(last_lstm_output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "    def custom_loss(self, y_true, y_pred):\n",
    "        squared_diff = torch.pow(y_true - y_pred, 2)\n",
    "        sum_squared_diff = torch.sum(squared_diff)\n",
    "        mse = sum_squared_diff / len(y_true)\n",
    "        return mse\n",
    "\n",
    "\n",
    "\n",
    "    def my_fit(self, \n",
    "                X_train, y_train, \n",
    "                X_val, y_val, \n",
    "                train_loss_list,val_loss_list,\n",
    "                epochs=1, batch_size=32, lr=0.001):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            epoch_loss = 0\n",
    "            for i in range(0, len(X_train), batch_size):\n",
    "                x_batch = torch.tensor(X_train[i:i+batch_size], dtype=torch.float32)\n",
    "                y_batch = torch.tensor(y_train[i:i+batch_size], dtype=torch.float32)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                y_pred = self(x_batch)\n",
    "                loss = self.custom_loss(y_batch, y_pred)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "            average_epoch_train_loss = epoch_loss / (len(X_train) / batch_size)\n",
    "            # 验证集评估\n",
    "            self.eval()\n",
    "            with torch.no_grad():\n",
    "                val_loss = 0\n",
    "                for i in range(0, len(X_val), batch_size):\n",
    "                    x_batch_val = torch.tensor(X_val[i:i+batch_size], dtype=torch.float32)\n",
    "                    y_batch_val = torch.tensor(y_val[i:i+batch_size], dtype=torch.float32)\n",
    "\n",
    "                    y_pred_val = self(x_batch_val)\n",
    "                    val_loss += self.custom_loss(y_batch_val, y_pred_val).item()\n",
    "\n",
    "                average_epoch_val_loss = val_loss / (len(X_val) / batch_size)\n",
    "\n",
    "            print(f'第 {epoch + 1}/{epochs} 轮, 训练误差: {average_epoch_train_loss:.4f}, 验证误差: {average_epoch_val_loss:.4f}', end='\\r')\n",
    "            train_loss_list.append(average_epoch_train_loss)\n",
    "            val_loss_list.append(average_epoch_val_loss)\n",
    "\n",
    "        return train_loss_list,val_loss_list\n",
    "    \n",
    "    \n",
    "\n",
    "    def my_predict(self, X_test):\n",
    "        # 设置模型为评估模式，这会关闭 dropout 等层\n",
    "        self.eval()\n",
    "        # 将输入数据转换为张量，并设置 requires_grad=True\n",
    "        x_tensor = torch.tensor(X_test, dtype=torch.float32, requires_grad=True)\n",
    "        \n",
    "        # 获取模型的预测输出\n",
    "        y_pred = self(x_tensor)\n",
    "        # 保留预测值的梯度信息\n",
    "        y_pred.retain_grad()\n",
    "        # 返回预测结果和包含梯度信息的张量\n",
    "        return y_pred[:,0].detach().numpy(),y_pred[:,1].detach().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_once_time = 25\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立高炉模型实例\n",
    "features_size = 16\n",
    "hidden_size = 16\n",
    "# 设置随机种子\n",
    "# torch.manual_seed(0)\n",
    "model_gaolu = MyRNNModel(features_size = features_size, \n",
    "                        hidden_size = hidden_size,\n",
    "                        isbidirectional=True)\n",
    "epoch_sum_gaolu = 0\n",
    "gaolu_train_loss_list = []\n",
    "gaolu_val_loss_list = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 高炉模型训练\n",
    "epoch_once = epoch_once_time\n",
    "epoch_sum_gaolu = epoch_sum_gaolu + epoch_once\n",
    "gaolu_train_loss_list,gaolu_val_loss_list = model_gaolu.my_fit(X_gaolu_train, y_gaolu_train,\n",
    "                                    X_gaolu_val, y_gaolu_val, \n",
    "                                    gaolu_train_loss_list, gaolu_val_loss_list,\n",
    "                                    epochs=epoch_once, \n",
    "                                    batch_size=32,\n",
    "                                    lr = 0.002)\n",
    "\n",
    "print('\\nepoch_sum:',epoch_sum_gaolu)\n",
    "\n",
    "# 绘制训练和验证损失曲线\n",
    "plt.plot(gaolu_train_loss_list, label='Train Loss')\n",
    "plt.plot(gaolu_val_loss_list, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 高炉模型预测\n",
    "y_pred_0,y_pred_1  = model_gaolu.my_predict(X_gaolu_test)\n",
    "# 计算 RMSE、MRE\n",
    "y_test = y_gaolu_test\n",
    "# y_test = y_test[:70]\n",
    "# y_pred_0 = y_pred_0[:70]\n",
    "# y_pred_1 = y_pred_1[:70]\n",
    "\n",
    "double_control_predict_result(scalers,output_term,y_test,y_pred_0,y_pred_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建预测模型实例\n",
    "features_size = 16\n",
    "hidden_size_predict = 16\n",
    "# 设置随机种子\n",
    "torch.manual_seed(0)\n",
    "model = MyRNNModel(features_size = features_size, \n",
    "                    hidden_size = hidden_size_predict,\n",
    "                    isbidirectional=True)\n",
    "epoch_sum_predict = 0\n",
    "predict_train_loss_list = []\n",
    "predict_val_loss_list = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预测模型训练\n",
    "epoch_once = epoch_once_time\n",
    "epoch_sum = epoch_sum_predict + epoch_once\n",
    "predict_train_loss_list, predict_val_loss_list = model.my_fit(X_predict_train, y_predict_train,\n",
    "                                    X_predict_val, y_predict_val, \n",
    "                                    predict_train_loss_list, predict_val_loss_list,\n",
    "                                    epochs=epoch_once, \n",
    "                                    batch_size=64,\n",
    "                                    lr = 0.002)\n",
    "\n",
    "print('\\nepoch_sum:',epoch_sum_predict)\n",
    "\n",
    "\n",
    "# 绘制训练和验证损失曲线\n",
    "plt.plot(predict_train_loss_list, label='Train Loss')\n",
    "plt.plot(predict_val_loss_list, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预测模型预测\n",
    "y_pred_0,y_pred_1  = model.my_predict(X_predict_test)\n",
    "\n",
    "# 计算 RMSE、MRE\n",
    "y_test = y_predict_test\n",
    "\n",
    "double_control_predict_result(scalers,output_term,y_test,y_pred_0,y_pred_1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 高炉、预测、原始对比\n",
    "gaolu_predict_raw(scalers,output_term,model,model_gaolu,X_predict_test,y_predict_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iscontrol = True\n",
    "# iscontrol = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自定义预测\n",
    "model_predict = model\n",
    "hidden_size = hidden_size_predict\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def lstm_forward(input, initial_states, w_ih, w_hh, b_ih, b_hh):\n",
    "    h_0, c_0 = initial_states  # 初始状态  [b_size, hidden_size]\n",
    "    b_size, seq_len, input_size = input.shape\n",
    "    h_size = h_0.shape[-1]\n",
    "\n",
    "    h_prev, c_prev = h_0, c_0\n",
    "\n",
    "    # 使用 np.newaxis 在第一个维度上插入一个新的维度  # 使用 np.tile 在第一个维度上复制 b_size 次\n",
    "    w_ih_expanded = w_ih[np.newaxis, :, :]    \n",
    "    w_ih_batch = np.tile(w_ih_expanded, (b_size, 1, 1))\n",
    "    w_hh_expanded = w_hh[np.newaxis, :, :]    \n",
    "    w_hh_batch = np.tile(w_hh_expanded, (b_size, 1, 1))\n",
    "    # print(w_ih_batch.shape)\n",
    "\n",
    "    output_size = h_size\n",
    "    output = np.zeros((b_size, seq_len, output_size))  # 初始化一个输出序列\n",
    "    for t in range(seq_len):\n",
    "        x = input[:, t, :]  # 当前时刻的输入向量 [b,in_size]->[b,in_size,1]\n",
    "        w_times_x = np.matmul(w_ih_batch, x[:, :, np.newaxis]).squeeze(-1)   # bmm:含有批量大小的矩阵相乘\n",
    "        # [b, 4*hidden_size, 1]->[b, 4*hidden_size]\n",
    "        # 这一步就是计算了 Wii*xt|Wif*xt|Wig*xt|Wio*xt\n",
    "        w_times_h_prev = np.matmul(w_hh_batch, h_prev[:, :, np.newaxis]).squeeze(-1)\n",
    "        # [b, 4*hidden_size, hidden_size]*[b, hidden_size, 1]->[b,4*hidden_size, 1]->[b, 4*hidden_size]\n",
    "        # 这一步就是计算了 Whi*ht-1|Whf*ht-1|Whg*ht-1|Who*ht-1\n",
    "\n",
    "        # 分别计算输入门(i)、遗忘门(f)、cell门(g)、输出门(o)  维度均为 [b, h_size]\n",
    "        i_t = sigmoid(w_times_x[:, :h_size] + w_times_h_prev[:, :h_size] + b_ih[:h_size] + b_hh[:h_size])  # 取前四分之一\n",
    "        f_t = sigmoid(w_times_x[:, h_size:2*h_size] + w_times_h_prev[:, h_size:2*h_size]\n",
    "                            + b_ih[h_size:2*h_size] + b_hh[h_size:2*h_size])\n",
    "        g_t = np.tanh(w_times_x[:, 2*h_size:3*h_size] + w_times_h_prev[:, 2*h_size:3*h_size]\n",
    "                            + b_ih[2*h_size:3*h_size] + b_hh[2*h_size:3*h_size])\n",
    "        o_t = sigmoid(w_times_x[:, 3*h_size:] + w_times_h_prev[:, 3*h_size:]\n",
    "                            + b_ih[3*h_size:] + b_hh[3*h_size:])\n",
    "        c_prev = f_t * c_prev + i_t * g_t\n",
    "        h_prev = o_t * np.tanh(c_prev)\n",
    "\n",
    "        output[:, t, :] = h_prev\n",
    "\n",
    "    return output, (np.expand_dims(h_prev, axis=0), np.expand_dims(c_prev, axis=0))  # 官方是三维，在第0维扩一维\n",
    "\n",
    "\n",
    "\n",
    "def predict_my(data_input):\n",
    "\n",
    "    input = data_input  # 随机初始化一个输入序列\n",
    "    c_0 = np.zeros((data_input.shape[0], hidden_size))  # 初始值，不会参与训练\n",
    "    h_0 = np.zeros((data_input.shape[0], hidden_size))\n",
    "\n",
    "    output_forward, (h_n_me, c_n_me) = lstm_forward(input, (h_0, c_0), \n",
    "                                                model_predict.lstm.weight_ih_l0.detach().numpy(),\n",
    "                                                model_predict.lstm.weight_hh_l0.detach().numpy(), \n",
    "                                                model_predict.lstm.bias_ih_l0.detach().numpy(), \n",
    "                                                model_predict.lstm.bias_hh_l0.detach().numpy())\n",
    "\n",
    "    last_lstm_output_forward = output_forward[:, -1, :]\n",
    "\n",
    "    output_backward, (h_n_me, c_n_me) = lstm_forward(input, (h_0, c_0), \n",
    "                                                model_predict.lstm.weight_ih_l0_reverse.detach().numpy(),\n",
    "                                                model_predict.lstm.weight_hh_l0_reverse.detach().numpy(), \n",
    "                                                model_predict.lstm.bias_ih_l0_reverse.detach().numpy(), \n",
    "                                                model_predict.lstm.bias_hh_l0_reverse.detach().numpy())\n",
    "\n",
    "    last_lstm_output_backward = output_backward[:, -1, :]\n",
    "    # print(last_lstm_output_forward.shape)\n",
    "    # print(last_lstm_output_backward.shape)\n",
    "    # 最终输出\n",
    "    combined_hidden = np.concatenate((last_lstm_output_forward, last_lstm_output_backward), axis=1)\n",
    "    # print(combined_hidden.shape)\n",
    "\n",
    "\n",
    "    output = (np.dot(combined_hidden, np.transpose(model_predict.fc.weight.detach().numpy()))\n",
    "                + model_predict.fc.bias.detach().numpy()\n",
    ")\n",
    "    y_pred_0, y_pred_1= output[:,0],output[:,1]\n",
    "\n",
    "    # y_pred_0 = scalers[output_term[0]].inverse_transform(np.array(y_pred_0).reshape(-1, 1)).flatten()\n",
    "    # y_pred_1 = scalers[output_term[1]].inverse_transform(np.array(y_pred_1).reshape(-1, 1)).flatten()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return y_pred_0, y_pred_1\n",
    "\n",
    "\n",
    "y_pred_0, y_pred_1= predict_my(X_predict_test)\n",
    "\n",
    "\n",
    "# plot_hit_rate_curve(y_test, y_pred_0, y_pred_1)\n",
    "# plot_hit_rate_curve(y_test, y_pred_0.detach().numpy(), y_pred_1.detach().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成期望数据\n",
    "Times = 100\n",
    "\n",
    "def generate_y_aim_data(Times):\n",
    "\n",
    "    set_y1 = np.full(Times,1500)\n",
    "    set_y1[30:] = 1510\n",
    "    set_y1[60:] = 1505\n",
    "    set_y1[90:] = 1515\n",
    "    # set_y1[70:] = 1520\n",
    "    # set_y1[90:] = 1525\n",
    "\n",
    "    set_y2 = np.full(Times,0.44)\n",
    "    set_y2[15:] = 0.48\n",
    "    set_y2[45:] = 0.52\n",
    "    set_y2[75:] = 0.44\n",
    "    # 限制设定值在 -1 到 1 之间\n",
    "    # set_y1 = np.clip(set_y1, -1, 1)\n",
    "    # set_y2 = np.clip(set_y2, -1, 1)\n",
    "\n",
    "\n",
    "    set_y1_trans = scalers[output_term[0]].transform(set_y1.reshape(-1,1)).flatten()\n",
    "    set_y2_trans = scalers[output_term[1]].transform(set_y2.reshape(-1,1)).flatten()\n",
    "\n",
    "    return set_y1, set_y2, set_y1_trans, set_y2_trans\n",
    "\n",
    "# # 调用示例\n",
    "# set_y1, set_y2, set_y1_trans, set_y2_trans= generate_y_aim_data(Times)\n",
    "# plt.plot(set_y1_trans)\n",
    "# plt.plot(set_y2_trans)\n",
    "# plt.title('y_sp')\n",
    "# plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成参考轨迹\n",
    "def get_yr(aim_value,current_value,alpha,P):\n",
    "    # 生成设定信号\n",
    "    setpoint_signal = np.full(10, aim_value)\n",
    "    # 初始化参数\n",
    "    alpha = alpha\n",
    "    y_r = np.zeros(P)\n",
    "    y_r[0] = current_value\n",
    "    # 模拟一阶模型\n",
    "    for k in range(1,P):\n",
    "        y_r[k] = alpha * y_r[k-1] + (1 - alpha) * aim_value\n",
    "\n",
    "    # # 绘制结果\n",
    "    # plt.plot(setpoint_signal, label='Setpoint Signal')\n",
    "    # plt.plot(y_r,'o-', label='Output Signal (Tracked)')\n",
    "    # plt.legend()\n",
    "    # plt.xlabel('Time')\n",
    "    # plt.ylabel('Amplitude')\n",
    "    # plt.title('Tracking Setpoint Signal with One-Order Model')\n",
    "    # plt.show()\n",
    "    return y_r\n",
    "# 测试\n",
    "y_r = get_yr(1,-0.5,0.667,5+1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#生成控制时域的数据格式\n",
    "def generate_k_data(u1_data, u2_data, u3_data, u4_data, u5_data, u6_data, u7_data, y1_data,y2_data, num_samples, P):\n",
    "    nearest_index = np.abs(y1_data - (-0.5)).argmin()\n",
    "    # 生成随机索引值\n",
    "    #从原有数据的randint时刻开始往下进行控制\n",
    "    randint = np.random.randint(1, num_samples - 2 - P - 1)\n",
    "    randint = nearest_index  # 如果你希望使用固定的值而不是随机生成\n",
    "    # randint = 250  # 如果你希望使用固定的值而不是随机生成\n",
    "    print(randint)\n",
    "    # 提取数据并构成 k_data\n",
    "    # 第一次得到下面五个变量，固定好格式构成k_data\n",
    "    u1   = u1_data[randint  :randint+3  ]\n",
    "    u2   = u2_data[randint  :randint+3  ]\n",
    "    u3   = u3_data[randint  :randint+3  ]\n",
    "    u4   = u4_data[randint  :randint+3  ]\n",
    "    u5   = u2_data[randint  :randint+3  ]\n",
    "    u6   = u3_data[randint  :randint+3  ]\n",
    "    u7   = u4_data[randint  :randint+3  ]\n",
    "\n",
    "    y1   = y1_data[randint  :randint+3  ]\n",
    "    y2   = y2_data[randint  :randint+3  ]\n",
    "    k_data = np.concatenate((u1, u2, u3, u4, u5, u6, u7, y1, y2), axis=0)\n",
    "    print(k_data.shape)\n",
    "\n",
    "    k_data = np.zeros_like(k_data)\n",
    "    return k_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义单时刻的MPC问题优化\n",
    "def my_MPC(k_data,params,M,P,y1_aim,y2_aim,isprint):\n",
    "    h1 = 1.0\n",
    "    h2 = 1.0\n",
    "    lamda1 = 0.001\n",
    "    lamda2 = 0.001\n",
    "    lamda3 = 0.001\n",
    "    lamda4 = 0.001\n",
    "    y1_percent = 1.0\n",
    "    y2_percent = 1.0\n",
    "\n",
    "    # 从固定格式k_data里面读取信息\n",
    "    u1   = k_data[0:3]\n",
    "    u2   = k_data[3:6]\n",
    "    u3   = k_data[6:9]\n",
    "    u4   = k_data[9:12]\n",
    "    u5   = k_data[12:15]\n",
    "    u6   = k_data[15:18]\n",
    "    u7   = k_data[18:21]\n",
    "\n",
    "    y1   = k_data[21:24]\n",
    "    y2   = k_data[24:27]\n",
    "\n",
    "    \n",
    "    # 获取猜测值[h U1 U2]\n",
    "    # h, U1, U2  =params[0], params[1:M+1],params[M+1:]\n",
    "    U1, U2, U3, U4, U5, U6, U7  =params[0:M], params[M:2*M],params[2*M:3*M], params[3*M:4*M], params[4*M:5*M],params[5*M:6*M], params[6*M:7*M]\n",
    "    \n",
    "    # 整理数据见   MPC推到.escel\n",
    "    u1   = np.concatenate((u1,U1,U1[-1]*np.ones(P-M)))\n",
    "    u2   = np.concatenate((u2,U2,U2[-1]*np.ones(P-M)))\n",
    "    u3   = np.concatenate((u3,U3,U3[-1]*np.ones(P-M)))\n",
    "    u4   = np.concatenate((u4,U4,U4[-1]*np.ones(P-M)))\n",
    "    u5   = np.concatenate((u5,U5,U5[-1]*np.ones(P-M)))\n",
    "    u6   = np.concatenate((u6,U6,U6[-1]*np.ones(P-M)))\n",
    "    u7   = np.concatenate((u7,U7,U7[-1]*np.ones(P-M)))\n",
    "    y1   = np.concatenate((y1,np.zeros(P)))\n",
    "    y2   = np.concatenate((y2,np.zeros(P)))\n",
    "    if isprint:\n",
    "        print(u1.round(4))\n",
    "        print(u2.round(4))\n",
    "        print(u3.round(4))\n",
    "        print(u4.round(4))\n",
    "        print(u5.round(4))\n",
    "        print(u6.round(4))\n",
    "        print(u7.round(4))\n",
    "        print(y1.round(4))    \n",
    "        print(y2.round(4))\n",
    "        print('开始预测')\n",
    "\n",
    "    y1_k = y1[2]\n",
    "    y2_k = y2[2]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # 总共预测 P+1 次\n",
    "    # 对k时刻进行预测-----1次\n",
    "    for j in range(1):   # j = 0\n",
    "        x = np.column_stack((   u1[j+2],u2[j+2],u3[j+2],u4[j+2],u5[j+2],u6[j+2],u7[j+2],\n",
    "                                u1[j+1],u2[j+1],u3[j+1],u4[j+1],u5[j+1],u6[j+1],u7[j+1],\n",
    "                                y1[j+1],y2[j+1]))\n",
    "        x = x.reshape((x.shape[0], 1, x.shape[1]))\n",
    "        y1_m_k, y2_m_k = predict_my(x)\n",
    "        E1_k = y1_k - y1_m_k\n",
    "        E2_k = y2_k - y2_m_k\n",
    "        if isprint:\n",
    "            print(j,'mode = 0')\n",
    "            print(x.round(4))\n",
    "            print(y1_k.round(4),y2_k.round(4))\n",
    "            print(y1_m_k.round(4),y2_m_k.round(4))\n",
    "\n",
    "    # 对控制时刻进行预测-----M次\n",
    "    for j in range(1,M+1):  # j = 1,2\n",
    "        x = np.column_stack((   u1[j+2],u2[j+2],u3[j+2],u4[j+2],u5[j+2],u6[j+2],u7[j+2],\n",
    "                                u1[j+1],u2[j+1],u3[j+1],u4[j+1],u5[j+1],u6[j+1],u7[j+1],\n",
    "                                y1[j+1],y2[j+1]))\n",
    "        x = x.reshape((x.shape[0], 1, x.shape[1]))\n",
    "        y1_k_j, y2_k_j = predict_my(x)\n",
    "        y1[j+2] = y1_k_j.item()\n",
    "        y2[j+2] = y2_k_j.item()\n",
    "        if isprint:\n",
    "            print(j,'mode = 1')\n",
    "            print(x.round(4))\n",
    "            print(y1_k_j.round(4),y2_k_j.round(4))\n",
    "            print('更新后:')\n",
    "            print(u1.round(4))\n",
    "            print(u2.round(4))\n",
    "            print(u3.round(4))\n",
    "            print(u4.round(4))\n",
    "            print(u5.round(4))\n",
    "            print(u6.round(4))\n",
    "            print(u7.round(4))\n",
    "            print(y1.round(4))    \n",
    "            print(y2.round(4))\n",
    "\n",
    "    # 对控制时域外的部分进行预测-----P-M次\n",
    "    # 注意：这部分的信号是保持控制不变下进行\n",
    "    for j in range(M+1,P+1):  #j = 3,4\n",
    "        x = np.column_stack((   u1[j+2],u2[j+2],u3[j+2],u4[j+2],u5[j+2],u6[j+2],u7[j+2],\n",
    "                                u1[j+1],u2[j+1],u3[j+1],u4[j+1],u5[j+1],u6[j+1],u7[j+1],\n",
    "                                y1[j+1],y2[j+1]))\n",
    "        x = x.reshape((x.shape[0], 1, x.shape[1]))\n",
    "        y1_k_j, y2_k_j = predict_my(x)\n",
    "        y1[j+2] = y1_k_j.item()#将预测值作为下一步的输出值\n",
    "        y2[j+2] = y2_k_j.item()\n",
    "        if isprint:\n",
    "            print(j,'mode = 2')\n",
    "            print(x.round(4))\n",
    "            print(y1_k_j.round(4),y2_k_j.round(4))\n",
    "            print('更新后:')\n",
    "            print(u1.round(4))\n",
    "            print(u2.round(4))\n",
    "            print(u3.round(4))\n",
    "            print(u4.round(4))\n",
    "            print(u5.round(4))\n",
    "            print(u6.round(4))\n",
    "            print(u7.round(4))\n",
    "            print(y1.round(4))    \n",
    "            print(y2.round(4))\n",
    "\n",
    "\n",
    "\n",
    "    k_data2 = np.concatenate((u1[1:4],u2[1:4],u3[1:4],u4[1:4],u5[1:4],u6[1:4],u7[1:4],y1[1:4],y2[1:4]),axis=0)\n",
    "    if isprint:\n",
    "        print('更新k_data')\n",
    "        print(k_data2.round(4))\n",
    "\n",
    "\n",
    "    #获取参考轨迹\n",
    "    # 一定要对照好做差的序列\n",
    "    y1_r_aim  = get_yr(y1_aim,y1_k,0.1,P+1)\n",
    "    y1_r = y1_r_aim[1:] \n",
    "\n",
    "\n",
    "    y2_r_aim  = get_yr(y2_aim,y2_k,0.1,P+1)\n",
    "    y2_r = y2_r_aim[1:] \n",
    "\n",
    "    y1_M_k = y1[3:]\n",
    "    y2_M_k = y2[3:]\n",
    "    if isprint==1:\n",
    "        print('反馈补偿:')\n",
    "        print('y1_k',y1_k.round(4))  \n",
    "        print('y1_m_k',y1_m_k.round(4))    \n",
    "        print('h*E1_k',(h1*E1_k).round(4)) \n",
    "        print('y2_k',y2_k.round(4))  \n",
    "        print('y2_m_k',y2_m_k.round(4))   \n",
    "        print('h*E2_k',(h2*E2_k).round(4))\n",
    "\n",
    "        print('temp:')\n",
    "        print('y1_aim',y1_aim.round(4))\n",
    "        print('y1_r_aim',y1_r_aim.round(4))\n",
    "        print('y1_r',y1_r.round(4))\n",
    "        print('y1_M_k',y1_M_k.round(4))\n",
    "        print('y1_M_k+h1*E1_k',(y1_M_k+h1*E1_k).round(4))\n",
    "\n",
    "        print('Si_percent:')\n",
    "        print('y2_aim',y2_aim.round(4))\n",
    "        print('y2_r_aim',y2_r_aim.round(4))\n",
    "        print('y2_r',y2_r.round(4))\n",
    "        print('y2_M_k',y2_M_k.round(4))\n",
    "        print('y2_M_k+h2*E2_k',(y2_M_k+h2*E2_k).round(4))\n",
    "\n",
    "        print('u:')\n",
    "        print(u1[2:].round(4))\n",
    "        print(u2[2:].round(4))\n",
    "        print(u3[2:].round(4))\n",
    "        print(u4[2:].round(4))\n",
    "        \n",
    "    # 计算mse\n",
    "    # lamda1太大的话会导致y1_r和y1_M_k的误差加大*****************导致超调的原因\\与目标值之间存在间隙\n",
    "\n",
    "\n",
    "    # y1_err = y1_percent*np.sum((y1_r-(y1_M_k+h1*E1_k))**2) \n",
    "    # y2_err = y2_percent*np.sum((y2_r-(y2_M_k+h2*E2_k))**2) \n",
    "    # u1_power = lamda1*np.sum((np.diff(u1[2:]))**2)\n",
    "    # u2_power = lamda2*np.sum((np.diff(u2[2:]))**2)\n",
    "    # u3_power = lamda3*np.sum((np.diff(u3[2:]))**2)\n",
    "    # u4_power = lamda4*np.sum((np.diff(u4[2:]))**2)\n",
    "    # u5_power = lamda2*np.sum((np.diff(u5[2:]))**2)\n",
    "    # u6_power = lamda3*np.sum((np.diff(u6[2:]))**2)\n",
    "    # u7_power = lamda4*np.sum((np.diff(u7[2:]))**2)\n",
    "\n",
    "    y1_err = y1_percent*np.sum(np.fabs(y1_r-(y1_M_k+h1*E1_k))) \n",
    "    y2_err = y2_percent*np.sum(np.fabs(y2_r-(y2_M_k+h2*E2_k))) \n",
    "    u1_power = lamda1*np.sum((np.fabs(np.diff(u1))))\n",
    "    u2_power = lamda2*np.sum((np.fabs(np.diff(u2))))\n",
    "    u3_power = lamda3*np.sum((np.fabs(np.diff(u3))))\n",
    "    u4_power = lamda4*np.sum((np.fabs(np.diff(u4))))\n",
    "    u5_power = lamda2*np.sum((np.fabs(np.diff(u5))))\n",
    "    u6_power = lamda3*np.sum((np.fabs(np.diff(u6))))\n",
    "    u7_power = lamda4*np.sum((np.fabs(np.diff(u7))))\n",
    "\n",
    "    mse = (0\n",
    "            +y1_err\n",
    "            +y2_err\n",
    "            +u1_power\n",
    "            +u2_power\n",
    "            +u3_power\n",
    "            +u4_power\n",
    "            +u5_power\n",
    "            +u6_power\n",
    "            +u7_power\n",
    "            )\n",
    "    \n",
    "    # print('mse {:.7f}'.format(mse))\n",
    "    if isprint==1:\n",
    "        print('mse {:.7f}'.format(mse))\n",
    "        print('1111 {:.7f}'.format(y1_err))\n",
    "        print('2222 {:.7f}'.format(y2_err))\n",
    "        print('1111 {:.7f}'.format(u1_power))\n",
    "        print('2222 {:.7f}'.format(u2_power))\n",
    "        print('3333 {:.7f}'.format(u3_power))\n",
    "        print('4444 {:.7f}'.format(u4_power))\n",
    "\n",
    "\n",
    "\n",
    "    return mse , k_data2, E1_k*h1,  E2_k*h2\n",
    "    # return mse , k_data2, E1_k*h1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Times = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对未来Times周期预测控制\n",
    "max_control = 1.0\n",
    "# 期望设定值\n",
    "set_y1, set_y2, set_y1_trans, set_y2_trans = generate_y_aim_data(Times)\n",
    "\n",
    "# MPC参数\n",
    "P = 4  # 预测时域长度  增大P能平稳些\n",
    "M = 2  # 控制时域长度\n",
    "#生成控制时域的数据格式\n",
    "k_data = generate_k_data(u1_data, u2_data, u3_data, u4_data, u5_data, u6_data, u7_data, \n",
    "                        y1_data, y2_data, num_samples, P)\n",
    "\n",
    "\n",
    "# MPC控制循环   迭代的只有：k_data\n",
    "all_pred_y1 = []\n",
    "all_pred_y2 = []\n",
    "all_pred_u1 = []\n",
    "all_pred_u2 = []\n",
    "all_pred_u3 = []\n",
    "all_pred_u4 = []\n",
    "all_pred_u5 = []\n",
    "all_pred_u6 = []\n",
    "all_pred_u7 = []\n",
    "# MPC控制循环40\n",
    "for k in range(Times):\n",
    "    if iscontrol == False:\n",
    "        break\n",
    "\n",
    "    print(f\"这是对第{k}时刻的最优U1、U2输入求解\")\n",
    "\n",
    "    # 定义优化目标函数\n",
    "    def objective_function(params, *k_data):\n",
    "        mse, k_data2, E1_k_0, E2_k_0 = my_MPC(k_data=k_data[0], params=params, \n",
    "                                M=M, P=P, \n",
    "                                y1_aim = set_y1_trans[k], y2_aim = set_y2_trans[k],\n",
    "                                isprint = 0) \n",
    "        return mse\n",
    "    \n",
    "    # 初始猜测值[h U1 U2]   定义参数的上下限    设置退出条件\n",
    "    params = np.concatenate([np.ones(M), np.ones(M),np.ones(M), np.ones(M), np.ones(M),np.ones(M), np.ones(M)])\n",
    "    bounds = [(-max_control, max_control) for _ in range(7 * M)]\n",
    "    exit_conditions = {'maxiter': 1000} \n",
    "    # 进行优化\n",
    "    result = minimize(objective_function, params, method='L-BFGS-B', \n",
    "                    bounds=bounds, args=k_data)#args传进来的是一个元组\n",
    "\n",
    "\n",
    "    U1, U2, U3, U4, U5, U6, U7 =    result.x[0:M], result.x[M:2*M], \\\n",
    "                                    result.x[2*M:3*M], result.x[3*M:4*M], \\\n",
    "                                    result.x[4*M:5*M], result.x[5*M:6*M], \\\n",
    "                                    result.x[6*M:7*M]\n",
    "    \n",
    "\n",
    "    u1   = k_data[0:3]\n",
    "    u2   = k_data[3:6]\n",
    "    u3   = k_data[6:9]\n",
    "    u4   = k_data[9:12]\n",
    "    u5   = k_data[12:15]\n",
    "    u6   = k_data[15:18]\n",
    "    u7   = k_data[18:21]\n",
    "\n",
    "    y1   = k_data[21:24]\n",
    "    y2   = k_data[24:27]\n",
    "    u1   = np.concatenate((u1,U1,U1[-1]*np.ones(P-M)))\n",
    "    u2   = np.concatenate((u2,U2,U2[-1]*np.ones(P-M)))\n",
    "    u3   = np.concatenate((u3,U3,U3[-1]*np.ones(P-M)))\n",
    "    u4   = np.concatenate((u4,U4,U4[-1]*np.ones(P-M)))\n",
    "    u5   = np.concatenate((u5,U5,U5[-1]*np.ones(P-M)))\n",
    "    u6   = np.concatenate((u6,U6,U6[-1]*np.ones(P-M)))\n",
    "    u7   = np.concatenate((u7,U7,U7[-1]*np.ones(P-M)))\n",
    "    y1   = np.concatenate((y1,np.zeros(P)))\n",
    "    y2   = np.concatenate((y2,np.zeros(P)))\n",
    "\n",
    "\n",
    "\n",
    "    # 将控制序列第一个数作用于高炉\n",
    "    j = 1\n",
    "    x = np.column_stack((   u1[j+2],u2[j+2],u3[j+2],u4[j+2],u5[j+2],u6[j+2],u7[j+2],\n",
    "                            u1[j+1],u2[j+1],u3[j+1],u4[j+1],u5[j+1],u6[j+1],u7[j+1],\n",
    "                            y1[j+1],y2[j+1]))\n",
    "    x = x.reshape((x.shape[0], 1, x.shape[1]))\n",
    "    y1_pred0, y2_pred0 = model.my_predict(x)\n",
    "    y1_pred, y2_pred = model_gaolu.my_predict(x)\n",
    "    \n",
    "\n",
    "\n",
    "    # 更新k_data\n",
    "    params = np.concatenate((U1, U2, U3, U4, U5, U6, U7),axis=0)\n",
    "    mse, k_data2, E1_k_0, E2_k_0 =my_MPC(k_data=k_data,params=params,\n",
    "                            M=M,P=P, \n",
    "                            y1_aim = set_y1_trans[k], y2_aim = set_y2_trans[k],\n",
    "                            isprint = 0) \n",
    "\n",
    "\n",
    "    print(  '1设定',set_y1_trans[k].round(4),\\\n",
    "            '预测',y1_pred0.round(4),\\\n",
    "            '高炉', y1_pred.round(4),\\\n",
    "            '高炉与设定误差',(set_y1_trans[k]-y1_pred).round(4),(set_y1_trans[k]-y1_pred).round(4)/d_temp,\\\n",
    "            '模型误差',(y1_pred0 - y1_pred).round(4),\\\n",
    "            '校正值',E1_k_0.round(4))\n",
    "    print(  '2设定',set_y2_trans[k].round(4),\\\n",
    "            '预测',y2_pred0.round(4),\\\n",
    "            '高炉', y2_pred.round(4),\\\n",
    "            '高炉与设定误差',(set_y2_trans[k]-y2_pred).round(4),(set_y2_trans[k]-y2_pred).round(4)/d_yuansu,\\\n",
    "            '模型误差',(y2_pred0 - y2_pred).round(4),\\\n",
    "            '校正值',E2_k_0.round(4))\n",
    "\n",
    "\n",
    "\n",
    "    all_pred_y1.append(y1_pred)\n",
    "    all_pred_y2.append(y2_pred)\n",
    "    all_pred_u1.append(U1[0])\n",
    "    all_pred_u2.append(U2[0])\n",
    "    all_pred_u3.append(U3[0])\n",
    "    all_pred_u4.append(U4[0])\n",
    "    all_pred_u5.append(U5[0])\n",
    "    all_pred_u6.append(U6[0])\n",
    "    all_pred_u7.append(U7[0])\n",
    "    k_data2[23] = y1_pred.item()\n",
    "    k_data2[26] = y2_pred.item()\n",
    "    k_data = k_data2\n",
    "    # 进入下一时刻，更新预测时域、控制时域，即k_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据转换、\n",
    "data_tranform_plot(scalers,Times ,max_control,\n",
    "                        output_term,input_term,\n",
    "                        set_y1,set_y2,set_y1_trans,set_y2_trans,\n",
    "                        all_pred_y1, all_pred_y2,\n",
    "                        all_pred_u1,\n",
    "                        all_pred_u2,\n",
    "                        all_pred_u3,\n",
    "                        all_pred_u4)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39_tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
